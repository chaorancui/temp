[toc]

# 大模型相关知识

## Transformer 中 Q K V

在 Transformer 架构中，**Q**、**K** 和 **V** 分别指 **Query**、**Key** 和 **Value**，它们是 **自注意力机制**（Self-Attention Mechanism）的核心组件。Transformer 模型的成功很大程度上依赖于这种自注意力机制，它能够在处理序列时捕捉全局信息，使得模型可以理解序列中的上下文关系。

### 自注意力机制

自注意力机制（Self-Attention Mechanism）的目标是让每个输入位置都能够**自适应地关注**其他输入位置，从而捕捉序列中不同部分的依赖关系。为了实现这个目的，输入序列的每个元素都会生成三个向量：**Query（Q）**、**Key（K）** 和 **Value（V）**。

- **Query (Q)**：表示模型当前关注的焦点。每个位置的 Query 向量用于发起"查询"来寻找与其他位置的相关性。
- **Key (K)**：表示输入序列中特征的编码。每个位置的 Key 向量用来与 Query 进行匹配，确定输入序列中其他位置的重要性。
- **Value (V)**：代表输入序列中实际的信息内容。最终的输出是基于与其他位置的相关性，从 Value 向量中提取信息。

### Q, K, V 的生成

对于输入序列中的每个元素，通过三个不同的**可学习矩阵**将输入向量分别映射为 `Q`、`K` 和 `V` 向量。这些矩阵分别为 $ W_Q $、$ W_K $ 和 $ W_V $，它们的形状通常为：

$$ Q = X \cdot W_Q, \quad K = X \cdot W_K, \quad V = X \cdot W_V $$

其中 $ X $ 是输入序列中的元素，$ W_Q $、$ W_K $、 $ W_V $ 是用于线性变换的可训练参数。

**注意**：

`Q`（Query）、`K`（Key）和 `V`（Value）这三个向量的**维度不一定相等**，但它们在 Transformer 模型中的具体维度安排有一些常见的要求和约定，特别是在**自注意力机制**中。

1. **Query 和 Key 的维度必须相同**；
2. **Value 的维度可以不同**。

   在自注意力机制中，`Q` 和 `K` 的维度**必须相同**，这是因为自注意力机制需要计算 `Q` 和 `K` 的**点积**，从而得到一个注意力分数（Attention Score）。只有在 `Q` 和 `K` 的维度一致的情况下，点积运算才能正确进行。

   - 如果 `Q` 的维度是 $ d_k $，那么 `K` 的维度也必须是 $ d_k $。
   - 这个维度 $ d_k $ 通常是可调参数，它可以比输入向量的维度小一些，以减少计算开销。

   `V`（Value）的维度可以不同于 `Q` 和 `K` 的维度。在注意力机制中，`V` 主要用于存储最终要输出的信息，它并不参与点积运算。因此，`V` 的维度 dvd_vdv 可以和 `Q`、`K` 的维度不同。

   - 通常，`V` 的维度可以设定为与 `Q`、`K` 相同，或者与输入的维度相同。这个维度的大小也是一个可调参数。

### 自注意力机制的计算过程

1. 计算 Query 和 Key 之间的相似度

   通过点积计算 Query 和 Key 的相似度，衡量当前 Query 与其他输入位置的相关性。点积越大，表示相关性越高。

   $$ \text{Attention Score} = Q \cdot K^T $$

   为了避免数值过大导致梯度爆炸，通常会将点积结果除以 $ \sqrt{d_k} $，其中 $ d_k $ 是 Key 的维度。

   $$ \text{Scaled Attention Score} = \frac{Q \cdot K^T}{\sqrt{d_k}} $$

2. 归一化相似度（Softmax）

   将相似度通过 **Softmax** 函数归一化为注意力权重，使得权重和为 1，表示不同输入位置的相关性权重。

   $$ \text{Attention Weights} = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) $$

3. 加权求和 Value

   根据计算出的注意力权重，对每个位置的 Value 向量进行加权求和，从而得到当前输入位置的加权输出。这一步使得模型可以提取相关输入位置的信息。

   $$ \text{Attention Output} = \sum(\text{Attention Weights} \cdot V) $$

   这个加权的输出就是输入序列经过自注意力机制后得到的新的特征表示。

### 多头自注意力

Transformer 模型中的注意力机制并不仅仅依赖单个 `Q`、`K` 和 `V`，而是通过**多头注意力**（Multi-Head Attention）来捕捉输入序列中多种不同的关系。多头自注意力机制的过程如下：

1. 将 `Q`、`K` 和 `V` 向量分成多个头（通常是 8 个或更多），分别计算每个头的自注意力。
2. 对每个头的输出进行拼接。
3. 将拼接后的结果通过线性变换得到最终的输出。

这样，多头自注意力允许模型从多个角度关注输入序列中不同位置的关系，从而增强模型的表现能力。

**多头注意力中的维度**：

在多头注意力机制（Multi-Head Attention）中，`Q`、`K` 和 `V` 的维度会进一步被分割成多个“头”。假设模型的输入维度是 $ d\_{\text{model}} $，而有 $ h $ 个头（多头注意力的数量），则每个头的 `Q`、`K`、`V` 向量的维度一般为：

$$ \text{Each head's dimension} = \frac{d\_{\text{model}}}{h} $$

这意味着，每个头都会处理一个更小的维度，计算并捕捉不同的上下文信息。最后，多头注意力机制会将每个头的输出拼接起来，并通过线性变换将其映射回模型的原始维度 $ d*{\text{model}} $。
假设模型输入的维度为 $ d*{\text{model}} = 512 $，并且有 8 个头的多头注意力机制。那么：

- 每个头的 `Q`、`K` 的维度为 $ \frac{512}{8} = 64 $。
- 每个头的 `V` 的维度可以是 64，也可以是其他维度（例如 128），具体取决于模型设计中的选择。

### Q、K、V 的作用总结

- **Query (Q)**：代表当前输入位置的查询向量，模型使用它来寻找与其他位置的相关性。
- **Key (K)**：代表每个输入位置的特征，用来与 Query 进行匹配，确定当前 Query 关注的输入位置。
- **Value (V)**：代表输入数据的实际内容，当模型确定了需要关注哪些位置后，提取相应的 Value 信息。

### 自注意力的优势

- **全局依赖**：自注意力机制允许输入序列的每个元素都能够与其他元素进行直接交互，打破了传统 RNN 或 CNN 中局部感受野的限制。
- **并行计算**：与 RNN 不同，自注意力机制可以并行处理输入序列中的所有元素，提高计算效率。
- **捕捉长距离依赖**：通过对不同位置进行注意力加权，自注意力机制可以轻松捕捉长距离的依赖关系。

### 总结

在 Transformer 中，`Q`（Query）、`K`（Key）、`V`（Value）是自注意力机制的核心，分别用于：

- **Query**：发起查询，表示当前元素需要的信息。
- **Key**：描述序列中每个元素的特征。
- **Value**：序列中每个元素的实际信息。
- **Query 和 Key 的维度必须相同**，因为它们需要进行点积运算来计算注意力分数。
- **Value 的维度可以不同于 Query 和 Key**，因为它不参与点积运算，而是用于存储最终的输出信息。
- **在多头注意力机制中**，`Q`、`K` 和 `V` 的维度会被分割成多个较小的维度来提高计算效率和模型的表达能力。

通过计算 Query 与 Key 的相似度并加权 Value，自注意力机制可以动态聚焦序列中的重要部分，极大地提升了模型在自然语言处理等任务中的表现能力。

## lora

在大模型（例如大型语言模型、深度学习模型）中，​**LoRA**​（Low-Rank Adaptation）是一种有效的**模型微调**方法，用于提高预训练大模型在**特定任务上**的表现，同时大幅减少训练过程中所需的计算资源和存储开销。

LoRA 的核心思想是通过引入低秩矩阵（Low-Rank Matrix）来对预训练模型进行微调，而不是对整个模型参数进行更新。具体来说，LoRA 方法在现有的神经网络架构中插入可训练的低秩矩阵（通常是较小的矩阵），然后仅更新这些新增的低秩参数，而保持原模型的大部分参数不变。

### LoRA 生效的原理

LoRA（Low-Rank Adaptation）生效的原理主要基于矩阵分解和低秩近似技术。它通过引入低秩矩阵来调整神经网络的参数，而不直接修改原始的预训练权重，从而实现高效的微调。下面从理论原理、矩阵分解的角度和如何减少计算成本等方面进行分析。

1. **权重矩阵的低秩近似**： 在传统的神经网络训练中，模型的权重矩阵通常是高维的。LoRA 提出的低秩近似方法认为，可以将这些**大矩阵分解为两个较小的矩阵（低秩矩阵）**，通过对这些较小矩阵进行调整，从而有效地改变原始权重矩阵的行为。

   ![lora结构图](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d76bfc266f6240cbaa55838045728b86~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.webp)

   **低秩矩阵的定义**：
   假设原始神经网络的权重矩阵 $ W $ 是一个 $ m \times n $ 的矩阵，其中 $ m $ 和 $ n $ 是矩阵的行和列数。LoRA 的核心思想是通过将 $ W $ 近似为两个低秩矩阵 $ A $ 和 $ B $ 的乘积，即：

   $$ W' = W + \Delta W = W + AB $$

   其中：

   - $ A $ 是一个 $ m \times r $ 的矩阵，
   - $ B $ 是一个 $ r \times n $ 的矩阵，
   - $ r $ 是秩（rank），通常 $ r $ 会比 $ m $ 和 $ n $ 小得多。

   这意味着原始权重矩阵 $ W $ 不需要完全被重新学习，只需要学习低秩矩阵 $ A $ 和 $ B $​，从而有效地减少了需要更新的参数数量。
   举例来说，假设 $ m=n=100, r=1 $，$ W $ 的参数量为 10000 个，而 $ \Delta W $ 的参数量只有 200 个，是**原来的 1/50**，参数下降的十分显著。

2. **冻结预训练模型的参数**： 在 LoRA 中，原始预训练模型的权重矩阵 $ W $ 是被冻结的，也就是说，训练过程中这些权重不会改变。只有新增的低秩矩阵 $ A $ 和 $ B $ 是需要更新的。

   这一做法的好处是，原始模型已经通过大量数据进行了训练，拥有了强大的知识表示能力。因此，我们只需要对新增的低秩矩阵进行训练，调整这些矩阵的参数**以使模型适应特定任务（例如分类、生成等），而不必重新训练整个模型**。

3. **低秩矩阵的作用**：

   - **降低计算/存储开销**：由于低秩矩阵 $ A $ 和 $ B $ 的维度远小于原始矩阵 $ W $，这意味着它们的计算成本和存储成本相对较低。
   - **高效的参数调整**：通过在预训练的基础上引入低秩矩阵，LoRA 使得在进行微调时，只需要对少量参数进行优化。尤其对于超大规模的预训练模型（如 GPT、BERT 等），LoRA 通过减少更新参数的数量，显著降低了计算和存储的开销。
   - **泛化能力**：LoRA 的低秩矩阵调整通常能够保留原始模型的泛化能力，同时在特定任务上取得良好的效果。因为低秩矩阵通常只能调整模型的特定部分，避免了过度拟合原始模型，从而能够提高微调的稳定性和泛化性能。

   因此，低秩矩阵的引入使得微调过程更加高效，同时保持了较高的任务性能。

### LoRA 的优点

1. **低计算成本**：由于只需要更新低秩矩阵，相比传统的微调方法，LoRA 大大减少了计算和存储的需求。
2. **节省存储空间**：LoRA 插入的低秩矩阵占用的存储空间远小于整个模型的参数，因此能大大降低存储成本。
3. **快速收敛**：LoRA 的低秩矩阵通常能以更少的训练步骤就收敛到较好的性能，这使得微调过程更加高效。
4. **不改变原始模型**：LoRA 不需要修改原始的预训练模型结构，这使得它在需要保持预训练模型完整性的情况下尤为有用。

### LoRA 的应用场景

LoRA 适用于许多需要进行高效微调的应用场景，特别是当预训练的大型模型已经在大量数据上训练过，并且希望在某个特定任务（如分类、翻译、问答等）上进行微调时。LoRA 能够在大规模模型上实现高效的迁移学习，同时保持计算和存储效率。

展开来看：

- **大模型微调**：当面临需要对大型预训练模型进行微调而资源有限的情况（如计算力、存储空间等），LoRA 提供了一种有效的解决方案。
- **个性化任务**：在不同任务或领域上微调大型预训练模型时，LoRA 可以减少重新训练的开销，同时保持较高的性能。
- **资源受限的环境**：对于在边缘设备或其他资源有限的环境中部署大模型时，LoRA 可以提供高效的训练和推理支持。

应用例子：

- **语言模型微调**：在语言生成任务中，LoRA 可以用来微调大型语言模型（如 GPT 或 BERT）以适应特定领域或特定任务，而不需要对整个模型进行训练。
- **计算机视觉模型微调**：在图像识别等任务中，LoRA 也可以有效地微调深度卷积神经网络（CNN），使其能够处理特定的视觉任务。

### 总结

LoRA 是一种低成本、资源高效的微调方法，特别适合在大型预训练模型上进行任务适配。它通过引入低秩矩阵对模型进行微调，既能保持较好的任务性能，又能显著减少计算和存储开销。

---

# 性能优化

[CUDA SGEMM 优化笔记](https://linn-ylz.com/Computer-Science/CUDA/CUDA-SGEMM-optimization-notes/)

---

# Embedding

## Embedding 介绍

**Embedding**是深度学习中一种用于将离散数据（如词汇、类别等）映射到连续向量空间的技术。它的核心思想是将离散的、高维的表示（如词汇表中的词）通过一个低维的向量来表示，这些向量能够捕捉数据之间的语义关系。**词嵌入（word embedding）** 是最典型的 Embedding 应用之一，它将每个单词映射为一个固定大小的向量，向量中包含了该单词的语义信息。

1. **Embedding 的作用**

   1. **降低维度**：将高维离散数据（如词汇 ID）转换为低维连续向量，减少计算复杂度。
   2. **语义表示**：Embedding 能捕捉数据之间的语义关系。相似的数据会被映射到相近的向量空间中，例如，在词嵌入中，"king"和"queen"可能会有相近的向量表示。
   3. **提高模型性能**：通过将离散输入转化为连续表示，使得模型能够更好地处理语义和关系信息，提高模型的表达能力和泛化能力。
   4. **可训练**：Embedding 是可以通过训练优化的，在特定任务中，模型能够自动学习出具有更好语义表示的嵌入向量。

2. **Embedding 常见应用**

   - **自然语言处理**：词嵌入（Word2Vec、GloVe、BERT 等）用于将词汇转换为向量表示，常用于文本分类、翻译、问答等任务。
   - **推荐系统**：将用户和物品的 ID 映射到一个共同的嵌入空间，用于捕捉用户兴趣和物品特征的相似性。
   - **图像处理**：将图像特征映射到向量空间，用于图像分类、目标检测等任务。

   Embedding 技术通过将高维离散数据转化为低维连续向量，显著提高了神经网络模型处理复杂语义和关系的能力。

3. **Embedding 层详细示例**

   我们假设有一个简单的词汇表，其中包含 5 个词汇，每个词都有一个唯一的 ID。我们想将每个词的 ID 映射到一个 2 维的向量空间（即每个词用一个 2 维向量表示）。

   词汇表与 ID 映射：

   ```log
   词汇表 = {'apple': 0, 'banana': 1, 'cherry': 2, 'date': 3, 'elderberry': 4}
   ```

   Embedding 设置：

   ```python
   import torch
   import torch.nn as nn

   # 创建一个词汇表大小为5，嵌入维度为2的Embedding层
   embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=2) # torch.nn.Embedding

   # 定义输入的词汇ID序列，假设我们有三个词：'apple', 'banana', 'elderberry'
   # 对应的词汇ID为[0, 1, 4]
   input_ids = torch.tensor([0, 1, 4])

   # 使用Embedding层将词汇ID映射为向量
   embedded_output = embedding_layer(input_ids)

   print("Embedding向量输出：\n", embedded_output)
   ```

   输入：

   ```log
   input_ids：[0, 1, 4]，这是我们要转换为向量的词汇 ID，表示“apple”、“banana”和“elderberry”。
   ```

   输出：

   ```log
   Embedding向量输出，形状为 (3, 2) 的张量：
   tensor([[-1.1071,  0.3695],  # 'apple' 对应的2维向量表示
           [-0.9181, -0.0533],  # 'banana' 对应的2维向量表示
           [ 0.3387,  0.2173]]) # 'elderberry' 对应的2维向量表示
   ```

## Embedding 与 Gather

Embedding 通常用一个简单的**查表（lookup table）** 操作实现。此查表操作可用 `gather` 算子完成，详细解释如下：

- `gather` 是一种常用的张量操作，用于从张量的特定位置提取数据。它常用于根据索引从一个张量中检索数据。
- 在词嵌入（Embedding）中，我们通常有一个**嵌入矩阵**，矩阵的每一行代表一个词的向量表示。当我们输入一个词汇 ID 时，模型会从嵌入矩阵中检索该词对应的行向量，这个操作与 `gather` 的效果一致。

举例说明：

- 假设我们有一个嵌入矩阵 `E`，其维度为 `V x D`，其中：`V` 是词汇表的大小（词汇数），`D` 是每个词的向量维度。

- 假设输入的词汇 ID 为一个索引列表 `ids = [2, 5, 0]`，我们需要从嵌入矩阵 `E` 中取出这 3 个词的嵌入向量，使用`gather`操作实现：

```python
import torch

# 假设嵌入矩阵E，大小为5 x 3（5个词，每个词嵌入3维向量）
E = torch.tensor([
    [0.1, 0.2, 0.3],  # 词1的向量
    [0.4, 0.5, 0.6],  # 词2的向量
    [0.7, 0.8, 0.9],  # 词3的向量
    [1.0, 1.1, 1.2],  # 词4的向量
    [1.3, 1.4, 1.5]   # 词5的向量
])

# 假设输入的ID是[2, 4, 0]，我们希望提取第2、4、0行向量
ids = torch.tensor([2, 4, 0])

# 使用gather提取对应行
# 对行进行索引，使用gather操作等价于直接按行提取
embedding_output = E[ids]

print(embedding_output)
```

输出：

```log
tensor([[0.7000, 0.8000, 0.9000],  # 对应ID 2的词
        [1.3000, 1.4000, 1.5000],  # 对应ID 4的词
        [0.1000, 0.2000, 0.3000]]) # 对应ID 0的词
```

在深度学习框架的实际实现中，Embedding 层的底层实现可能会使用`gather`，因为它高效且简洁，能快速从嵌入矩阵中取出对应的向量。

## token/word embedding 区别

**Token embedding** 和 **word embedding** 虽然有相似之处，但它们并不是完全相同的概念，主要的区别在于它们所处理的基本单位不同。

1. **Word Embedding**

   - **基本概念**：**Word embedding** 是将**单词**（word）映射到连续向量空间中的表示方法。常见的词嵌入技术如 **Word2Vec**、**GloVe** 等会为每个单词（完整的词）生成一个固定维度的向量。
   - **处理单位**：**单词**，通常是文本中自然语言的最小语义单位。
   - **使用场景**：在较早的自然语言处理任务中，模型通常是基于词汇表（vocabulary），将每个词映射为对应的嵌入向量。在这种情况下，一个词就是一个基本处理单位。

2. **Token Embedding**

   - **基本概念**：**Token embedding** 是针对**令牌（token）** 的嵌入，将文本中的每个 token（令牌）映射为向量。令牌可能是一个完整的词、一个子词（subword），甚至是一个字符。
   - **处理单位**：**令牌（token）**，通常由**词、子词或字符**组成。例如，BERT 和 GPT 等模型使用**子词**作为 token 单位，这样即使遇到生僻词或新词，也可以将词拆分为多个子词 token。
   - **使用场景**：在现代预训练语言模型（如 BERT、GPT 等）中，使用**子词级别的分词**（如 BPE 或 WordPiece 算法）将文本拆分为更小的单位。这种 token 可能是一个完整的词，也可能是子词的一部分。因此，token embedding 既可以表示词，也可以表示比词更小的单位。

3. **主要区别**

   1. **基本处理单位不同**：
      粒度：Token 通常比 word 更细粒度。
      词汇表大小：Token embedding 通常有更小的词汇表。
      - **Word embedding** 的基本单位是词汇表中的**完整单词**。
      - **Token embedding** 的基本单位是**令牌（token）**，它可能是完整的单词，也可能是子词，甚至是字符。
   2. **对 OOV（Out-Of-Vocabulary）词的处理**：
      灵活性：Token embedding 可以处理更多种类的输入。
      - **Word embedding** 模型无法处理未在词汇表中出现的词（称为 OOV 问题）。
      - **Token embedding** 由于使用了子词级别的分词方法，即使遇到未知的词，也可以将其拆解为已知的子词来处理，从而缓解 OOV 问题。
   3. **应用模型**：
      - **Word embedding** 通常应用于较早期的 NLP 模型（如 Word2Vec、GloVe），它们依赖于固定的词汇表。
      - **Token embedding** 广泛应用于现代的预训练语言模型（如 BERT、GPT），这些模型通常采用基于子词的分词技术。

4. **举例说明**

   **Token embedding 在处理未知词时确实有显著优势**。假设我们有一个英语词汇表，其中并不包含"unfriendliness"这个词。

   - **Word Embedding** 的处理方式：
     如果使用传统的 word embedding，"unfriendliness"会被视为一个未知词（Out-of-Vocabulary，OOV）。
     系统可能会将其替换为特殊的 `<UNK>` 标记，或者完全忽略这个词。
     **结果**：词的含义完全丢失，模型无法理解这个词。

   - **Token Embedding** 的处理方式：
     使用子词（subword）tokenization，"unfriendliness" 可能被拆分为：["un", "friend", "li", "ness"]，每个子词都有其 own embedding。
     模型可以组合这些子词的含义：
     "un-"表示否定
     "friend"表示朋友
     "-li-"是连接词素
     "-ness"表示状态或品质
     **结果**：虽然"unfriendliness"作为一个完整的词不在词汇表中，但模型仍能大致理解其含义（不友好的状态或品质）。

   - Token embedding 的几个优势：

     - **词形解析**：可以处理词缀、复合词等。
     - **跨语言应用**：这种方法对于形态学丰富的语言（如德语、土耳其语）特别有效。
     - **词汇表压缩**：可以用更小的词汇表表示更多的词。
     - **处理专业术语**：在特定领域（如医学、法律）中的生僻词也可以得到合理处理。

5. **总结**

   - **Word embedding** 关注完整的单词，适合较早的 NLP 任务。
   - **Token embedding** 关注更细粒度的语言单位（如子词），适合现代预训练模型的需求，能够更灵活地处理复杂的语言结构。

## token embedding

Token embedding 是自然语言处理（NLP）中的一种技术，用于将文本数据中的单词、子词或字符（称为 token）转换为**稠密的向量表示**。这种向量表示能够捕捉文本中词语的**语义和语法信息**，并在机器学习模型中作为输入特征。Token embedding 的目标是将**高维、稀疏的离散表示**（如 one-hot 编码）转换为**低维、稠密的连续表示**，从而使得文本数据可以更高效地被处理和理解。

**常见的 Token Embedding 方法**：

1. **One-hot Encoding**
   - 每个 token 用一个与词汇表大小相同的向量表示，向量中只有一个位置为 1，其余位置为 0。这种表示方式非常稀疏且高维，不适合直接用于复杂的模型。
2. **Word Embeddings**
   - **Word2Vec**：基于神经网络的模型，通过训练 Skip-gram 或 CBOW 模型生成词嵌入。它能够捕捉词语之间的语义关系，例如“国王”与“王后”之间的关系。
   - **GloVe**：基于共现矩阵的统计方法，通过矩阵分解生成词嵌入。它能够捕捉全局语义信息。
3. **Subword Embeddings**
   - **FastText**：在 Word2Vec 的基础上，考虑了词的子词（subword）信息，这样可以更好地处理未登录词（OOV）和词形变化。
4. **Contextualized Embeddings**
   - **ELMo**：基于双向 LSTM，通过上下文信息生成动态词嵌入。每个词的表示会根据其在句子中的上下文变化。
   - **BERT**：基于 Transformer 的双向编码器，通过预训练和微调方法生成词嵌入。它能够捕捉深层次的上下文信息，并且在许多 NLP 任务中表现出色。

**示例**：

假设我们有一个句子：“I love natural language processing”。

1. **One-hot Encoding**

   ```log
   I: [1, 0, 0, 0, 0, 0]
   love: [0, 1, 0, 0, 0, 0]
   natural: [0, 0, 1, 0, 0, 0]
   language: [0, 0, 0, 1, 0, 0]
   processing: [0, 0, 0, 0, 1, 0]
   ```

2. **Word2Vec Embedding** (假设嵌入维度为 3)

   ```log
   I: [0.2, -0.1, 0.7]
   love: [0.8, 0.3, -0.4]
   natural: [0.5, -0.2, 0.1]
   language: [0.3, 0.6, -0.5]
   processing: [-0.4, 0.2, 0.9]
   ```

3. **BERT Embedding**

   ```log
   I: [0.1, 0.2, 0.3, ..., 0.8]
   love: [0.4, 0.5, 0.1, ..., 0.2]
   natural: [0.3, 0.1, 0.6, ..., 0.9]
   language: [0.5, 0.7, 0.2, ..., 0.1]
   processing: [0.2, 0.4, 0.8, ..., 0.6]
   ```

在实际应用中，token embedding 可以通过预训练好的嵌入模型（如 GloVe 或 BERT）来生成，或者通过在特定任务上微调这些预训练模型以获得更好的性能。

---

# 术语

## 按通道/按元素

1. Channel-wise（按通道）：Channel-wise 归一化是指对每个通道独立进行归一化操作。每个通道都有自己的均值和标准差。例如，在 Batch Normalization（BN）中，通常会对每个通道分别计算均值和标准差，然后进行归一化。
2. Element-wise（按元素）：Element-wise 操作是对张量中的每个元素逐个进行处理。对于归一化来说，这意味着每个元素都应用相同的归一化参数，例如均值和标准差。

## NCHW 与 NHWC 数据格式

流行的[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)框架中有不同的数据格式，典型的有 NCHW 和 NHWC 格式。本文从逻辑表达和物理存储角度用图的方式来理解这两种数据格式，最后以 RGB 图像为例来加深 NHWC 和 NCHW 数据存储格式的理解。

### 一、基本概念

深度学习框架中，数据一般是 4D，用 NCHW 或 NHWC 表达，其中：

- N - Batch
- C - Channel
- H - Height
- W - Width

### 二、逻辑表达

假定 N = 2，C = 16，H = 5，W = 4，那么这个 4D 数据，看起来是这样的：

![](https://i-blog.csdnimg.cn/blog_migrate/fe8a6e587ff4760a0d13c2c70aaf3340.png)

人类比较直接的理解方式是 3D，上图中从三个方向上理解，C 方向/H 方向/W 方向。然后是 N 方向上，就是 4D。

上图中红色标准的数值是这个数据里每个元素的数值。

### 三、物理存储

无论逻辑表达上是几维的数据，在计算机中存储时都是按照 1D 来存储的。NCHW 和 NHWC 格式数据的存储形式如下图所示：

![](https://i-blog.csdnimg.cn/blog_migrate/964468ffadc3c2aff2028f8a22ed91cf.png)

1. NCHW

   NCHW 是先取 W 方向数据；然后 H 方向；再 C 方向；最后 N 方向。

   所以，序列化出 1D 数据：

   000 (W 方向) 001 002 003，(H 方向) 004 005 ... 019，(C 方向) 020 ... 318 319，(N 方向) 320 321 ...

2. NHWC

   NHWC 是先取 C 方向数据；然后 W 方向；再 H 方向；最后 N 方向。

   所以，序列化出 1D 数据：

   000 (C 方向) 020 ... 300，(W 方向) 001 021 ... 303，(H 方向) 004 ... 319，(N 方向) 320 340 ...

### 四、RGB 图像数据举例

表达 RGB 彩色图像时，一个像素的 RGB 值用 3 个数值表示，对应 Channel 为 3。易于理解这里假定 N=1，那么 NCHW 和 NHWC 数据格式可以很直接的这样表达：

![](https://i-blog.csdnimg.cn/blog_migrate/d21b1b8b279cf814e5a5c5bf45b67d83.png)

**NCHW**是

- 先在一个 Channel 面上把 W 方向|H 方向上元素存储起来 // R
- 然后再在另一个 Channel 切面上把 W 方向|H 方向上元素存储起来 // G
- 最后一个 Channel 切面上把 W 方向|H 方向上元素存储起来 // B

这样看起来，就是先把 R 通道的每个像素都存储；然后存储 G 通道；再然后 B 通道。

**NHWC**是

- 先把 3 个 Channel 上元素存储起来 // 也就是一个像素的 RGB
- 然后再在 W 方向|H 方向上元素存储起来

这样看起来，就是顺序地取像素的 RGB 数值存储起来

### 五、不同框架支持

目前的主流 ML 框架对 NCHW 和 NHWC 数据格式做了支持，有些框架可以支持两种且用户未做设置时有一个缺省值：

- TensorFlow：缺省 NHWC，GPU 也支持 NCHW
- Caffe：NCHW
- PyTorch：NCHW

## CNN 和 RNN

卷积神经网络（CNN）和循环神经网络（RNN）是两种广泛应用于深度学习的神经网络，它们各自适用于不同类型的数据和任务。以下是它们的主要区别：

1. **数据类型**

   - **CNN**（Convolutional Neural Network）：**主要用于处理具有网格结构的数据**，如图像和视频。图像可以看作是二维的网格，而视频则可以看作是三维的网格（时间 + 空间）。
   - **RNN**（Recurrent Neural Network）：**主要用于处理序列数据**，如时间序列、文本、语音信号等。RNN 能够利用序列的顺序信息，通过其循环结构捕捉时间或序列中的依赖关系。

2. **结构**

   - **CNN**：采用卷积层和池化层交替堆叠的结构。卷积层用于提取特征，池化层用于降维和减小计算量。卷积操作具有局部连接和权重共享的特点，能够有效地捕捉局部特征。
   - **RNN**：具有循环结构，网络中的每个节点不仅接收当前输入，还接收前一时刻的隐藏状态作为输入。这样可以将前一时刻的信息传递到下一时刻，从而捕捉序列中的时间依赖关系。

3. **主要应用**

   - **CNN**：广泛应用于计算机视觉任务，如图像分类、目标检测、图像分割等。
   - **RNN**：广泛应用于自然语言处理（NLP）任务，如语言模型、机器翻译、情感分析等，以及时间序列预测、语音识别等。

4. **处理方式**

   - **CNN**：通过卷积核在输入数据上进行滑动窗口操作，提取局部区域的特征。其主要特点是局部连接、权重共享和池化操作。
   - **RNN**：通过循环单元（如标准 RNN 单元、长短期记忆（LSTM）单元或门控循环单元（GRU））来处理序列数据，每个时间步的输出依赖于当前输入和前一时刻的隐藏状态。

5. **参数共享**

   - **CNN**：卷积核的参数在整个输入图像上共享，这减少了参数的数量，提高了计算效率。
   - **RNN**：在每个时间步上共享参数，这使得 RNN 能够处理变长的序列。

6. **计算复杂度**

   - **CNN**：通常计算复杂度较低，适合并行计算，因为卷积操作可以在不同的局部区域独立进行。
   - **RNN**：计算复杂度较高，因为每个时间步的计算依赖于前一个时间步，难以并行化。

7. **总结**

   CNN 和 RNN 各自擅长处理不同类型的数据和任务。CNN 由于其卷积操作的特性，适合处理图像等网格结构数据；而 RNN 由于其循环结构，适合处理序列数据。两者在深度学习中的应用广泛且各有优势，可以根据具体任务的需求选择合适的模型。

## RNN 模型中参数解释

以下是对批次大小、序列长度等参数的简要解释:

1. 批次大小 (Batch Size):
   批次大小指的是在**一次迭代中用于训练的样本数量**。较大的批次大小可以提高训练速度,但可能会影响模型的泛化能力。较小的批次大小可能会使训练更慢,但可能有助于模型更好地泛化。
2. 序列长度 (Sequence Length):
   序列长度是指 RNN 处理的**输入序列中的时间步数**。在自然语言处理任务中,这通常对应于一个句子或文档中的单词数。较长的序列可以捕捉更长期的依赖关系,但也会增加计算复杂度。
3. 隐藏层大小 (Hidden Layer Size):
   这是 RNN**隐藏状态的维度**。较大的隐藏层可以捕捉更复杂的模式,但也会增加模型的参数数量和计算复杂度。
4. 学习率 (Learning Rate):
   学习率控制**每次参数更新的步长**。较高的学习率可能导致快速学习但也可能导致不稳定,而较低的学习率可能导致学习过慢。
5. 迭代次数 (Number of Epochs):
   一个 epoch 指的是**整个训练数据集被传递一次**。增加 epoch 数可以改善模型性能,但也可能导致过拟合。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   这个参数用于**防止梯度爆炸**问题。它限制了梯度的最大范数。
7. 丢弃率 (Dropout Rate):
   丢弃率是一种正则化技术,用于防止过拟合。它指定了在训练过程中随机"丢弃"（即设置为零）的神经元的比例。

**示例：**

1. 批次大小 (Batch Size):
   例子: 假设我们有一个包含**10,000 个样本**的数据集。如果我们设置**批次大小为 32**,那么在每次迭代中,模型会处理 32 个样本,并更新一次参数。这意味着完成一个 epoch 需要大约 313 次迭代 Iteration(10,000/32≈313)。
2. 序列长度 (Sequence Length):
   例子: 在情感分析任务中,如果我们**设置序列长度为 50**,这意味着我们会考虑每条评论的前 50 个词。如果评论短于 50 个词,我们会进行填充;如果长于 50 个词,我们会截断。
3. 隐藏层大小 (Hidden Layer Size):
   例子: 在一个语言模型中,如果我们设置隐藏层大小为 256,这意味着 RNN 的**每个时间步都会产生一个 256 维的向量**来表示当前的隐藏状态。
4. 学习率 (Learning Rate):
   例子: 假设我们从 0.01 的学习率开始训练。如果发现模型学习不稳定,我们可能会降低到 0.001。或者,我们可能使用学习率调度器,在训练过程中逐渐降低学习率,如从 0.01 开始,每 5 个 epochs 降低 10%。
5. 迭代次数 (Number of Epochs):
   例子: 在训练一个机器翻译模型时,我们可能会**设置 100 个 epochs**。但是,如果我们发现在第 50 个 epoch 后验证集性能不再提高,我们可能会提前停止训练。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   例子: 如果我们设置梯度裁剪阈值为 5.0,这意味着如果计算出的梯度的 L2 范数大于 5.0,我们会将其缩放,使其范数等于 5.0。这**有助于防止梯度爆炸**。
7. 丢弃率 (Dropout Rate):
   例子: 在一个文本分类模型中,我们可能在 RNN 层后设置 0.5 的丢弃率。这意味着在每次训练迭代中,有 50%的神经元输出会被随机设置为 0,从而减少过拟合。
8. 网络架构:
   例子: 在一个命名实体识别任务中,我们可能使用双向 LSTM。这允许模型不仅考虑单词的左侧上下文,还能考虑右侧上下文,从而提高识别准确率。
9. 注意力机制:
   例子: 在机器翻译任务中,当翻译长句子时,注意力机制允许模型在生成每个目标词时"关注"源句子的不同部分。例如,翻译"The cat sat on the mat"时,在生成"猫"这个词时,模型会主要关注源句子中的"cat"。
10. 词嵌入:
    例子: 在情感分析任务中,我们可能使用预训练的 300 维 GloVe 词向量。这意味着每个词都会被表示为一个 300 维的向量,这些向量已经在大规模文本语料上预先训练好,能捕捉词的语义信息。
11. 教师强制 (Teacher Forcing):
    例子: 在训练一个文本生成模型时,如果我们使用 80%的教师强制率,这意味着在 80%的时间里,模型会使用真实的前一个单词作为下一步的输入,而在 20%的时间里使用自己生成的单词。这有助于模型学习正确的序列,同时也允许一些探索。

这些是 RNN 模型中的一些关键参数。根据具体任务和数据集,这些参数的最佳值可能会有所不同。通常需要通过实验和调优来找到最适合特定问题的参数设置。

**iteration 和 epoch 的区别：**

1. 迭代 (Iteration):
   - 一次迭代是指使用一个批次(batch)的数据对模型进行一次前向传播和反向传播的过程。
   - 在每次迭代后,模型参数会更新一次。
2. Epoch:
   - 一个 epoch 是指整个训练数据集被完整地通过神经网络一次并返回。
   - 一个 epoch 包含了多个迭代,具体取决于批次大小和数据集大小。

让我们用一个具体的例子来说明:

例子:
假设我们有一个包含 10,000 个样本的数据集,我们选择的批次大小(batch size)是 32。

计算:

- 每个 epoch 需要的迭代次数 = 数据集大小 / 批次大小
- 在这个例子中: 10,000 / 32 ≈ 313 (向上取整)

这意味着:

1. 每个 epoch 包含 313 次迭代。
2. 在每次迭代中,模型处理 32 个样本。
3. 完成 313 次迭代后,模型就处理了整个数据集一次,这就完成了一个 epoch。

现在,如果我们设置训练为 100 个 epochs:

- 总迭代次数 = 每个 epoch 的迭代次数 × epoch 数
- 在这个例子中: 313 × 100 = 31,300 次迭代

因此:

- 313 次迭代代表了 1 个 epoch。
- 100 个 epochs 意味着模型将经过 31,300 次迭代,处理整个数据集 100 遍。

为什么这很重要:

1. Epochs 给出了模型看到整个数据集的次数。
2. 迭代给出了参数更新的次数。
3. 理解这两个概念有助于更好地控制训练过程,比如实施学习率衰减或早停。

**学习率：**

学习率的定义和作用:
学习率控制了在每次参数更新时,模型学习的"步长"。具体来说,它决定了在梯度下降过程中,参数向最优值移动的速度。

数学表示:
在梯度下降算法中,参数更新的公式为:
$$ θ = θ - η \* ∇J(θ) $$
其中:

- $θ$ 是模型参数
- $η$ 是学习率
- $∇J(θ)$ 是损失函数关于参数的梯度

**RNN CNN 输入参数对应关系：**

批次大小 B <<-->> 样本数 N

序列长度 S <<-->> 宽度和长度的乘积 HW

隐藏层大小 H <<-->>通道数 C

## 算子 IR

在 NPU（Neural Processing Unit）编程中，**算子 IR（Intermediate Representation， 中间表示）** 是指用于描述算子（Operators）的中间表达形式，**通常是计算图中的节点**，表示在深度学习框架中执行的单个计算单元。**算子 IR 是硬件加速器（如 NPU）与上层框架（如 TensorFlow、PyTorch）之间的桥梁，能够将高层次的神经网络模型映射到硬件的执行指令**。

### 算子 IR 的作用

1. **硬件抽象**：算子 IR 是一种中间层，将深度学习框架中的操作抽象出来，以便适配不同硬件（如 CPU、GPU、NPU）。不同硬件有不同的架构和指令集，算子 IR 使得框架可以通过一种标准化的表示来描述计算。
2. **算子优化**：在执行模型时，算子 IR 提供了一个优化的机会。通过优化中间表示，可以提高计算效率，最大化硬件资源的利用率，避免冗余计算。
3. **硬件指令生成**：算子 IR 是在高层模型定义和底层硬件指令之间的转换层，通过 IR，硬件可以理解并执行高层模型中的计算。

### 算子 IR 定义

算子 IR 通常会定义以下几个方面的信息：

1. **算子名称**：算子 IR 会定义算子的名称，如 `Conv2D`、`MatMul`、`ReLU` 等，这是每个算子在框架中的基础类型。
2. **输入/输出**：每个算子都会有输入和输出，通常由张量（Tensor）表示。IR 中会定义输入输出张量的维度、数据类型和布局（如 NCHW、NHWC 等）。
3. **属性（Attributes）**：算子可能会有一些特定的参数属性，比如卷积的窗口大小、步长、填充模式等。这些属性在 IR 中以键值对的形式定义。
4. **数据流依赖**：算子 IR 也会描述不同算子之间的依赖关系，表示某个算子的输出是另一个算子的输入，形成计算图。
5. **目标硬件信息**：某些情况下，IR 还会包含目标硬件的信息，比如是否要在 NPU 上执行，或者是否需要特定的硬件加速指令。

### 算子 IR 在 NPU 编程中的应用

NPU 编程中，深度学习模型会被编译成一系列算子 IR，然后通过编译器（如 Huawei MindSpore、Ascend 的编译器）将这些 IR 转换为 NPU 能够理解的指令和代码。

示例流程:

1. **高层框架模型**：用户在高层次框架（如 TensorFlow、PyTorch）中定义模型。
2. **算子映射和优化**：模型会被分解为基础的计算单元（算子），这些算子通过 IR 进行优化。
3. **硬件适配**：IR 被进一步转换为与 NPU 硬件相关的执行代码（如 TVM 中的 LLVM IR 或者 NPU 的特定指令集）。
4. **运行**：最终这些低层次指令在 NPU 上执行，实现模型的加速推理或训练。

### 总结

在 NPU 编程中，算子 IR 是深度学习模型的中间表示，起到将高层次框架中的算子映射到底层硬件的桥梁作用。通过 IR，编译器可以优化和调度算子，以充分利用硬件资源，实现高效的推理和训练。

## 模型文件格式

`.onnx`、`.pb`、和 `.pth` 文件是三种常见的机器学习模型文件格式，它们分别用于不同的深度学习框架和推理环境。以下是对它们的详细解释：

### `.onnx` 文件（Open Neural Network Exchange）

- **用途**: `.onnx` 是 Open Neural Network Exchange (ONNX) 的文件格式。它是一种开放的神经网络模型交换格式，旨在使不同深度学习框架（如 PyTorch、TensorFlow、MXNet、Caffe2 等）之间的模型互操作。
- **特点**：
  - ONNX 的目标是提供跨框架兼容性，使你可以在一个框架中训练模型，并在另一个框架中进行推理或部署。
  - `.onnx` 文件可以在支持 ONNX 格式的多种推理引擎中运行（如 ONNX Runtime、TensorRT 等），以实现更快的推理速度。
  - 广泛用于跨平台推理部署，如在云、边缘或移动设备上运行模型。
- **生成方法**: 可以通过支持的深度学习框架（如 PyTorch、TensorFlow 等）导出为 `.onnx` 文件。例如，PyTorch 中可以通过 `torch.onnx.export` 导出模型为 `.onnx` 格式。

### `.pb` 文件（Protocol Buffers）

- **用途**: `.pb` 文件是 TensorFlow 的模型格式，通常代表一个已训练好的 TensorFlow 模型。`.pb` 文件是 Protocol Buffers 格式的缩写，它是一种序列化数据的方式，TensorFlow 使用它来**保存计算图和权重**。
- **特点**：
  - `.pb` 文件可以是 TensorFlow 的静态计算图（Frozen Graph）或 SavedModel 的一部分。静态图是指图中的所有变量都已经转化为常量，用于高效推理。
  - TensorFlow 使用 `.pb` 文件进行模型保存和加载，这对于**模型部署和推理非常有用**。
  - TensorFlow Serving 或 TensorFlow Lite 等工具可以直接加载和运行 `.pb` 格式的模型。
- **生成方法**: 通过 TensorFlow 的 `tf.saved_model.save` 可以保存模型为 `SavedModel` 格式（包含 `.pb` 文件），或者通过 `tf.compat.v1.graph_util.convert_variables_to_constants` 可以将模型冻结并保存为 `.pb` 文件。

### `.pth` 文件（PyTorch）

- **用途**: `.pth` 文件是 PyTorch 框架中**保存模型权重或整个模型状态的文件格式**。PyTorch 使用 Python 的 `pickle` 序列化机制来存储模型的参数、优化器状态以及训练状态。
- **特点**：
  - `.pth` 文件通常用于保存模型的**权重**或**模型状态**，可以在训练结束后保存，也可以在训练过程中保存用于断点续训。
  - 与 ONNX 或 `.pb` 文件不同，`.pth` 文件依赖于 PyTorch 框架，并且模型的加载和推理也需要 PyTorch 环境。
  - `.pth` 文件可以保存整个模型（模型架构和参数）或仅保存模型参数。加载模型时，用户需要自定义模型类的定义来恢复模型。
- **生成方法**: 通过 PyTorch 的 `torch.save(model.state_dict(), 'model.pth')` 保存模型的权重，或者通过 `torch.save(model, 'model.pth')` 保存整个模型（包含架构和参数）。

### 总结对比

| 文件格式 | 框架                                   | 用途                                 | 典型使用场景                       |
| -------- | -------------------------------------- | ------------------------------------ | ---------------------------------- |
| `.onnx`  | 适用于多框架（PyTorch、TensorFlow 等） | 跨框架模型交换，部署推理             | 在一个框架中训练，另一个框架中推理 |
| `.pb`    | TensorFlow                             | 保存 TensorFlow 模型（计算图和权重） | TensorFlow 模型的部署和推理        |
| `.pth`   | PyTorch                                | 保存模型权重或状态                   | 保存和加载 PyTorch 模型            |

这些文件格式各有其适用场景：`.onnx` 用于跨框架模型交换，`.pb` 用于 TensorFlow 模型保存与部署，`.pth` 用于 PyTorch 模型的存储与加载。

## 训练和推理

在神经网络的领域，**训练**（Training）和**推理**（Inference）是两个核心的概念，它们分别对应神经网络模型的**学习过程**和**实际应用**阶段。

### 训练（Training）

**训练**是指通过已有的训练数据集，对神经网络进行学习的过程，使其能够自动调整内部参数（例如权重和偏置），从而更好地执行特定任务。训练的目标是让模型在数据上找到合适的模式，以便能够进行预测、分类、生成等任务。

**训练的过程**：

- **数据输入**：将输入数据（例如图像、文本或音频）喂入神经网络。
- **前向传播**（Forward Propagation）：输入数据依次通过神经网络的层，直到产生最终的输出结果。
- **损失计算**（Loss Calculation）：将网络输出结果与真实标签（或目标）进行比较，计算误差（称为损失函数）。
- **反向传播**（Backpropagation）：根据误差，通过梯度下降算法更新神经网络的参数（权重和偏置），以最小化损失函数。
- **迭代优化**：重复前向传播、损失计算、反向传播的步骤，直到模型在训练数据上达到满意的效果。

**常见优化方法**：

- **梯度下降算法**（Gradient Descent）
- **随机梯度下降**（Stochastic Gradient Descent, SGD）
- **Adam 优化器**（Adaptive Moment Estimation）

### 推理（Inference）

**推理**是指当模型训练完成后，使用模型进行预测或分类的过程。此时，神经网络的参数（权重和偏置）是固定的，不会再进行更新。推理的主要目标是根据新的输入数据，生成模型的输出结果。

**推理的过程**：

- **数据输入**：新的未见过的输入数据被喂入模型。
- **前向传播**：数据通过网络进行前向传播，生成输出。
- **结果输出**：模型根据训练时学习到的模式输出结果（例如分类标签、预测值等）。

推理过程相对较快，因为不涉及反向传播和参数更新。

**训练与推理的区别**：

- **目标**：训练的目的是优化模型的参数，使其能够正确地执行任务，而推理则是利用训练好的模型进行实际应用。
- **计算量**：训练通常涉及大量的数据和计算（如梯度计算），因此计算量大；推理则只需要前向传播，计算量相对较小。
- **更新参数**：训练过程中模型的参数会不断调整，推理时模型参数保持不变。

### 应用场景

- **训练**：通常在有大量标注数据的环境下进行，例如在数据中心或云计算平台上完成，要求较高的计算资源（如 GPU、TPU）。
- **推理**：通常发生在应用的实际场景中，可能是在移动设备、嵌入式设备或云端执行，目标是快速做出决策或预测。

总结来说，训练是让神经网络学会如何处理数据的过程，而推理则是使用已经训练好的模型来进行预测或决策。

## 量化反量化

在深度学习中，**神经网络量化（quantization）** 和 **神经网络反量化（dequantization）** 是两种与神经网络模型优化和推理效率提升相关的重要技术。

### 神经网络量化 (Quantization)

量化是指将神经网络中的**权重和激活值**从高精度（通常是 32 位浮点数）转换为较低精度（例如 8 位整数），以减少模型的存储和计算需求。**量化的主要目标是加速推理过程**，特别是在边缘设备或资源受限的硬件（如移动设备、嵌入式设备）上运行时。

1. 量化的主要类型：

   - **权重量化（Weight Quantization）**：将神经网络中存储的权重值从浮点数表示转为较低精度的整数表示，通常是 8 位整数。
   - **激活量化（Activation Quantization）**：将神经网络推理过程中生成的中间激活值从浮点数表示转为整数表示。

2. 量化方法：

   - **对称量化**：在零点对齐的情况下，正负方向的范围是对称的。所有值以相同的比例缩放，常用于整数量化。
   - **非对称量化**：正负方向的范围不同，可以适应不对称的数据分布。

3. 量化的优点：

   - **减少模型大小**：通过减少表示所需的位数，量化可以显著减小模型的存储需求。
   - **加速推理速度**：低精度的整数运算在很多硬件上比浮点数运算更快。
   - **降低内存带宽要求**：量化减少了内存访问时的开销，因为每次传输的数据量更少。

4. 量化的挑战：

   - **精度损失**：量化过程中可能会引入近似误差，从而导致模型精度下降，特别是在极端情况下（如 2bit、4bit 量化）。
   - **硬件支持**：并不是所有的硬件都原生支持低精度的运算，因此需要特定的硬件优化。

### 神经网络反量化 (Dequantization)

反量化是量化的逆过程，主要用于恢复量化后的值的近似浮点表示，以便在需要高精度计算时进行进一步处理。

1. 反量化通常用于以下场景：

   - **在推理时需要恢复高精度的表示**：例如在一些混合精度的计算中，部分操作可以在低精度下执行，而一些关键计算步骤可能需要恢复为高精度浮点数进行处理。
   - **在训练中模拟量化推理（即量化感知训练）**：在模型训练过程中，通常仍然使用高精度浮点数进行反向传播，但为了适应部署时的量化推理，可以在前向传播时应用模拟量化与反量化操作。

2. 反量化的步骤：

   1. **恢复缩放系数**：在量化时每个值都会使用特定的缩放因子，反量化时会用到这些缩放因子来将整数值还原为近似的浮点值。
   2. **重构原始值**：通过反量化，可以从量化表示（如 8 位整数）中恢复出接近 32 位浮点数的值，尽可能保留原始信息。

3. 反量化的优点：

   - **结合高精度运算**：可以在某些关键部分恢复浮点精度，确保不会丢失太多的模型精度。
   - **灵活性**：通过反量化，可以选择性地只对需要高精度的部分恢复，从而在性能和精度之间进行折衷。

### 在实践中的应用

- **量化感知训练（Quantization-Aware Training，QAT）**：这是神经网络量化的一种高级方法，训练过程考虑到了推理时量化带来的影响，因此能够更好地适应量化后的精度损失。
- **后训练量化（Post-Training Quantization，PTQ）**：指的是在训练完成后再对模型进行量化，适用于大多数场景，但可能会带来一定的精度损失。
- **混合精度推理**：在推理过程中，某些操作可以在低精度（量化值）下进行，而另一些重要操作可能需要反量化，恢复到高精度浮点数。

### 总结

- **量化** 是为了提升推理效率，通过降低权重和激活值的精度来加速计算和减少内存需求。
- **反量化** 则是为了在需要时恢复高精度表示，通常用于混合精度计算或训练过程中。

量化和反量化技术共同帮助神经网络在推理阶段实现性能与精度的平衡，尤其是在移动设备和边缘计算等资源有限的场景中非常有用。

## 量化基本知识

> [1]. [一起实践神经网络量化系列教程（一）！](https://oldpan.me/archives/how-to-quan-1)

实际点来说，量化就是将我们训练好的模型，不论是权重、还是计算 op，都转换为低精度去计算。因为 FP16 的量化很简单，所以实际中我们谈论的量化更多的是 INT8 的量化，当然也有 3-bit、4-bit 的量化，不过目前来说比较常见比较实用的，也就是 INT8 量化了，之后老潘的重点也是 INT8 量化。

1. `pre-tensor` 和 `pre-channel`

一般量化过程中，有 `pre-tensor` 和 `pre-channel` 两种方式。
`pre-tensor` 显而易见，就是对于同一块输入（比如某个卷积前的输入 tensor）我们采用一个 scale，该层所有的输入数据共享一个 scale 值；
`pre-channel`一般是作用于权重，比如一个卷积的权重维度是[64,3,3,3]（输入通道为 3 输出通道为 64，卷积核为 3x3），`pre-channel`就是会产生 64 个 scale 值，分别作用于该卷积权重参数的 64 个通道。

## 图计算中 pass

在图计算领域，**pass** 是指对计算图（或图结构）进行一系列**变换、优化或分析**的过程。计算图是由节点和边组成的结构，节点表示计算操作（如算术运算、函数调用等），边表示数据流或依赖关系。在图计算中，**pass** 用于改善图的性能、减少资源消耗、或是为后续的编译和执行做准备。

Pass 通常分为以下几类：

1. **优化 Pass**：对图进行优化，使得计算更加高效。
   - **常量折叠（Constant Folding）**：提前计算常量表达式，以减少运行时的计算。
   - **循环展开（Loop Unrolling）**：展开循环以减少循环控制开销并增加并行化的机会。
   - **节点合并（Node Merging）**：将多个节点操作合并为一个节点，减少中间结果的存储和传输。
   - **冗余计算消除（Dead Code Elimination）**：移除计算中不必要的部分，优化资源使用。
2. **分析 Pass**：对图进行分析，以获取某些信息。
   - **数据流分析（Data Flow Analysis）**：分析节点和边的依赖关系，以确定计算顺序或并行化的可能性。
   - **内存使用分析（Memory Usage Analysis）**：分析计算图中不同节点的内存需求，优化内存分配。
3. **变换 Pass**：改变图的结构，使其符合特定的执行环境或优化需求。
   - **图分割（Graph Partitioning）**：将计算图分割为若干个子图，适应不同硬件加速器或并行执行的需求。
   - **算子替换（Operator Replacement）**：将高开销的算子替换为性能更优的算子或等效的低成本操作。

每个 pass 通过对图的变换或分析，实现对图的不同层面的优化或调整。Pass 是一种模块化的操作，多个 pass 可以按顺序应用，以逐步提高计算图的效率和性能。

**NPU 中图优化**：

在 NPU（Neural Processing Unit）图计算中，如深度学习编译器框架（如 TensorFlow、TVM、ONNX、XLA 等）中，pass 是图优化流程中的一个阶段。常见的 pass 类型包括：

1. **常量折叠（Constant Folding Pass）**：将可以在编译时计算的常量操作提前计算，以减少运行时的计算负担。
2. **算子融合（Operator Fusion Pass）**：将多个简单的算子（操作符）合并为一个更复杂的算子，减少计算步骤和内存传输。
3. **冗余计算消除（Dead Code Elimination Pass）**：移除在计算中不会对输出产生影响的冗余操作，减少不必要的计算。
4. **内存优化（Memory Optimization Pass）**：优化内存使用，例如通过重用内存块或减少内存拷贝操作来降低内存需求。
5. **量化（Quantization Pass）**：将模型的浮点数权重和激活函数转换为低精度格式（如 INT8），以提高计算效率。
6. **调度优化（Scheduling Pass）**：重新安排计算顺序，使得能更好地利用 NPU 的并行能力和缓存。

这些 pass 操作通常是自动化的，并由编译器或框架在模型编译、优化阶段应用，以确保模型能够高效地在硬件上运行。
