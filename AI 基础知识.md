# 主流AI算子

1. Matmul : matrix multiply的缩写，是专门用于矩阵乘法的函数。
2. Conv2d : 二维卷积运算，常用于二维图像。相对应的还有一维卷积方法Conv1d，常用于文本数据的处理。
3. Relu : 整流线性单位函数（Rectified       Linear Unit, ReLU），又称修正线性单元，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。
    线性整流被认为有一定的生物学原理，并且由于在实践中通常有着比其他常用激活函数（如sigmiod/tanh）更好的效果。

4. Softmax : 一种激活函数，它可以将一个数值向量归一化为一个概率分布向量，且各个概率之和为1。Softmax可以用来作为神经网络的最后一层，用于多分类问题的输出。
5. Pooling : 卷积神经网络中常见的一种操作，Pooling层是模仿人的视觉系统对数据进行降维，其本质是降维。减小网络的模型参数量和计算成本，也在一定程度上降低过拟合的风险
6. Img2Col : 是一种实现卷积操作的加速计算策略。它能将卷积操作转化为 GEMM（通用矩阵乘 General      Matrix Multiply)，从而最大化地缩短卷积计算的时间。



# 术语

## 按通道/按元素

1. Channel-wise（按通道）：Channel-wise归一化是指对每个通道独立进行归一化操作。每个通道都有自己的均值和标准差。例如，在Batch       Normalization（BN）中，通常会对每个通道分别计算均值和标准差，然后进行归一化。
2. Element-wise（按元素）：Element-wise操作是对张量中的每个元素逐个进行处理。对于归一化来说，这意味着每个元素都应用相同的归一化参数，例如均值和标准差。

## NCHW与NHWC数据格式

流行的[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)框架中有不同的数据格式，典型的有NCHW和NHWC格式。本文从逻辑表达和物理存储角度用图的方式来理解这两种数据格式，最后以RGB图像为例来加深NHWC和NCHW数据存储格式的理解。



### 一、基本概念
深度学习框架中，数据一般是4D，用NCHW或NHWC表达，其中：

*   N - Batch
*   C - Channel
*   H - Height
*   W - Width

### 二、逻辑表达

假定N = 2，C = 16，H = 5，W = 4，那么这个4D数据，看起来是这样的：

![](https://i-blog.csdnimg.cn/blog_migrate/fe8a6e587ff4760a0d13c2c70aaf3340.png)

人类比较直接的理解方式是3D，上图中从三个方向上理解，C方向/H方向/W方向。然后是N方向上，就是4D。

上图中红色标准的数值是这个数据里每个元素的数值。

### 三、物理存储

无论逻辑表达上是几维的数据，在计算机中存储时都是按照1D来存储的。NCHW和NHWC格式数据的存储形式如下图所示：

![](https://i-blog.csdnimg.cn/blog_migrate/964468ffadc3c2aff2028f8a22ed91cf.png)

#### 3.1 NCHW

NCHW是先取W方向数据；然后H方向；再C方向；最后N方向。

所以，序列化出1D数据：

000 (W方向) 001 002 003，(H方向) 004 005 ... 019，(C方向) 020 ... 318 319，(N方向) 320 321 ...

#### 3.2 NHWC

NHWC是先取C方向数据；然后W方向；再H方向；最后N方向。

所以，序列化出1D数据：

000 (C方向) 020 ... 300，(W方向) 001 021 ... 303，(H方向) 004 ... 319，(N方向) 320 340 ...

### 四、RGB图像数据举例

表达RGB彩色图像时，一个像素的RGB值用3个数值表示，对应Channel为3。易于理解这里假定N=1，那么NCHW和NHWC数据格式可以很直接的这样表达：

![](https://i-blog.csdnimg.cn/blog_migrate/d21b1b8b279cf814e5a5c5bf45b67d83.png)

**NCHW**是

*   先在一个Channel面上把W方向|H方向上元素存储起来 // R
*   然后再在另一个Channel切面上把W方向|H方向上元素存储起来 // G
*   最后一个Channel切面上把W方向|H方向上元素存储起来 // B

这样看起来，就是先把R通道的每个像素都存储；然后存储G通道；再然后B通道。

**NHWC**是

*   先把3个Channel上元素存储起来 // 也就是一个像素的RGB
*   然后再在W方向|H方向上元素存储起来

这样看起来，就是顺序地取像素的RGB数值存储起来

### 五、不同框架支持

目前的主流ML框架对NCHW和NHWC数据格式做了支持，有些框架可以支持两种且用户未做设置时有一个缺省值：

*   TensorFlow：缺省NHWC，GPU也支持NCHW
*   Caffe：NCHW
*   PyTorch：NCHW



## CNN 和 RNN

卷积神经网络（CNN）和循环神经网络（RNN）是两种广泛应用于深度学习的神经网络，它们各自适用于不同类型的数据和任务。以下是它们的主要区别：

### 1. 数据类型

- **CNN**（Convolutional Neural Network）：**主要用于处理具有网格结构的数据**，如图像和视频。图像可以看作是二维的网格，而视频则可以看作是三维的网格（时间 + 空间）。
- **RNN**（Recurrent Neural Network）：**主要用于处理序列数据**，如时间序列、文本、语音信号等。RNN能够利用序列的顺序信息，通过其循环结构捕捉时间或序列中的依赖关系。

### 2. 结构

- **CNN**：采用卷积层和池化层交替堆叠的结构。卷积层用于提取特征，池化层用于降维和减小计算量。卷积操作具有局部连接和权重共享的特点，能够有效地捕捉局部特征。
- **RNN**：具有循环结构，网络中的每个节点不仅接收当前输入，还接收前一时刻的隐藏状态作为输入。这样可以将前一时刻的信息传递到下一时刻，从而捕捉序列中的时间依赖关系。

### 3. 主要应用

- **CNN**：广泛应用于计算机视觉任务，如图像分类、目标检测、图像分割等。
- **RNN**：广泛应用于自然语言处理（NLP）任务，如语言模型、机器翻译、情感分析等，以及时间序列预测、语音识别等。

### 4. 处理方式

- **CNN**：通过卷积核在输入数据上进行滑动窗口操作，提取局部区域的特征。其主要特点是局部连接、权重共享和池化操作。
- **RNN**：通过循环单元（如标准RNN单元、长短期记忆（LSTM）单元或门控循环单元（GRU））来处理序列数据，每个时间步的输出依赖于当前输入和前一时刻的隐藏状态。

### 5. 参数共享

- **CNN**：卷积核的参数在整个输入图像上共享，这减少了参数的数量，提高了计算效率。
- **RNN**：在每个时间步上共享参数，这使得RNN能够处理变长的序列。

### 6. 计算复杂度

- **CNN**：通常计算复杂度较低，适合并行计算，因为卷积操作可以在不同的局部区域独立进行。
- **RNN**：计算复杂度较高，因为每个时间步的计算依赖于前一个时间步，难以并行化。

### 总结

CNN和RNN各自擅长处理不同类型的数据和任务。CNN由于其卷积操作的特性，适合处理图像等网格结构数据；而RNN由于其循环结构，适合处理序列数据。两者在深度学习中的应用广泛且各有优势，可以根据具体任务的需求选择合适的模型。



## token embedding

Token embedding 是自然语言处理（NLP）中的一种技术，用于将文本数据中的单词、子词或字符（称为 token）转换为**稠密的向量表示**。这种向量表示能够捕捉文本中词语的**语义和语法信息**，并在机器学习模型中作为输入特征。Token embedding 的目标是将**高维、稀疏的离散表示**（如 one-hot 编码）转换为**低维、稠密的连续表示**，从而使得文本数据可以更高效地被处理和理解。

### 常见的 Token Embedding 方法

1. **One-hot Encoding**
   - 每个 token 用一个与词汇表大小相同的向量表示，向量中只有一个位置为1，其余位置为0。这种表示方式非常稀疏且高维，不适合直接用于复杂的模型。
2. **Word Embeddings**
   - **Word2Vec**：基于神经网络的模型，通过训练 Skip-gram 或 CBOW 模型生成词嵌入。它能够捕捉词语之间的语义关系，例如“国王”与“王后”之间的关系。
   - **GloVe**：基于共现矩阵的统计方法，通过矩阵分解生成词嵌入。它能够捕捉全局语义信息。
3. **Subword Embeddings**
   - **FastText**：在 Word2Vec 的基础上，考虑了词的子词（subword）信息，这样可以更好地处理未登录词（OOV）和词形变化。
4. **Contextualized Embeddings**
   - **ELMo**：基于双向 LSTM，通过上下文信息生成动态词嵌入。每个词的表示会根据其在句子中的上下文变化。
   - **BERT**：基于 Transformer 的双向编码器，通过预训练和微调方法生成词嵌入。它能够捕捉深层次的上下文信息，并且在许多 NLP 任务中表现出色。

### 示例

假设我们有一个句子：“I love natural language processing”。

1. **One-hot Encoding**

   ```
   I: [1, 0, 0, 0, 0, 0]
   love: [0, 1, 0, 0, 0, 0]
   natural: [0, 0, 1, 0, 0, 0]
   language: [0, 0, 0, 1, 0, 0]
   processing: [0, 0, 0, 0, 1, 0]
   ```

2. **Word2Vec Embedding** (假设嵌入维度为3)

   ```
   I: [0.2, -0.1, 0.7]
   love: [0.8, 0.3, -0.4]
   natural: [0.5, -0.2, 0.1]
   language: [0.3, 0.6, -0.5]
   processing: [-0.4, 0.2, 0.9]
   ```

3. **BERT Embedding**

   ```
   I: [0.1, 0.2, 0.3, ..., 0.8]
   love: [0.4, 0.5, 0.1, ..., 0.2]
   natural: [0.3, 0.1, 0.6, ..., 0.9]
   language: [0.5, 0.7, 0.2, ..., 0.1]
   processing: [0.2, 0.4, 0.8, ..., 0.6]
   ```

在实际应用中，token embedding 可以通过预训练好的嵌入模型（如 GloVe 或 BERT）来生成，或者通过在特定任务上微调这些预训练模型以获得更好的性能。



## RNN 模型中参数解释

以下是对批次大小、序列长度等参数的简要解释:

1. 批次大小 (Batch Size):
   批次大小指的是在**一次迭代中用于训练的样本数量**。较大的批次大小可以提高训练速度,但可能会影响模型的泛化能力。较小的批次大小可能会使训练更慢,但可能有助于模型更好地泛化。
2. 序列长度 (Sequence Length):
   序列长度是指RNN处理的**输入序列中的时间步数**。在自然语言处理任务中,这通常对应于一个句子或文档中的单词数。较长的序列可以捕捉更长期的依赖关系,但也会增加计算复杂度。
3. 隐藏层大小 (Hidden Layer Size):
   这是RNN**隐藏状态的维度**。较大的隐藏层可以捕捉更复杂的模式,但也会增加模型的参数数量和计算复杂度。
4. 学习率 (Learning Rate):
   学习率控制**每次参数更新的步长**。较高的学习率可能导致快速学习但也可能导致不稳定,而较低的学习率可能导致学习过慢。
5. 迭代次数 (Number of Epochs):
   一个epoch指的是**整个训练数据集被传递一次**。增加epoch数可以改善模型性能,但也可能导致过拟合。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   这个参数用于**防止梯度爆炸**问题。它限制了梯度的最大范数。
7. 丢弃率 (Dropout Rate):
   丢弃率是一种正则化技术,用于防止过拟合。它指定了在训练过程中随机"丢弃"（即设置为零）的神经元的比例。

**示例：**

1. 批次大小 (Batch Size):
   例子: 假设我们有一个包含**10,000个样本**的数据集。如果我们设置**批次大小为32**,那么在每次迭代中,模型会处理32个样本,并更新一次参数。这意味着完成一个epoch需要大约313次迭代Iteration(10,000/32≈313)。
2. 序列长度 (Sequence Length):
   例子: 在情感分析任务中,如果我们**设置序列长度为50**,这意味着我们会考虑每条评论的前50个词。如果评论短于50个词,我们会进行填充;如果长于50个词,我们会截断。
3. 隐藏层大小 (Hidden Layer Size):
   例子: 在一个语言模型中,如果我们设置隐藏层大小为256,这意味着RNN的**每个时间步都会产生一个256维的向量**来表示当前的隐藏状态。
4. 学习率 (Learning Rate):
   例子: 假设我们从0.01的学习率开始训练。如果发现模型学习不稳定,我们可能会降低到0.001。或者,我们可能使用学习率调度器,在训练过程中逐渐降低学习率,如从0.01开始,每5个epochs降低10%。
5. 迭代次数 (Number of Epochs):
   例子: 在训练一个机器翻译模型时,我们可能会**设置100个epochs**。但是,如果我们发现在第50个epoch后验证集性能不再提高,我们可能会提前停止训练。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   例子: 如果我们设置梯度裁剪阈值为5.0,这意味着如果计算出的梯度的L2范数大于5.0,我们会将其缩放,使其范数等于5.0。这**有助于防止梯度爆炸**。
7. 丢弃率 (Dropout Rate):
   例子: 在一个文本分类模型中,我们可能在RNN层后设置0.5的丢弃率。这意味着在每次训练迭代中,有50%的神经元输出会被随机设置为0,从而减少过拟合。
8. 网络架构:
   例子: 在一个命名实体识别任务中,我们可能使用双向LSTM。这允许模型不仅考虑单词的左侧上下文,还能考虑右侧上下文,从而提高识别准确率。
9. 注意力机制:
   例子: 在机器翻译任务中,当翻译长句子时,注意力机制允许模型在生成每个目标词时"关注"源句子的不同部分。例如,翻译"The cat sat on the mat"时,在生成"猫"这个词时,模型会主要关注源句子中的"cat"。
10. 词嵌入:
    例子: 在情感分析任务中,我们可能使用预训练的300维GloVe词向量。这意味着每个词都会被表示为一个300维的向量,这些向量已经在大规模文本语料上预先训练好,能捕捉词的语义信息。
11. 教师强制 (Teacher Forcing):
    例子: 在训练一个文本生成模型时,如果我们使用80%的教师强制率,这意味着在80%的时间里,模型会使用真实的前一个单词作为下一步的输入,而在20%的时间里使用自己生成的单词。这有助于模型学习正确的序列,同时也允许一些探索。

这些是RNN模型中的一些关键参数。根据具体任务和数据集,这些参数的最佳值可能会有所不同。通常需要通过实验和调优来找到最适合特定问题的参数设置。

**iteration和epoch的区别：**

1. 迭代 (Iteration):
   - 一次迭代是指使用一个批次(batch)的数据对模型进行一次前向传播和反向传播的过程。
   - 在每次迭代后,模型参数会更新一次。
2. Epoch:
   - 一个epoch是指整个训练数据集被完整地通过神经网络一次并返回。
   - 一个epoch包含了多个迭代,具体取决于批次大小和数据集大小。

让我们用一个具体的例子来说明:

例子:
假设我们有一个包含10,000个样本的数据集,我们选择的批次大小(batch size)是32。

计算:

- 每个epoch需要的迭代次数 = 数据集大小 / 批次大小
- 在这个例子中: 10,000 / 32 ≈ 313 (向上取整)

这意味着:

1. 每个epoch包含313次迭代。
2. 在每次迭代中,模型处理32个样本。
3. 完成313次迭代后,模型就处理了整个数据集一次,这就完成了一个epoch。

现在,如果我们设置训练为100个epochs:

- 总迭代次数 = 每个epoch的迭代次数 × epoch数
- 在这个例子中: 313 × 100 = 31,300次迭代

因此:

- 313次迭代代表了1个epoch。
- 100个epochs意味着模型将经过31,300次迭代,处理整个数据集100遍。

为什么这很重要:

1. Epochs给出了模型看到整个数据集的次数。
2. 迭代给出了参数更新的次数。
3. 理解这两个概念有助于更好地控制训练过程,比如实施学习率衰减或早停。

**学习率：**

学习率的定义和作用:
学习率控制了在每次参数更新时,模型学习的"步长"。具体来说,它决定了在梯度下降过程中,参数向最优值移动的速度。

数学表示:
在梯度下降算法中,参数更新的公式为:
θ = θ - η * ∇J(θ)
其中:
θ 是模型参数
η 是学习率
∇J(θ) 是损失函数关于参数的梯度



**RNN CNN输入参数对应关系：**

批次大小B <<-->> 样本数N

序列长度S <<-->> 宽度和长度的乘积HW

隐藏层大小H <<-->>通道数C



## 算子 IR

在 NPU（Neural Processing Unit）编程中，**算子 IR（Intermediate Representation， 中间表示）** 是指用于描述算子（Operators）的中间表达形式，**通常是计算图中的节点**，表示在深度学习框架中执行的单个计算单元。**算子 IR 是硬件加速器（如 NPU）与上层框架（如 TensorFlow、PyTorch）之间的桥梁，能够将高层次的神经网络模型映射到硬件的执行指令**。

### **算子 IR 的作用**

1. **硬件抽象**：算子 IR 是一种中间层，将深度学习框架中的操作抽象出来，以便适配不同硬件（如 CPU、GPU、NPU）。不同硬件有不同的架构和指令集，算子 IR 使得框架可以通过一种标准化的表示来描述计算。
2. **算子优化**：在执行模型时，算子 IR 提供了一个优化的机会。通过优化中间表示，可以提高计算效率，最大化硬件资源的利用率，避免冗余计算。
3. **硬件指令生成**：算子 IR 是在高层模型定义和底层硬件指令之间的转换层，通过 IR，硬件可以理解并执行高层模型中的计算。

### **算子 IR 定义**

算子 IR 通常会定义以下几个方面的信息：

1. **算子名称**：算子 IR 会定义算子的名称，如 `Conv2D`、`MatMul`、`ReLU` 等，这是每个算子在框架中的基础类型。
2. **输入/输出**：每个算子都会有输入和输出，通常由张量（Tensor）表示。IR 中会定义输入输出张量的维度、数据类型和布局（如 NCHW、NHWC 等）。
3. **属性（Attributes）**：算子可能会有一些特定的参数属性，比如卷积的窗口大小、步长、填充模式等。这些属性在 IR 中以键值对的形式定义。
4. **数据流依赖**：算子 IR 也会描述不同算子之间的依赖关系，表示某个算子的输出是另一个算子的输入，形成计算图。
5. **目标硬件信息**：某些情况下，IR 还会包含目标硬件的信息，比如是否要在 NPU 上执行，或者是否需要特定的硬件加速指令。

### **算子 IR 在 NPU 编程中的应用**

NPU 编程中，深度学习模型会被编译成一系列算子 IR，然后通过编译器（如 Huawei MindSpore、Ascend 的编译器）将这些 IR 转换为 NPU 能够理解的指令和代码。

#### 示例流程：

1. **高层框架模型**：用户在高层次框架（如 TensorFlow、PyTorch）中定义模型。
2. **算子映射和优化**：模型会被分解为基础的计算单元（算子），这些算子通过 IR 进行优化。
3. **硬件适配**：IR 被进一步转换为与 NPU 硬件相关的执行代码（如 TVM 中的 LLVM IR 或者 NPU 的特定指令集）。
4. **运行**：最终这些低层次指令在 NPU 上执行，实现模型的加速推理或训练。

### **总结**

在 NPU 编程中，算子 IR 是深度学习模型的中间表示，起到将高层次框架中的算子映射到底层硬件的桥梁作用。通过 IR，编译器可以优化和调度算子，以充分利用硬件资源，实现高效的推理和训练。



## 模型文件格式

`.onnx`、`.pb`、和 `.pth` 文件是三种常见的机器学习模型文件格式，它们分别用于不同的深度学习框架和推理环境。以下是对它们的详细解释：

### 1. **.onnx 文件**（Open Neural Network Exchange）

- **用途**: `.onnx` 是 Open Neural Network Exchange (ONNX) 的文件格式。它是一种开放的神经网络模型交换格式，旨在使不同深度学习框架（如 PyTorch、TensorFlow、MXNet、Caffe2 等）之间的模型互操作。
- **特点**：
  - ONNX 的目标是提供跨框架兼容性，使你可以在一个框架中训练模型，并在另一个框架中进行推理或部署。
  - `.onnx` 文件可以在支持 ONNX 格式的多种推理引擎中运行（如 ONNX Runtime、TensorRT 等），以实现更快的推理速度。
  - 广泛用于跨平台推理部署，如在云、边缘或移动设备上运行模型。
- **生成方法**: 可以通过支持的深度学习框架（如 PyTorch、TensorFlow 等）导出为 `.onnx` 文件。例如，PyTorch 中可以通过 `torch.onnx.export` 导出模型为 `.onnx` 格式。

### 2. **.pb 文件**（Protocol Buffers）

- **用途**: `.pb` 文件是 TensorFlow 的模型格式，通常代表一个已训练好的 TensorFlow 模型。`.pb` 文件是 Protocol Buffers 格式的缩写，它是一种序列化数据的方式，TensorFlow 使用它来**保存计算图和权重**。
- **特点**：
  - `.pb` 文件可以是 TensorFlow 的静态计算图（Frozen Graph）或 SavedModel 的一部分。静态图是指图中的所有变量都已经转化为常量，用于高效推理。
  - TensorFlow 使用 `.pb` 文件进行模型保存和加载，这对于**模型部署和推理非常有用**。
  - TensorFlow Serving 或 TensorFlow Lite 等工具可以直接加载和运行 `.pb` 格式的模型。
- **生成方法**: 通过 TensorFlow 的 `tf.saved_model.save` 可以保存模型为 `SavedModel` 格式（包含 `.pb` 文件），或者通过 `tf.compat.v1.graph_util.convert_variables_to_constants` 可以将模型冻结并保存为 `.pb` 文件。

### 3. **.pth 文件**（PyTorch）

- **用途**: `.pth` 文件是 PyTorch 框架中**保存模型权重或整个模型状态的文件格式**。PyTorch 使用 Python 的 `pickle` 序列化机制来存储模型的参数、优化器状态以及训练状态。
- **特点**：
  - `.pth` 文件通常用于保存模型的**权重**或**模型状态**，可以在训练结束后保存，也可以在训练过程中保存用于断点续训。
  - 与 ONNX 或 `.pb` 文件不同，`.pth` 文件依赖于 PyTorch 框架，并且模型的加载和推理也需要 PyTorch 环境。
  - `.pth` 文件可以保存整个模型（模型架构和参数）或仅保存模型参数。加载模型时，用户需要自定义模型类的定义来恢复模型。
- **生成方法**: 通过 PyTorch 的 `torch.save(model.state_dict(), 'model.pth')` 保存模型的权重，或者通过 `torch.save(model, 'model.pth')` 保存整个模型（包含架构和参数）。

### 总结对比：

| 文件格式 | 框架                                   | 用途                                 | 典型使用场景                       |
| -------- | -------------------------------------- | ------------------------------------ | ---------------------------------- |
| `.onnx`  | 适用于多框架（PyTorch、TensorFlow 等） | 跨框架模型交换，部署推理             | 在一个框架中训练，另一个框架中推理 |
| `.pb`    | TensorFlow                             | 保存 TensorFlow 模型（计算图和权重） | TensorFlow 模型的部署和推理        |
| `.pth`   | PyTorch                                | 保存模型权重或状态                   | 保存和加载 PyTorch 模型            |

这些文件格式各有其适用场景：`.onnx` 用于跨框架模型交换，`.pb` 用于 TensorFlow 模型保存与部署，`.pth` 用于 PyTorch 模型的存储与加载。
