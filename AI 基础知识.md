[toc]

# 主流 AI 算子

1. Matmul : matrix multiply 的缩写，是专门用于矩阵乘法的函数。
2. Conv2d : 二维卷积运算，常用于二维图像。相对应的还有一维卷积方法 Conv1d，常用于文本数据的处理。
3. Relu : 整流线性单位函数（Rectified Linear Unit, ReLU），又称修正线性单元，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。
   线性整流被认为有一定的生物学原理，并且由于在实践中通常有着比其他常用激活函数（如 sigmiod/tanh）更好的效果。

4. Softmax : 一种激活函数，它可以将一个数值向量归一化为一个概率分布向量，且各个概率之和为 1。Softmax 可以用来作为神经网络的最后一层，用于多分类问题的输出。
5. Pooling : 卷积神经网络中常见的一种操作，Pooling 层是模仿人的视觉系统对数据进行降维，其本质是降维。减小网络的模型参数量和计算成本，也在一定程度上降低过拟合的风险
6. Img2Col : 是一种实现卷积操作的加速计算策略。它能将卷积操作转化为 GEMM（通用矩阵乘 General Matrix Multiply)，从而最大化地缩短卷积计算的时间。

## MM 和 GEMM

MM 和 GEMM 都是大模型中常用的矩阵运算，但它们在数学上有一些重要的区别:

1. **MM (Matrix Multiplication)**:
   <!-- prettier-ignore -->
   MM 指的是标准的矩阵乘法。对于两个矩阵 $ A_{m \times p} $ 和 $ B_{p \times n} $，它们的乘积 $ C_{m \times n} = AB $ 是一个 $ m \times n $ 的矩阵。
   数学表达式:
   $$ C_{(i, j)} = \Sigma_{k=1}^n A_{(i, k)} \times B_{(k, j)} $$

2. **GEMM (General Matrix Multiplication)**:

   GEMM 是一种更通用的矩阵运算,全称为"General Matrix Multiply"。它的形式是:

   $$ C = \alpha AB + \beta C $$

   其中 $ \alpha, \beta $ 是标量，$ A, B, C $ 是矩阵。

3. **主要区别**:

   1. 操作复杂度:
      - MM 只进行矩阵乘法
      - GEMM 除了矩阵乘法外，还包含缩放($ \alpha $)和累加($ \beta $)操作
   2. 灵活性:
      - MM 是 GEMM 的一个特例(当 $ \alpha = 1, \beta = 0 $ 时)
      - GEMM 允许更灵活的矩阵运算组合
   3. 性能优化:
      - GEMM 通常有更优化的实现，因为它可以一次性完成多个操作,减少内存访问
   4. 应用场景:
      - MM 在简单的矩阵乘法中使用
      - GEMM 在需要频繁进行矩阵乘加操作的场景中更有优势，如深度学习中的全连接层或卷积层
   5. 数值稳定性:
      - GEMM 由于其缩放和累加操作，在某些情况下可能提供更好的数值稳定性

   在大模型中，GEMM 通常更受欢迎，因为它能提供更高的计算效率和更大的灵活性。但具体使用哪种方法还要根据实际需求和硬件支持情况来决定。

4. **SGEMM 和 DGEMM**：
   GEMM 分成双精度（DGEMM）和单精度（SGEMM）两个版本，这两个版本的参数是一致的，只不过在一些参数类型上是 double 和 float 的区别。

   1. **SGEMM (Single precision General Matrix Multiply)**:

      SGEMM 是用于单精度浮点数(32 位浮点数，也称为 float)的 GEMM 操作。特点:

      - 使用 32 位浮点数进行计算
      - occupies less memory
      - 计算速度通常比 DGEMM 快
      - 精度相对较低，适用于对精度要求不是特别高的场景

   2. **DGEMM (Double precision General Matrix Multiply)**:

      DGEMM 是用于双精度浮点数(64 位浮点数，也称为 double)的 GEMM 操作。特点:

      - 使用 64 位浮点数进行计算
      - 占用更多内存
      - 计算速度通常比 SGEMM 慢
      - 精度更高,适用于需要高精度计算的场景

   3. **主要区别**:

      1. 精度:
         - SGEMM: 约 7 位十进制精度
         - DGEMM: 约 15-17 位十进制精度
      2. 内存使用:
         - SGEMM 使用的内存大约是 DGEMM 的一半
      3. 计算速度:
         - 在相同硬件上，SGEMM 通常比 DGEMM 快 1.5-2 倍
      4. 适用场景:
         - SGEMM: 深度学习训练和推理、图形处理等对速度要求高但对精度要求相对较低的场景
         - DGEMM: 科学计算、金融模型等需要高精度的场景
      5. 硬件支持:
         - 现代 GPU 通常对 SGEMM 有更好的优化
         - 一些专门的 AI 加速器可能只支持或主要优化 SGEMM

   4. **在大模型中的应用**:

      1. 训练阶段:
         - 通常使用 SGEMM，因为它能提供足够的精度，同时具有更快的速度和更低的内存占用
      2. 推理阶段:
         - 大多数情况下使用 SGEMM
         - 在一些需要高精度的特殊应用中可能使用 DGEMM
      3. 混合精度训练:
         - 有时会结合使用 SGEMM 和 DGEMM，以平衡精度和性能

      选择使用 SGEMM 还是 DGEMM 通常需要在精度、速度和内存使用之间做权衡。在大多数深度学习应用中，SGEMM 已经能提供足够的精度，同时带来显著的性能提升。

# 性能优化

[CUDA SGEMM 优化笔记](https://linn-ylz.com/Computer-Science/CUDA/CUDA-SGEMM-optimization-notes/)

# 术语

## 按通道/按元素

1. Channel-wise（按通道）：Channel-wise 归一化是指对每个通道独立进行归一化操作。每个通道都有自己的均值和标准差。例如，在 Batch Normalization（BN）中，通常会对每个通道分别计算均值和标准差，然后进行归一化。
2. Element-wise（按元素）：Element-wise 操作是对张量中的每个元素逐个进行处理。对于归一化来说，这意味着每个元素都应用相同的归一化参数，例如均值和标准差。

## NCHW 与 NHWC 数据格式

流行的[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)框架中有不同的数据格式，典型的有 NCHW 和 NHWC 格式。本文从逻辑表达和物理存储角度用图的方式来理解这两种数据格式，最后以 RGB 图像为例来加深 NHWC 和 NCHW 数据存储格式的理解。

### 一、基本概念

深度学习框架中，数据一般是 4D，用 NCHW 或 NHWC 表达，其中：

- N - Batch
- C - Channel
- H - Height
- W - Width

### 二、逻辑表达

假定 N = 2，C = 16，H = 5，W = 4，那么这个 4D 数据，看起来是这样的：

![](https://i-blog.csdnimg.cn/blog_migrate/fe8a6e587ff4760a0d13c2c70aaf3340.png)

人类比较直接的理解方式是 3D，上图中从三个方向上理解，C 方向/H 方向/W 方向。然后是 N 方向上，就是 4D。

上图中红色标准的数值是这个数据里每个元素的数值。

### 三、物理存储

无论逻辑表达上是几维的数据，在计算机中存储时都是按照 1D 来存储的。NCHW 和 NHWC 格式数据的存储形式如下图所示：

![](https://i-blog.csdnimg.cn/blog_migrate/964468ffadc3c2aff2028f8a22ed91cf.png)

1. NCHW

   NCHW 是先取 W 方向数据；然后 H 方向；再 C 方向；最后 N 方向。

   所以，序列化出 1D 数据：

   000 (W 方向) 001 002 003，(H 方向) 004 005 ... 019，(C 方向) 020 ... 318 319，(N 方向) 320 321 ...

2. NHWC

   NHWC 是先取 C 方向数据；然后 W 方向；再 H 方向；最后 N 方向。

   所以，序列化出 1D 数据：

   000 (C 方向) 020 ... 300，(W 方向) 001 021 ... 303，(H 方向) 004 ... 319，(N 方向) 320 340 ...

### 四、RGB 图像数据举例

表达 RGB 彩色图像时，一个像素的 RGB 值用 3 个数值表示，对应 Channel 为 3。易于理解这里假定 N=1，那么 NCHW 和 NHWC 数据格式可以很直接的这样表达：

![](https://i-blog.csdnimg.cn/blog_migrate/d21b1b8b279cf814e5a5c5bf45b67d83.png)

**NCHW**是

- 先在一个 Channel 面上把 W 方向|H 方向上元素存储起来 // R
- 然后再在另一个 Channel 切面上把 W 方向|H 方向上元素存储起来 // G
- 最后一个 Channel 切面上把 W 方向|H 方向上元素存储起来 // B

这样看起来，就是先把 R 通道的每个像素都存储；然后存储 G 通道；再然后 B 通道。

**NHWC**是

- 先把 3 个 Channel 上元素存储起来 // 也就是一个像素的 RGB
- 然后再在 W 方向|H 方向上元素存储起来

这样看起来，就是顺序地取像素的 RGB 数值存储起来

### 五、不同框架支持

目前的主流 ML 框架对 NCHW 和 NHWC 数据格式做了支持，有些框架可以支持两种且用户未做设置时有一个缺省值：

- TensorFlow：缺省 NHWC，GPU 也支持 NCHW
- Caffe：NCHW
- PyTorch：NCHW

## CNN 和 RNN

卷积神经网络（CNN）和循环神经网络（RNN）是两种广泛应用于深度学习的神经网络，它们各自适用于不同类型的数据和任务。以下是它们的主要区别：

1. **数据类型**

   - **CNN**（Convolutional Neural Network）：**主要用于处理具有网格结构的数据**，如图像和视频。图像可以看作是二维的网格，而视频则可以看作是三维的网格（时间 + 空间）。
   - **RNN**（Recurrent Neural Network）：**主要用于处理序列数据**，如时间序列、文本、语音信号等。RNN 能够利用序列的顺序信息，通过其循环结构捕捉时间或序列中的依赖关系。

2. **结构**

   - **CNN**：采用卷积层和池化层交替堆叠的结构。卷积层用于提取特征，池化层用于降维和减小计算量。卷积操作具有局部连接和权重共享的特点，能够有效地捕捉局部特征。
   - **RNN**：具有循环结构，网络中的每个节点不仅接收当前输入，还接收前一时刻的隐藏状态作为输入。这样可以将前一时刻的信息传递到下一时刻，从而捕捉序列中的时间依赖关系。

3. **主要应用**

   - **CNN**：广泛应用于计算机视觉任务，如图像分类、目标检测、图像分割等。
   - **RNN**：广泛应用于自然语言处理（NLP）任务，如语言模型、机器翻译、情感分析等，以及时间序列预测、语音识别等。

4. **处理方式**

   - **CNN**：通过卷积核在输入数据上进行滑动窗口操作，提取局部区域的特征。其主要特点是局部连接、权重共享和池化操作。
   - **RNN**：通过循环单元（如标准 RNN 单元、长短期记忆（LSTM）单元或门控循环单元（GRU））来处理序列数据，每个时间步的输出依赖于当前输入和前一时刻的隐藏状态。

5. **参数共享**

   - **CNN**：卷积核的参数在整个输入图像上共享，这减少了参数的数量，提高了计算效率。
   - **RNN**：在每个时间步上共享参数，这使得 RNN 能够处理变长的序列。

6. **计算复杂度**

   - **CNN**：通常计算复杂度较低，适合并行计算，因为卷积操作可以在不同的局部区域独立进行。
   - **RNN**：计算复杂度较高，因为每个时间步的计算依赖于前一个时间步，难以并行化。

7. **总结**

   CNN 和 RNN 各自擅长处理不同类型的数据和任务。CNN 由于其卷积操作的特性，适合处理图像等网格结构数据；而 RNN 由于其循环结构，适合处理序列数据。两者在深度学习中的应用广泛且各有优势，可以根据具体任务的需求选择合适的模型。

## token embedding

Token embedding 是自然语言处理（NLP）中的一种技术，用于将文本数据中的单词、子词或字符（称为 token）转换为**稠密的向量表示**。这种向量表示能够捕捉文本中词语的**语义和语法信息**，并在机器学习模型中作为输入特征。Token embedding 的目标是将**高维、稀疏的离散表示**（如 one-hot 编码）转换为**低维、稠密的连续表示**，从而使得文本数据可以更高效地被处理和理解。

### 常见的 Token Embedding 方法

1. **One-hot Encoding**
   - 每个 token 用一个与词汇表大小相同的向量表示，向量中只有一个位置为 1，其余位置为 0。这种表示方式非常稀疏且高维，不适合直接用于复杂的模型。
2. **Word Embeddings**
   - **Word2Vec**：基于神经网络的模型，通过训练 Skip-gram 或 CBOW 模型生成词嵌入。它能够捕捉词语之间的语义关系，例如“国王”与“王后”之间的关系。
   - **GloVe**：基于共现矩阵的统计方法，通过矩阵分解生成词嵌入。它能够捕捉全局语义信息。
3. **Subword Embeddings**
   - **FastText**：在 Word2Vec 的基础上，考虑了词的子词（subword）信息，这样可以更好地处理未登录词（OOV）和词形变化。
4. **Contextualized Embeddings**
   - **ELMo**：基于双向 LSTM，通过上下文信息生成动态词嵌入。每个词的表示会根据其在句子中的上下文变化。
   - **BERT**：基于 Transformer 的双向编码器，通过预训练和微调方法生成词嵌入。它能够捕捉深层次的上下文信息，并且在许多 NLP 任务中表现出色。

### 示例

假设我们有一个句子：“I love natural language processing”。

1. **One-hot Encoding**

   ```log
   I: [1, 0, 0, 0, 0, 0]
   love: [0, 1, 0, 0, 0, 0]
   natural: [0, 0, 1, 0, 0, 0]
   language: [0, 0, 0, 1, 0, 0]
   processing: [0, 0, 0, 0, 1, 0]
   ```

2. **Word2Vec Embedding** (假设嵌入维度为 3)

   ```log
   I: [0.2, -0.1, 0.7]
   love: [0.8, 0.3, -0.4]
   natural: [0.5, -0.2, 0.1]
   language: [0.3, 0.6, -0.5]
   processing: [-0.4, 0.2, 0.9]
   ```

3. **BERT Embedding**

   ```log
   I: [0.1, 0.2, 0.3, ..., 0.8]
   love: [0.4, 0.5, 0.1, ..., 0.2]
   natural: [0.3, 0.1, 0.6, ..., 0.9]
   language: [0.5, 0.7, 0.2, ..., 0.1]
   processing: [0.2, 0.4, 0.8, ..., 0.6]
   ```

在实际应用中，token embedding 可以通过预训练好的嵌入模型（如 GloVe 或 BERT）来生成，或者通过在特定任务上微调这些预训练模型以获得更好的性能。

## RNN 模型中参数解释

以下是对批次大小、序列长度等参数的简要解释:

1. 批次大小 (Batch Size):
   批次大小指的是在**一次迭代中用于训练的样本数量**。较大的批次大小可以提高训练速度,但可能会影响模型的泛化能力。较小的批次大小可能会使训练更慢,但可能有助于模型更好地泛化。
2. 序列长度 (Sequence Length):
   序列长度是指 RNN 处理的**输入序列中的时间步数**。在自然语言处理任务中,这通常对应于一个句子或文档中的单词数。较长的序列可以捕捉更长期的依赖关系,但也会增加计算复杂度。
3. 隐藏层大小 (Hidden Layer Size):
   这是 RNN**隐藏状态的维度**。较大的隐藏层可以捕捉更复杂的模式,但也会增加模型的参数数量和计算复杂度。
4. 学习率 (Learning Rate):
   学习率控制**每次参数更新的步长**。较高的学习率可能导致快速学习但也可能导致不稳定,而较低的学习率可能导致学习过慢。
5. 迭代次数 (Number of Epochs):
   一个 epoch 指的是**整个训练数据集被传递一次**。增加 epoch 数可以改善模型性能,但也可能导致过拟合。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   这个参数用于**防止梯度爆炸**问题。它限制了梯度的最大范数。
7. 丢弃率 (Dropout Rate):
   丢弃率是一种正则化技术,用于防止过拟合。它指定了在训练过程中随机"丢弃"（即设置为零）的神经元的比例。

**示例：**

1. 批次大小 (Batch Size):
   例子: 假设我们有一个包含**10,000 个样本**的数据集。如果我们设置**批次大小为 32**,那么在每次迭代中,模型会处理 32 个样本,并更新一次参数。这意味着完成一个 epoch 需要大约 313 次迭代 Iteration(10,000/32≈313)。
2. 序列长度 (Sequence Length):
   例子: 在情感分析任务中,如果我们**设置序列长度为 50**,这意味着我们会考虑每条评论的前 50 个词。如果评论短于 50 个词,我们会进行填充;如果长于 50 个词,我们会截断。
3. 隐藏层大小 (Hidden Layer Size):
   例子: 在一个语言模型中,如果我们设置隐藏层大小为 256,这意味着 RNN 的**每个时间步都会产生一个 256 维的向量**来表示当前的隐藏状态。
4. 学习率 (Learning Rate):
   例子: 假设我们从 0.01 的学习率开始训练。如果发现模型学习不稳定,我们可能会降低到 0.001。或者,我们可能使用学习率调度器,在训练过程中逐渐降低学习率,如从 0.01 开始,每 5 个 epochs 降低 10%。
5. 迭代次数 (Number of Epochs):
   例子: 在训练一个机器翻译模型时,我们可能会**设置 100 个 epochs**。但是,如果我们发现在第 50 个 epoch 后验证集性能不再提高,我们可能会提前停止训练。
6. 梯度裁剪阈值 (Gradient Clipping Threshold):
   例子: 如果我们设置梯度裁剪阈值为 5.0,这意味着如果计算出的梯度的 L2 范数大于 5.0,我们会将其缩放,使其范数等于 5.0。这**有助于防止梯度爆炸**。
7. 丢弃率 (Dropout Rate):
   例子: 在一个文本分类模型中,我们可能在 RNN 层后设置 0.5 的丢弃率。这意味着在每次训练迭代中,有 50%的神经元输出会被随机设置为 0,从而减少过拟合。
8. 网络架构:
   例子: 在一个命名实体识别任务中,我们可能使用双向 LSTM。这允许模型不仅考虑单词的左侧上下文,还能考虑右侧上下文,从而提高识别准确率。
9. 注意力机制:
   例子: 在机器翻译任务中,当翻译长句子时,注意力机制允许模型在生成每个目标词时"关注"源句子的不同部分。例如,翻译"The cat sat on the mat"时,在生成"猫"这个词时,模型会主要关注源句子中的"cat"。
10. 词嵌入:
    例子: 在情感分析任务中,我们可能使用预训练的 300 维 GloVe 词向量。这意味着每个词都会被表示为一个 300 维的向量,这些向量已经在大规模文本语料上预先训练好,能捕捉词的语义信息。
11. 教师强制 (Teacher Forcing):
    例子: 在训练一个文本生成模型时,如果我们使用 80%的教师强制率,这意味着在 80%的时间里,模型会使用真实的前一个单词作为下一步的输入,而在 20%的时间里使用自己生成的单词。这有助于模型学习正确的序列,同时也允许一些探索。

这些是 RNN 模型中的一些关键参数。根据具体任务和数据集,这些参数的最佳值可能会有所不同。通常需要通过实验和调优来找到最适合特定问题的参数设置。

**iteration 和 epoch 的区别：**

1. 迭代 (Iteration):
   - 一次迭代是指使用一个批次(batch)的数据对模型进行一次前向传播和反向传播的过程。
   - 在每次迭代后,模型参数会更新一次。
2. Epoch:
   - 一个 epoch 是指整个训练数据集被完整地通过神经网络一次并返回。
   - 一个 epoch 包含了多个迭代,具体取决于批次大小和数据集大小。

让我们用一个具体的例子来说明:

例子:
假设我们有一个包含 10,000 个样本的数据集,我们选择的批次大小(batch size)是 32。

计算:

- 每个 epoch 需要的迭代次数 = 数据集大小 / 批次大小
- 在这个例子中: 10,000 / 32 ≈ 313 (向上取整)

这意味着:

1. 每个 epoch 包含 313 次迭代。
2. 在每次迭代中,模型处理 32 个样本。
3. 完成 313 次迭代后,模型就处理了整个数据集一次,这就完成了一个 epoch。

现在,如果我们设置训练为 100 个 epochs:

- 总迭代次数 = 每个 epoch 的迭代次数 × epoch 数
- 在这个例子中: 313 × 100 = 31,300 次迭代

因此:

- 313 次迭代代表了 1 个 epoch。
- 100 个 epochs 意味着模型将经过 31,300 次迭代,处理整个数据集 100 遍。

为什么这很重要:

1. Epochs 给出了模型看到整个数据集的次数。
2. 迭代给出了参数更新的次数。
3. 理解这两个概念有助于更好地控制训练过程,比如实施学习率衰减或早停。

**学习率：**

学习率的定义和作用:
学习率控制了在每次参数更新时,模型学习的"步长"。具体来说,它决定了在梯度下降过程中,参数向最优值移动的速度。

数学表示:
在梯度下降算法中,参数更新的公式为:
$$ θ = θ - η \* ∇J(θ) $$
其中:

- $θ$ 是模型参数
- $η$ 是学习率
- $∇J(θ)$ 是损失函数关于参数的梯度

**RNN CNN 输入参数对应关系：**

批次大小 B <<-->> 样本数 N

序列长度 S <<-->> 宽度和长度的乘积 HW

隐藏层大小 H <<-->>通道数 C

## 算子 IR

在 NPU（Neural Processing Unit）编程中，**算子 IR（Intermediate Representation， 中间表示）** 是指用于描述算子（Operators）的中间表达形式，**通常是计算图中的节点**，表示在深度学习框架中执行的单个计算单元。**算子 IR 是硬件加速器（如 NPU）与上层框架（如 TensorFlow、PyTorch）之间的桥梁，能够将高层次的神经网络模型映射到硬件的执行指令**。

### 算子 IR 的作用

1. **硬件抽象**：算子 IR 是一种中间层，将深度学习框架中的操作抽象出来，以便适配不同硬件（如 CPU、GPU、NPU）。不同硬件有不同的架构和指令集，算子 IR 使得框架可以通过一种标准化的表示来描述计算。
2. **算子优化**：在执行模型时，算子 IR 提供了一个优化的机会。通过优化中间表示，可以提高计算效率，最大化硬件资源的利用率，避免冗余计算。
3. **硬件指令生成**：算子 IR 是在高层模型定义和底层硬件指令之间的转换层，通过 IR，硬件可以理解并执行高层模型中的计算。

### 算子 IR 定义

算子 IR 通常会定义以下几个方面的信息：

1. **算子名称**：算子 IR 会定义算子的名称，如 `Conv2D`、`MatMul`、`ReLU` 等，这是每个算子在框架中的基础类型。
2. **输入/输出**：每个算子都会有输入和输出，通常由张量（Tensor）表示。IR 中会定义输入输出张量的维度、数据类型和布局（如 NCHW、NHWC 等）。
3. **属性（Attributes）**：算子可能会有一些特定的参数属性，比如卷积的窗口大小、步长、填充模式等。这些属性在 IR 中以键值对的形式定义。
4. **数据流依赖**：算子 IR 也会描述不同算子之间的依赖关系，表示某个算子的输出是另一个算子的输入，形成计算图。
5. **目标硬件信息**：某些情况下，IR 还会包含目标硬件的信息，比如是否要在 NPU 上执行，或者是否需要特定的硬件加速指令。

### 算子 IR 在 NPU 编程中的应用

NPU 编程中，深度学习模型会被编译成一系列算子 IR，然后通过编译器（如 Huawei MindSpore、Ascend 的编译器）将这些 IR 转换为 NPU 能够理解的指令和代码。

示例流程:

1. **高层框架模型**：用户在高层次框架（如 TensorFlow、PyTorch）中定义模型。
2. **算子映射和优化**：模型会被分解为基础的计算单元（算子），这些算子通过 IR 进行优化。
3. **硬件适配**：IR 被进一步转换为与 NPU 硬件相关的执行代码（如 TVM 中的 LLVM IR 或者 NPU 的特定指令集）。
4. **运行**：最终这些低层次指令在 NPU 上执行，实现模型的加速推理或训练。

### 总结

在 NPU 编程中，算子 IR 是深度学习模型的中间表示，起到将高层次框架中的算子映射到底层硬件的桥梁作用。通过 IR，编译器可以优化和调度算子，以充分利用硬件资源，实现高效的推理和训练。

## 模型文件格式

`.onnx`、`.pb`、和 `.pth` 文件是三种常见的机器学习模型文件格式，它们分别用于不同的深度学习框架和推理环境。以下是对它们的详细解释：

### `.onnx` 文件（Open Neural Network Exchange）

- **用途**: `.onnx` 是 Open Neural Network Exchange (ONNX) 的文件格式。它是一种开放的神经网络模型交换格式，旨在使不同深度学习框架（如 PyTorch、TensorFlow、MXNet、Caffe2 等）之间的模型互操作。
- **特点**：
  - ONNX 的目标是提供跨框架兼容性，使你可以在一个框架中训练模型，并在另一个框架中进行推理或部署。
  - `.onnx` 文件可以在支持 ONNX 格式的多种推理引擎中运行（如 ONNX Runtime、TensorRT 等），以实现更快的推理速度。
  - 广泛用于跨平台推理部署，如在云、边缘或移动设备上运行模型。
- **生成方法**: 可以通过支持的深度学习框架（如 PyTorch、TensorFlow 等）导出为 `.onnx` 文件。例如，PyTorch 中可以通过 `torch.onnx.export` 导出模型为 `.onnx` 格式。

### `.pb` 文件（Protocol Buffers）

- **用途**: `.pb` 文件是 TensorFlow 的模型格式，通常代表一个已训练好的 TensorFlow 模型。`.pb` 文件是 Protocol Buffers 格式的缩写，它是一种序列化数据的方式，TensorFlow 使用它来**保存计算图和权重**。
- **特点**：
  - `.pb` 文件可以是 TensorFlow 的静态计算图（Frozen Graph）或 SavedModel 的一部分。静态图是指图中的所有变量都已经转化为常量，用于高效推理。
  - TensorFlow 使用 `.pb` 文件进行模型保存和加载，这对于**模型部署和推理非常有用**。
  - TensorFlow Serving 或 TensorFlow Lite 等工具可以直接加载和运行 `.pb` 格式的模型。
- **生成方法**: 通过 TensorFlow 的 `tf.saved_model.save` 可以保存模型为 `SavedModel` 格式（包含 `.pb` 文件），或者通过 `tf.compat.v1.graph_util.convert_variables_to_constants` 可以将模型冻结并保存为 `.pb` 文件。

### `.pth` 文件（PyTorch）

- **用途**: `.pth` 文件是 PyTorch 框架中**保存模型权重或整个模型状态的文件格式**。PyTorch 使用 Python 的 `pickle` 序列化机制来存储模型的参数、优化器状态以及训练状态。
- **特点**：
  - `.pth` 文件通常用于保存模型的**权重**或**模型状态**，可以在训练结束后保存，也可以在训练过程中保存用于断点续训。
  - 与 ONNX 或 `.pb` 文件不同，`.pth` 文件依赖于 PyTorch 框架，并且模型的加载和推理也需要 PyTorch 环境。
  - `.pth` 文件可以保存整个模型（模型架构和参数）或仅保存模型参数。加载模型时，用户需要自定义模型类的定义来恢复模型。
- **生成方法**: 通过 PyTorch 的 `torch.save(model.state_dict(), 'model.pth')` 保存模型的权重，或者通过 `torch.save(model, 'model.pth')` 保存整个模型（包含架构和参数）。

### 总结对比

| 文件格式 | 框架                                   | 用途                                 | 典型使用场景                       |
| -------- | -------------------------------------- | ------------------------------------ | ---------------------------------- |
| `.onnx`  | 适用于多框架（PyTorch、TensorFlow 等） | 跨框架模型交换，部署推理             | 在一个框架中训练，另一个框架中推理 |
| `.pb`    | TensorFlow                             | 保存 TensorFlow 模型（计算图和权重） | TensorFlow 模型的部署和推理        |
| `.pth`   | PyTorch                                | 保存模型权重或状态                   | 保存和加载 PyTorch 模型            |

这些文件格式各有其适用场景：`.onnx` 用于跨框架模型交换，`.pb` 用于 TensorFlow 模型保存与部署，`.pth` 用于 PyTorch 模型的存储与加载。

## 训练和推理

在神经网络的领域，**训练**（Training）和**推理**（Inference）是两个核心的概念，它们分别对应神经网络模型的**学习过程**和**实际应用**阶段。

### 训练（Training）

**训练**是指通过已有的训练数据集，对神经网络进行学习的过程，使其能够自动调整内部参数（例如权重和偏置），从而更好地执行特定任务。训练的目标是让模型在数据上找到合适的模式，以便能够进行预测、分类、生成等任务。

**训练的过程**：

- **数据输入**：将输入数据（例如图像、文本或音频）喂入神经网络。
- **前向传播**（Forward Propagation）：输入数据依次通过神经网络的层，直到产生最终的输出结果。
- **损失计算**（Loss Calculation）：将网络输出结果与真实标签（或目标）进行比较，计算误差（称为损失函数）。
- **反向传播**（Backpropagation）：根据误差，通过梯度下降算法更新神经网络的参数（权重和偏置），以最小化损失函数。
- **迭代优化**：重复前向传播、损失计算、反向传播的步骤，直到模型在训练数据上达到满意的效果。

**常见优化方法**：

- **梯度下降算法**（Gradient Descent）
- **随机梯度下降**（Stochastic Gradient Descent, SGD）
- **Adam 优化器**（Adaptive Moment Estimation）

### 推理（Inference）

**推理**是指当模型训练完成后，使用模型进行预测或分类的过程。此时，神经网络的参数（权重和偏置）是固定的，不会再进行更新。推理的主要目标是根据新的输入数据，生成模型的输出结果。

**推理的过程**：

- **数据输入**：新的未见过的输入数据被喂入模型。
- **前向传播**：数据通过网络进行前向传播，生成输出。
- **结果输出**：模型根据训练时学习到的模式输出结果（例如分类标签、预测值等）。

推理过程相对较快，因为不涉及反向传播和参数更新。

**训练与推理的区别**：

- **目标**：训练的目的是优化模型的参数，使其能够正确地执行任务，而推理则是利用训练好的模型进行实际应用。
- **计算量**：训练通常涉及大量的数据和计算（如梯度计算），因此计算量大；推理则只需要前向传播，计算量相对较小。
- **更新参数**：训练过程中模型的参数会不断调整，推理时模型参数保持不变。

### 应用场景

- **训练**：通常在有大量标注数据的环境下进行，例如在数据中心或云计算平台上完成，要求较高的计算资源（如 GPU、TPU）。
- **推理**：通常发生在应用的实际场景中，可能是在移动设备、嵌入式设备或云端执行，目标是快速做出决策或预测。

总结来说，训练是让神经网络学会如何处理数据的过程，而推理则是使用已经训练好的模型来进行预测或决策。

## 量化反量化

在深度学习中，**神经网络量化（quantization）** 和 **神经网络反量化（dequantization）** 是两种与神经网络模型优化和推理效率提升相关的重要技术。

### 神经网络量化 (Quantization)

量化是指将神经网络中的**权重和激活值**从高精度（通常是 32 位浮点数）转换为较低精度（例如 8 位整数），以减少模型的存储和计算需求。**量化的主要目标是加速推理过程**，特别是在边缘设备或资源受限的硬件（如移动设备、嵌入式设备）上运行时。

1. 量化的主要类型：

   - **权重量化（Weight Quantization）**：将神经网络中存储的权重值从浮点数表示转为较低精度的整数表示，通常是 8 位整数。
   - **激活量化（Activation Quantization）**：将神经网络推理过程中生成的中间激活值从浮点数表示转为整数表示。

2. 量化方法：

   - **对称量化**：在零点对齐的情况下，正负方向的范围是对称的。所有值以相同的比例缩放，常用于整数量化。
   - **非对称量化**：正负方向的范围不同，可以适应不对称的数据分布。

3. 量化的优点：

   - **减少模型大小**：通过减少表示所需的位数，量化可以显著减小模型的存储需求。
   - **加速推理速度**：低精度的整数运算在很多硬件上比浮点数运算更快。
   - **降低内存带宽要求**：量化减少了内存访问时的开销，因为每次传输的数据量更少。

4. 量化的挑战：

   - **精度损失**：量化过程中可能会引入近似误差，从而导致模型精度下降，特别是在极端情况下（如 2bit、4bit 量化）。
   - **硬件支持**：并不是所有的硬件都原生支持低精度的运算，因此需要特定的硬件优化。

### 神经网络反量化 (Dequantization)

反量化是量化的逆过程，主要用于恢复量化后的值的近似浮点表示，以便在需要高精度计算时进行进一步处理。

1. 反量化通常用于以下场景：

   - **在推理时需要恢复高精度的表示**：例如在一些混合精度的计算中，部分操作可以在低精度下执行，而一些关键计算步骤可能需要恢复为高精度浮点数进行处理。
   - **在训练中模拟量化推理（即量化感知训练）**：在模型训练过程中，通常仍然使用高精度浮点数进行反向传播，但为了适应部署时的量化推理，可以在前向传播时应用模拟量化与反量化操作。

2. 反量化的步骤：

   1. **恢复缩放系数**：在量化时每个值都会使用特定的缩放因子，反量化时会用到这些缩放因子来将整数值还原为近似的浮点值。
   2. **重构原始值**：通过反量化，可以从量化表示（如 8 位整数）中恢复出接近 32 位浮点数的值，尽可能保留原始信息。

3. 反量化的优点：

   - **结合高精度运算**：可以在某些关键部分恢复浮点精度，确保不会丢失太多的模型精度。
   - **灵活性**：通过反量化，可以选择性地只对需要高精度的部分恢复，从而在性能和精度之间进行折衷。

### 在实践中的应用

- **量化感知训练（Quantization-Aware Training，QAT）**：这是神经网络量化的一种高级方法，训练过程考虑到了推理时量化带来的影响，因此能够更好地适应量化后的精度损失。
- **后训练量化（Post-Training Quantization，PTQ）**：指的是在训练完成后再对模型进行量化，适用于大多数场景，但可能会带来一定的精度损失。
- **混合精度推理**：在推理过程中，某些操作可以在低精度（量化值）下进行，而另一些重要操作可能需要反量化，恢复到高精度浮点数。

### 总结

- **量化** 是为了提升推理效率，通过降低权重和激活值的精度来加速计算和减少内存需求。
- **反量化** 则是为了在需要时恢复高精度表示，通常用于混合精度计算或训练过程中。

量化和反量化技术共同帮助神经网络在推理阶段实现性能与精度的平衡，尤其是在移动设备和边缘计算等资源有限的场景中非常有用。

## 量化基本知识

> [1]. [一起实践神经网络量化系列教程（一）！](https://oldpan.me/archives/how-to-quan-1)

实际点来说，量化就是将我们训练好的模型，不论是权重、还是计算 op，都转换为低精度去计算。因为 FP16 的量化很简单，所以实际中我们谈论的量化更多的是 INT8 的量化，当然也有 3-bit、4-bit 的量化，不过目前来说比较常见比较实用的，也就是 INT8 量化了，之后老潘的重点也是 INT8 量化。

1. `pre-tensor` 和 `pre-channel`

一般量化过程中，有 `pre-tensor` 和 `pre-channel` 两种方式。
`pre-tensor` 显而易见，就是对于同一块输入（比如某个卷积前的输入 tensor）我们采用一个 scale，该层所有的输入数据共享一个 scale 值；
`pre-channel`一般是作用于权重，比如一个卷积的权重维度是[64,3,3,3]（输入通道为 3 输出通道为 64，卷积核为 3x3），`pre-channel`就是会产生 64 个 scale 值，分别作用于该卷积权重参数的 64 个通道。
