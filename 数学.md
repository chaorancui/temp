[toc]

# 数学

# 概率论

## 归一化和标准化

最小-最大归一化（Min-Max Normalization）和标准化（Standardization）是两种常见的特征缩放技术，它们在机器学习和数据预处理中有不同的用途和特点。

### 最小-最大归一化（Min-Max Normalization）

**定义**：最小-最大归一化将数据缩放到一个固定范围，通常是 0 到 1，或者-1 到 1。

**公式**：$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$

其中，$x$ 是原始数据，$x_{min}$ 和 $x_{max}$ 分别是数据集中的最小值和最大值，$x′$ 是归一化后的数据。

**特点**：

- **范围固定**：数据被缩放到一个固定范围内。
- **易受异常值影响**：如果数据中存在异常值，最小值和最大值会被拉大，从而影响归一化结果。
- **适用场景**：适用于特征范围已知且固定的情况，如图像像素值的归一化（0 到 255 之间）。

**示例代码**：

```python
import numpy as np

# 生成数据
data = np.array([1, 2, 3, 4, 5])
x_min = np.min(data)
x_max = np.max(data)

# 进行最小-最大归一化
data_normalized = (data - x_min) / (x_max - x_min)
print(data_normalized)
```

### 标准化（Standardization）

**定义**：标准化将数据转换为均值为 0、标准差为 1 的正态分布。

**公式**： $x' = \frac{x - \mu}{\sigma}$

其中，$x$ 是原始数据，$\mu$ 是数据的均值，$\sigma$ 是数据的标准差，$x′$ 是标准化后的数据。

**特点**：

- **无固定范围**：标准化后的数据没有固定的范围，但会符合标准正态分布。
- **减少异常值影响**：标准化减少了异常值对数据分布的影响，因为它基于数据的均值和标准差。
- **适用场景**：适用于数据分布不确定，且可能具有不同量级的情况，如机器学习算法中的特征缩放（例如 SVM、KNN）。

**示例代码**：

```python
import numpy as np

# 生成数据
data = np.array([1, 2, 3, 4, 5])
mu = np.mean(data)
sigma = np.std(data)

# 进行标准化
data_standardized = (data - mu) / sigma
print(data_standardized)
```

### 区别总结

| 特点       | 最小-最大归一化                              | 标准化                        |
| ---------- | -------------------------------------------- | ----------------------------- |
| 公式       | $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$ | $x' = \frac{x - \mu}{\sigma}$ |
| 范围       | 固定（通常是 0 到 1）                        | 无固定范围                    |
| 异常值影响 | 易受影响                                     | 影响较小                      |
| 适用场景   | 特征范围已知且固定                           | 数据分布不确定或不同量级      |

这两种方法在实际应用中有不同的优缺点，选择哪种方法取决于具体的应用场景和数据特性 。

## 常用概率分布

> 参考 GPT 及 如下 WIKI：
> [概率分布与泊松分布、正态分布](https://lrita.github.io/2018/12/28/poisson-normal-distribution/)

不同的概率分布适用于不同的随机现象。
下面是对均匀分布、正态分布、二项分布、泊松分布和指数分布的介绍，以及各自的例子：

1. 均匀分布 (Uniform Distribution)
   定义：
   均匀分布是一种基本的概率分布，其中所有可能的结果出现的概率是相等的。均匀分布可以是离散的，也可以是连续的。

   - **离散均匀分布**：每个离散的结果有相同的概率。例如，掷一个公平的骰子，每个面（1 到 6）出现的概率都是 $ \tfrac{1}{6} $。
   - **连续均匀分布**：在一个区间内的任意子区间内出现的概率相同。例如，随机选择一个在区间 $ [𝑎, 𝑏] $ 内的实数，选择范围内每个值出现的概率都是相等的。其概率密度函数（PDF）为：

   $$ f(x)=\begin{cases}\frac1{b-a}&a\leq x\leq b\\0&\mathrm{otherwise}&\end{cases} $$

   例子：
   假设我们有一个范围为 0 到 10 的均匀分布，如果我们从中随机抽取一个数，那么每个数被选中的概率都是一样的。

2. 正态分布 (Normal Distribution)
   定义：
   正态分布是最常见的连续概率分布之一，其图形呈现为对称的钟形曲线。正态分布由两个参数决定：均值($ \mu $)和标准差($ \sigma $)。
   正态分布的概率密度函数为：

   $$ f(x)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

   - $ \mu $ 是平均值，决定了曲线的中心位置。
   - $ \sigma $ 是标准差，决定了曲线的宽度。

   例子：
   人的身高、考试成绩、产品的制造误差等通常近似为正态分布。例如，某个班级学生的考试成绩平均为 75 分，标准差为 10 分，那么成绩可以近似表示为 $ \mathcal{N}(75,10^2) $。

3. 二项分布 (Binomial Distribution)
   定义：
   二项分布描述的是在 n 次独立试验中，某事件 A 发生 k 次的概率，每次试验中事件 A 发生的概率为 p。其概率质量函数（PMF）为：

   $$ P(X=k)=\binom nkp^k(1-p)^{n-k} $$

   - $ n $ 是试验的总次数。
   - $ p $ 是每次试验中事件 A 发生的概率。
   - $ k $ 是事件 A 发生的次数。

   例子：
   在一个测试中，硬币被抛掷 10 次，问正面朝上的次数。如果硬币是公平的，则每次正面朝上的概率为 0.5，这就是一个二项分布，参数为 $ 𝑛 = 10 $ 和 $ p = 0.5 $。

4. 泊松分布 (Poisson Distribution)
   定义：
   泊松分布用于描述在单位时间或单位空间内某事件发生的次数的概率。泊松分布有一个参数 $ \lambda $，表示单位时间或空间中事件的平均发生次数。其概率质量函数（PMF）为：

   $$ P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!} $$

   - $ \lambda $ 是单位时间内事件的平均发生次数。
   - $ k $ 是事件发生的次数。

   例子：
   某一客服中心每小时平均接到 3 个电话，求某小时内接到 5 个电话的概率，则可以用泊松分布来计算，其中 $ \lambda = 3 $：

   $$ P(X=5)=\frac{3^5e^{-3}}{5!} $$

5. 指数分布 (Exponential Distribution)
   定义：
   指数分布用于描述事件之间的时间间隔或距离的概率。它是泊松过程中的时间间隔分布。指数分布由一个参数 $ \lambda $ 决定，它是事件发生的速率。其概率密度函数为：

   $$ f(x)=\lambda e^{-\lambda x}\quad(x\geq0) $$

   - $ λ $ 是事件的速率参数。

   例子：
   假设在一个顾客服务中心，每小时平均有 3 个顾客到来，那么顾客之间到达的时间间隔可以表示为一个参数为 $ \lambda = 3 $ 的指数分布。即每隔 $ \tfrac{1}{3} $ 小时(约 20 分钟)期望有一个顾客到来。

这五种分布各有应用场景，它们在统计分析和随机过程建模中发挥着重要作用。

## 变量、协变量

在统计学和数据分析中，“变量”和“协变量”是两个常用的术语。虽然它们都涉及数据中的数值或类别，但它们在具体含义和应用上有所不同。以下是对变量和协变量的详细解释和它们之间的区别。

### 变量（Variable）

变量是指数据集中可以变化和测量的任何属性或特征。在统计学和数据分析中，变量可以分为以下几类：

1. **自变量（Independent Variable）**：
   - 也称为预测变量或解释变量。
   - 这些是研究中可以操控或选择的变量。
   - 例子：在一个关于肥料对植物生长影响的实验中，肥料的类型和数量就是自变量。
2. **因变量（Dependent Variable）**：
   - 也称为响应变量或结果变量。
   - 这些是研究中受到自变量影响的变量。
   - 例子：在上述实验中，植物的生长高度就是因变量。
3. **控制变量（Control Variable）**：
   - 这些变量需要保持恒定，以确保研究的有效性。
   - 例子：在植物生长实验中，光照和水分应该是控制变量。

### 协变量（Covariate）

协变量是指在统计模型中包含的额外变量，这些变量可能与研究中的自变量和因变量都有关系。协变量的主要作用是控制其他可能影响因变量的因素，以提高模型的精度和解释力。协变量常用于回归分析和实验设计中。

- 例子：在一个研究体重与血压关系的研究中，年龄和性别可能是协变量，因为它们也会影响血压。

### 变量和协变量的区别

1. **目的和角色**：
   - **变量**：通常是指自变量和因变量，直接涉及到研究的主要问题或假设。
   - **协变量**：是那些可能影响因变量但不是研究主要关注点的变量，主要用于控制其他影响因素。
2. **在模型中的作用**：
   - **自变量**：在模型中作为预测或解释因变量的主要因素。
   - **因变量**：在模型中作为主要结果或响应。
   - **协变量**：在模型中作为控制因素，用于消除混淆或提高模型的精确度。
3. **实例应用**：
   - **变量**：在一个实验中，如果我们研究药物剂量对病人康复时间的影响，药物剂量是自变量，康复时间是因变量。
   - **协变量**：如果我们考虑病人的年龄和初始健康状况对康复时间的影响，这些因素就是协变量。

### 总结

- **变量**是指研究中**可以变化和测量的任何属性或特征**，包括自变量（独立变量）、因变量（依赖变量）和控制变量。
- **协变量**是指在统计模型中包含的**额外变量**，用于**控制其他可能影响因变量的因素**，以提高模型的精度和解释力。

理解这两者的区别和应用，对于设计有效的实验和建立准确的统计模型非常重要。

## 概率密度函数/累积分布函数

**概率密度函数（Probability Density Function，简称 PDF）**和**累积分布函数（Cumulative Distribution Function，简称 CDF）**是描述随机变量分布的两种常用数学工具。它们有不同的作用和使用场景。下面分别介绍这两者。

### 概率密度函数（PDF）

**定义：**
概率分布函数是描述连续随机变量在某个特定值附近取值的“概率密度”的函数。对于离散随机变量，它叫做**概率质量函数（PMF）**。PDF 的值不直接表示概率，而是概率密度。真正的概率是通过在某个区间内对 PDF 进行积分得到的。

**数学表达：**
对于连续随机变量 $ X $，其概率密度函数 $ f_X(x)$ 满足：

$$ P(a \leq X \leq b) = \int_a^b f_X(x) dx $$

这里，PDF 本身的值并不代表概率，而是概率密度，表示随机变量取某个值的相对可能性。

**性质**：

- PDF 必须满足非负性：$ f_X(x) \geq 0 $ 对所有 $ x $ 都成立。
- PDF 的积分值为 1：$ \int\_{-\infty}^{\infty} f_X(x) dx = 1 $，因为总概率为 1。

**作用：**
PDF 描述了随机变量可能取各个值的相对“可能性”，它反映了概率在某个点附近的密集程度。通过对 PDF 进行积分，可以得到该随机变量落在某个区间内的概率。

**使用场景：**

- **概率计算**：通过 PDF，可以计算随机变量在某个区间内取值的概率，或者计算某个点附近的概率密度。
- **生成随机数**：许多算法，如蒙特卡洛模拟，会基于给定的 PDF 生成符合分布的随机数。
- **建模与分析**：PDF 常用于建模连续随机变量，如金融市场的价格波动、气象数据、物理实验中的测量误差等。

**例子：**
假设一个随机变量 $ X $ 服从正态分布 $ X \sim N(\mu, \sigma^2) $，则其 PDF 为：

$$ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right) $$

它描述了正态分布在各个点的概率密度。

### 累积分布函数（CDF）

> 在一些文献中，**概率分布函数**这个术语有时会被用来指代 **累积分布函数（CDF）**，尤其是在一些早期的或简化的统计学和概率学教材中。
> 但在严格的概率论中，**概率分布函数**这个术语应当指代 **累积分布函数（CDF）**，而 **概率密度函数（PDF）** 则描述随机变量的概率密度。

**定义：**
累积分布函数是描述随机变量取值小于或等于某一特定值的概率的函数。它是通过对概率密度函数进行积分得到的。CDF 是一个非递减函数，其值在区间 [0, 1] 内。

**数学表达：**
对于随机变量 $ X $，其累积分布函数 $ F_X(x) $ 定义为：

<!--prettier-ignore-->
$$ F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(t) dt $$

其中，$ f_X(x) $ 是概率密度函数。对于离散随机变量，CDF 是通过求和来定义的。
CDF 给出了 $ X $ 取值小于或等于 $ x $ 的概率。

**作用：**
CDF 给出了随机变量小于或等于某个值的累积概率，它可以帮助我们回答“随机变量取值小于或等于某一数值的概率有多大？”的问题。

**使用场景：**

- **概率计算**：当需要计算随机变量小于某个特定值的概率时，CDF 非常有用。直接通过 CDF 可以得到这样的概率。
- **模型评估**：CDF 也可以用来比较不同分布的表现，例如在机器学习和统计中，比较模型的误差分布。
- **区间概率**：通过 CDF，可以轻松计算随机变量落在某个区间内的概率： $ P(a \leq X \leq b) = F_X(b) - F_X(a) $

**例子：**
如果 $ X \sim N(\mu, \sigma^2) $，则其 CDF 是正态分布的累积分布函数：

$$ F_X(x) = \frac{1}{2} \left[ 1 + \text{erf}\left(\frac{x - \mu}{\sigma \sqrt{2}}\right) \right] $$

这里的 `erf` 是误差函数。

### 总结

1. 使用场景对比

   - 如果你想知道一个随机变量在某个区间内的概率，通常需要通过 PDF 进行积分来计算。
   - 如果你想要知道一个随机变量小于等于某个值的累积概率，CDF 会更加直观方便。

2. PDF 和 CDF 之间的关系

   - **数学表达上的关系：**

     假设 $ X $ 是一个连续随机变量，且其概率分布函数为 $ f_X(x) $（PDF），累积分布函数为 $ F_X(x) $（CDF）。

     1. **从 PDF 到 CDF：** 累积分布函数 $ F_X(x) 可以通过对概率密度函数 $ f_X(x) $ 进行积分得到：

        $$ F*X(x) = P(X \leq x) = \int*{-\infty}^x f_X(t) dt $$

        这表示 CDF 是 PDF 在区间 $ (-\infty, x] $ 上的累积概率。

     2. **从 CDF 到 PDF：** 如果 $ F_X(x) $ 是可导的，概率密度函数 $ f_X(x) $ 则是累积分布函数 $ F_X(x) $ 的导数：

        $$ f_X(x) = \frac{d}{dx} F_X(x) $$

        换句话说，CDF 对 $ x $ 求导得到 PDF。这个关系意味着，如果你知道了 CDF，你可以通过求导得到 PDF。

   - **实际意义上的关系：**

     - **PDF 是局部的，CDF 是累积的：**
       - **PDF** 描述了随机变量在某个点附近的概率密度。它提供了关于随机变量取值的“密集程度”信息。例如，PDF 在某个点的值告诉我们随机变量在该点附近的相对可能性，但由于它是概率密度，所以它的数值本身并不代表实际概率（实际概率是通过对区间的积分得到的）。
       - **CDF** 则描述了随机变量小于或等于某个值的累计概率。它是一个从零到一单调增加的函数，能够直观地告诉我们“随机变量小于某个值的概率是多少”。
     - **PDF 提供局部概率，CDF 提供累积概率：**
       - 通过 PDF，我们可以获得随机变量在某个小区间内（如 $ [x, x+dx] $）的概率近似值。通常，PDF 的值越高，表示随机变量在这个点附近取值的概率越大。
       - 通过 CDF，我们可以得到随机变量小于或等于某个特定值的总概率。因此，CDF 提供的是一个“累积”或“累计”的视角，回答了“随机变量小于或等于某个值的概率有多大”这样的问题。

3. 概率分布
   - **“概率分布”** 这一术语通常是泛指的，可以指代**离散型随机变量的 PMF** 或 **连续型随机变量的 PDF**。
   - 在**实际应用中**，人们通常会根据上下文来区分离散型和连续型随机变量的概率分布。对于离散型关注 PMF；对于连续型关注 PDF。

- **PDF** 提供随机变量在某一点的概率密度，描述了局部的概率分布，且它的积分（在某个区间上）给出了区间内的概率。
- **CDF** 提供了随机变量小于等于某个值的累积概率，描述了累积的概率分布，并且是一个单调递增的函数。它与 PDF 之间的关系是，CDF 是 PDF 的积分，PDF 是 CDF 的导数。

# 线性代数

## 仿射变换

仿射变换（Affine transformation）是指一种**保持直线平行性质的几何变换**，它包括**平移、旋转、缩放和剪切**等操作。在二维空间中，一个仿射变换可以用一个非奇异的二阶矩阵和一个平移向量来描述，通常表示为：

$T(\mathbf{x}) = A \mathbf{x} + \mathbf{b}$

其中，$\mathbf{x}$ 是原始点的坐标向量，$A$ 是一个 2x2 的矩阵，$\mathbf{b}$ 是一个二维平移向量。

1. 平移变换 (Translation)

   平移变换是将所有点沿某一方向移动一个固定距离。二维平移变换可以表示为：

   $$T(\mathbf{x}) = \mathbf{x} + \mathbf{b}$$

   其中，$\mathbf{x}$ 是原始点的坐标向量，$\mathbf{b}$ 是平移向量。

2. 旋转变换 (Rotation)

   旋转变换是将所有点绕一个固定点旋转一定角度。二维旋转变换可以表示为：

   $$R(\mathbf{x}) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \mathbf{x}$$

   其中，$\thetaθ$ 是旋转角度。

3. 缩放变换 (Scaling)

   缩放变换是按比例放大或缩小所有点。二维缩放变换可以表示为：

   $$S(\mathbf{x}) = \begin{pmatrix} s_x & 0 \\ 0 & s_y \end{pmatrix} \mathbf{x}$$

   其中，$s_x$ $s_y$ 分别是 x 和 y 方向的缩放因子。

4. 剪切变换 (Shear)

   剪切变换是将所有点沿某一方向按比例移动，使图形倾斜。二维剪切变换可以表示为：

   $$H(\mathbf{x}) = \begin{pmatrix} 1 & k_x \\ k_y & 1 \end{pmatrix} \mathbf{x}$$

   其中，$k_x$ 和 $k_y$ 分别是 x 和 y 方向的剪切因子。

5. 仿射变换 (Affine Transformation)

   仿射变换是包括上述所有变换的一种综合变换，它可以保持线的平行性。二维仿射变换可以表示为：

   $$A(\mathbf{x}) = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \mathbf{x} + \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$$

6. 齐次变换 (Homogeneous Transformation)

   齐次变换是一种在项目空间中表示变换的方法，通过引入齐次坐标，可以将仿射变换表示为矩阵乘法，从而统一各种变换。三维齐次变换表示为：

   $$T(\mathbf{x}) = \begin{pmatrix} A & \mathbf{b} \\ 0 & 1 \end{pmatrix} \mathbf{x}$$

   其中，$A$ 是一个 3x3 的矩阵，$\mathbf{b}$ 是平移向量，$\mathbf{x}$ 是齐次坐标向量。

通过这些变换，可以灵活地操纵图形和图像，实现各种效果。

## 矩阵乘法

[矩阵乘法](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3%E4%B9%98%E6%B3%95)
[矩阵的几种乘法](https://zhaoyangchen.github.io/2023/07/03/matrix-multiply.html)

1. **矩阵乘积**

   矩阵乘积(matrix product，也叫 matmul product)。

   - 符号： $ \cdot $ 或 省略（ `省略 或 $\cdot$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/ecf70982d400cece8e8f6c86ba8a4796.png)
   - 运算律：不满足交换律；满足结合律；

2. **矩阵哈德玛乘积**

   Hadamard product(又称 element\-wise product)。

   - 符号：$ \circ $ 或 $ \odot $（`$\circ$ 或 $\odot$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/dc52a6e57be0d132c5212653f6ed3f9a.png#pic_center)
   - 运算律：满足交换律；满足结合律；

3. **矩阵克罗内克积**

   Kronecker product(克罗内克积)。

   - 符号： $ \otimes $ （`$\otimes$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/86f8a96edd438d4ff2ed0570ad62634b.png#pic_center)
   - 运算律：不满足交换律；满足结合律；

4. 共同性质

   上述三种乘积都符合：

   - 结合律
     $ A(BC) = (AB)C $
   - 分配律：
     $ A(B + C) = AB + AC $
     $ (A + B)C = AC + BC $
   - 和标量乘积相容：
     $ c(AB) = c(A)B $
     $ (Ac)B = A(cB) $
     $ (AB)c = A(Bc) $

# 工程概念

## numpy 数组中轴

### 数组形状与轴维度的关系

- 数组的形状（shape）描述了每个轴的维度。对于形状为 `(m, n)` 的二维数组：
  - `m` 表示 `axis 0` 上的元素数量（**即行数**），沿着这个轴可以访问不同的行。
  - `n` 表示 `axis 1` 上的元素数量（**即列数**），沿着这个轴可以访问不同的列。
- 在更高维的数组中，每个维度对应于特定的轴，形状的每个数字表示相应轴上的元素数量。

**举例说明**：

- 一维数组:
  - `a = np.array([1, 2, 3, 4])`
  - 形状为 `(4,)`，表示这个数组只有一个轴（`axis 0`），该轴上有 4 个元素。
- 二维数组:
  - `b = np.array([[1, 2, 3], [4, 5, 6]])`
  - 形状为 `(2, 3)`，表示这个数组有两个轴：
    - `axis 0` 有 2 个元素（即有 2 行）。
    - `axis 1` 有 3 个元素（即每行有 3 列）。
- 三维数组:
  - `c = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])`
  - 形状为 `(2, 2, 3)`，表示这个数组有三个轴：
    - `axis 0` 有 2 个元素（即最外层的两个矩阵）。
    - `axis 1` 有 2 个元素（即每个矩阵中的两行）。
    - `axis 2` 有 3 个元素（即每行中的三个值）。

### 轴的意义与作用

1. **数据处理**

   - 在进行操作时，指定轴可以控制操作沿着哪个维度进行。例如，求和、平均、最大值、最小值等操作可以沿特定轴进行，得到不同的结果。
   - 示例: 对二维数组 A 进行操作：
     - `np.sum(A, axis=0)`: 沿着轴 0 求和（压缩行），结果是对每一列求和。
     - `np.sum(A, axis=1)`: 沿着轴 1 求和（压缩列），结果是对每一行求和。

2. **数据表示**

   - 轴提供了理解数据形状和结构的途径。在多维数据中，轴决定了数据的排列方式。例如，在图像处理中，常见的图像张量通常是 `(height, width, channels)`，其中 `height` 对应于轴 0，`width` 对应于轴 1，`channels` 对应于轴 2。

3. **广播机制**

   - 轴的概念在广播机制中也很重要。广播允许不同形状的数组在一起进行运算时，沿某个轴自动扩展维度以适配另一个数组。

## matlab 与 numpy 中的轴

### MATLAB 中数组的轴

> **每新增一个轴，原来的轴保持不变，新增的轴变为新的维度，看起来轴是在尾部新增的。**

MATLAB 中的数组是**列优先的**（column-major order），轴的编号从 1 开始。多维数组中的轴表示如下：

- **第 1 轴 (axis 1)**: 行方向（row），即沿着第一个维度变化。
- **第 2 轴 (axis 2)**: 列方向（column），即沿着第二个维度变化。
- **第 3 轴 (axis 3)**: 第三维度方向，依此类推。

例如，一个 $3 \times 4 \times 5$ 的数组：

- 第 1 轴（axis 1）长度为 3（行数）。
- 第 2 轴（axis 2）长度为 4（列数）。
- 第 3 轴（axis 3）长度为 5。

### NumPy 中数组的轴

> **新增的轴总是添加在最前面，变成 0 轴，原来的轴依次加 1。**

NumPy 中的数组是**行优先的**（row-major order），轴的编号从 0 开始。多维数组中的轴表示如下：

- **第 0 轴 (axis 0)**: 行方向（row），即沿着第一个维度变化。
- **第 1 轴 (axis 1)**: 列方向（column），即沿着第二个维度变化。
- **第 2 轴 (axis 2)**: 第三维度方向，依此类推。

例如，一个形状为 `(3, 4, 5)` 的 NumPy 数组：

- 第 0 轴（axis 0）长度为 3（行数）。
- 第 1 轴（axis 1）长度为 4（列数）。
- 第 2 轴（axis 2）长度为 5。

### 总结

- MATLAB 轴的编号从 **1** 开始，NumPy 轴的编号从 **0** 开始。
- MATLAB 的第`n`轴对应 NumPy 的第 `n-1` 轴。

举个例子：

- 在 MATLAB 中，对一个二维数组 `A`，`A(2, :)` 取第二行数据，对应 NumPy 中的 `A[1, :]`。
- 在 MATLAB 中，`sum(A, 2)` 是沿着列（第 2 轴）求和，对应 NumPy 中的 `np.sum(A, axis=1)`。

## numpy 中 axis=-1

在 NumPy 中，`axis=-1`表示数组的最后一个轴。这是一种便捷的方式来引用数组的最后一个维度，而不需要知道数组具体有多少维。

- 对于一维数组，`axis=-1`等同于`axis=0`
- 对于二维数组，`axis=-1`等同于`axis=1`
- 对于三维数组，`axis=-1`等同于`axis=2`
- 以此类推...

假设我们有以下数组：

```python
import numpy as np

# 创建一个3x4x5的数组
arr = np.array([[[1, 2, 3, 4, 5],
                 [6, 7, 8, 9, 10],
                 [11, 12, 13, 14, 15],
                 [16, 17, 18, 19, 20]],

                [[21, 22, 23, 24, 25],
                 [26, 27, 28, 29, 30],
                 [31, 32, 33, 34, 35],
                 [36, 37, 38, 39, 40]],

                [[41, 42, 43, 44, 45],
                 [46, 47, 48, 49, 50],
                 [51, 52, 53, 54, 55],
                 [56, 57, 58, 59, 60]]])
```

这个数组的形状是 `(3, 4, 5)`，即有 3 个矩阵，每个矩阵有 4 行 5 列。

如果我们对这个数组沿着 `axis=-1` 进行求和：

```python
sum_along_last_axis = np.sum(arr, axis=-1)
print(sum_along_last_axis)
```

输出将是一个形状为 `(3, 4)` 的数组：

```python
[[ 15  40  65  90]
 [115 140 165 190]
 [215 240 265 290]]
```

**详细解释：**

- **原始数组形状为 `(3, 4, 5)`**，表示有 3 个 4x5 的矩阵。
- **`axis=-1`** 表示最后一个轴（即每个 5 元素的列），沿着这个轴进行求和。
- 求和的结果是在每个 5 个元素的集合上进行的，因此原始的 `(3, 4, 5)` 数组变成了 `(3, 4)` 数组，最后一个轴消失，剩下的元素是沿着最后一个轴求和的结果。

## numpy 中多个轴做规约

假设我们有一个三维数组，我们想沿着最后两个轴计算均值。

```python
import numpy as np

# 创建一个形状为 (2, 3, 4) 的三维数组
arr = np.array([[[1, 2, 3, 4],
                 [5, 6, 7, 8],
                 [9, 10, 11, 12]],

                [[13, 14, 15, 16],
                 [17, 18, 19, 20],
                 [21, 22, 23, 24]]])

# 沿着轴1和轴2计算均值
mean_across_axes = np.mean(arr, axis=(1, 2))
print("Mean across axis 1 and 2:\n", mean_across_axes)
```

输出：

```log
Mean across axis 1 and 2:
 [ 6.5 18.5]
```

解释：

- **输入数组 `arr`** 的形状为 `(2, 3, 4)`，表示有 2 个 3x4 的矩阵。
- **`axis=(1, 2)`** 表示我们希望沿着第二个和第三个轴（即每个矩阵的行和列）计算均值。
- 对于第一个矩阵（形状 `(3, 4)`），其元素的均值为 6.5。第二个矩阵，其元素的均值为 18.5。
- 结果是一个一维数组，形状为 `(2,)`，每个元素对应于原数组中沿着指定轴计算的均值。

**更多示例**：

1. **计算二维数组中最后两个轴的均值**：

   ```python
   arr = np.random.rand(4, 5, 6)
   mean_across_last_two_axes = np.mean(arr, axis=(-2, -1))
   print("Mean across last two axes:\n", mean_across_last_two_axes)
   ```

   - 此例子中，`axis=(-2, -1)` 表示沿着倒数第二和最后一个轴（即二维数组的行和列）计算均值。

2. **保持原维度**：

   ```python
   mean_with_keepdims = np.mean(arr, axis=(1, 2), keepdims=True)
   print("Mean with keepdims:\n", mean_with_keepdims)
   ```

   - 设置 `keepdims=True` 时，输出数组保持原维度，只是沿着指定轴计算的均值位置的维度被压缩为 1。
