[toc]

# 数学

# 概率论

## 归一化和标准化

最小-最大归一化（Min-Max Normalization）和标准化（Standardization）是两种常见的特征缩放技术，它们在机器学习和数据预处理中有不同的用途和特点。

### 最小-最大归一化（Min-Max Normalization）

**定义**：最小-最大归一化将数据缩放到一个固定范围，通常是 0 到 1，或者-1 到 1。

**公式**：$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$

其中，$x$ 是原始数据，$x_{min}$ 和 $x_{max}$ 分别是数据集中的最小值和最大值，$x′$ 是归一化后的数据。

**特点**：

- **范围固定**：数据被缩放到一个固定范围内。
- **易受异常值影响**：如果数据中存在异常值，最小值和最大值会被拉大，从而影响归一化结果。
- **适用场景**：适用于特征范围已知且固定的情况，如图像像素值的归一化（0 到 255 之间）。

**示例代码**：

```python
import numpy as np

# 生成数据
data = np.array([1, 2, 3, 4, 5])
x_min = np.min(data)
x_max = np.max(data)

# 进行最小-最大归一化
data_normalized = (data - x_min) / (x_max - x_min)
print(data_normalized)
```

### 标准化（Standardization）

**定义**：标准化将数据转换为均值为 0、标准差为 1 的正态分布。

**公式**： $x' = \frac{x - \mu}{\sigma}$

其中，$x$ 是原始数据，$\mu$ 是数据的均值，$\sigma$ 是数据的标准差，$x′$ 是标准化后的数据。

**特点**：

- **无固定范围**：标准化后的数据没有固定的范围，但会符合标准正态分布。
- **减少异常值影响**：标准化减少了异常值对数据分布的影响，因为它基于数据的均值和标准差。
- **适用场景**：适用于数据分布不确定，且可能具有不同量级的情况，如机器学习算法中的特征缩放（例如 SVM、KNN）。

**示例代码**：

```python
import numpy as np

# 生成数据
data = np.array([1, 2, 3, 4, 5])
mu = np.mean(data)
sigma = np.std(data)

# 进行标准化
data_standardized = (data - mu) / sigma
print(data_standardized)
```

### 区别总结

| 特点       | 最小-最大归一化                              | 标准化                        |
| ---------- | -------------------------------------------- | ----------------------------- |
| 公式       | $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$ | $x' = \frac{x - \mu}{\sigma}$ |
| 范围       | 固定（通常是 0 到 1）                        | 无固定范围                    |
| 异常值影响 | 易受影响                                     | 影响较小                      |
| 适用场景   | 特征范围已知且固定                           | 数据分布不确定或不同量级      |

这两种方法在实际应用中有不同的优缺点，选择哪种方法取决于具体的应用场景和数据特性 。

## 常用概率分布

> 参考 GPT 及 如下 WIKI：
> [概率分布与泊松分布、正态分布](https://lrita.github.io/2018/12/28/poisson-normal-distribution/)

不同的概率分布适用于不同的随机现象。
下面是对均匀分布、正态分布、二项分布、泊松分布和指数分布的介绍，以及各自的例子：

1. 均匀分布 (Uniform Distribution)
   定义：
   均匀分布是一种基本的概率分布，其中所有可能的结果出现的概率是相等的。均匀分布可以是离散的，也可以是连续的。

   - **离散均匀分布**：每个离散的结果有相同的概率。例如，掷一个公平的骰子，每个面（1 到 6）出现的概率都是 $ \tfrac{1}{6} $。
   - **连续均匀分布**：在一个区间内的任意子区间内出现的概率相同。例如，随机选择一个在区间 $ [𝑎, 𝑏] $ 内的实数，选择范围内每个值出现的概率都是相等的。其概率密度函数（PDF）为：

   $$ f(x)=\begin{cases}\frac1{b-a}&a\leq x\leq b\\0&\mathrm{otherwise}&\end{cases} $$

   例子：
   假设我们有一个范围为 0 到 10 的均匀分布，如果我们从中随机抽取一个数，那么每个数被选中的概率都是一样的。

2. 正态分布 (Normal Distribution)
   定义：
   正态分布是最常见的连续概率分布之一，其图形呈现为对称的钟形曲线。正态分布由两个参数决定：均值($ \mu $)和标准差($ \sigma $)。
   正态分布的概率密度函数为：

   $$ f(x)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

   - $ \mu $ 是平均值，决定了曲线的中心位置。
   - $ \sigma $ 是标准差，决定了曲线的宽度。

   例子：
   人的身高、考试成绩、产品的制造误差等通常近似为正态分布。例如，某个班级学生的考试成绩平均为 75 分，标准差为 10 分，那么成绩可以近似表示为 $ \mathcal{N}(75,10^2) $。

3. 二项分布 (Binomial Distribution)
   定义：
   二项分布描述的是在 n 次独立试验中，某事件 A 发生 k 次的概率，每次试验中事件 A 发生的概率为 p。其概率质量函数（PMF）为：

   $$ P(X=k)=\binom nkp^k(1-p)^{n-k} $$

   - $ n $ 是试验的总次数。
   - $ p $ 是每次试验中事件 A 发生的概率。
   - $ k $ 是事件 A 发生的次数。

   例子：
   在一个测试中，硬币被抛掷 10 次，问正面朝上的次数。如果硬币是公平的，则每次正面朝上的概率为 0.5，这就是一个二项分布，参数为 $ 𝑛 = 10 $ 和 $ p = 0.5 $。

4. 泊松分布 (Poisson Distribution)
   定义：
   泊松分布用于描述在单位时间或单位空间内某事件发生的次数的概率。泊松分布有一个参数 $ \lambda $，表示单位时间或空间中事件的平均发生次数。其概率质量函数（PMF）为：

   $$ P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!} $$

   - $ \lambda $ 是单位时间内事件的平均发生次数。
   - $ k $ 是事件发生的次数。

   例子：
   某一客服中心每小时平均接到 3 个电话，求某小时内接到 5 个电话的概率，则可以用泊松分布来计算，其中 $ \lambda = 3 $：

   $$ P(X=5)=\frac{3^5e^{-3}}{5!} $$

5. 指数分布 (Exponential Distribution)
   定义：
   指数分布用于描述事件之间的时间间隔或距离的概率。它是泊松过程中的时间间隔分布。指数分布由一个参数 $ \lambda $ 决定，它是事件发生的速率。其概率密度函数为：

   $$ f(x)=\lambda e^{-\lambda x}\quad(x\geq0) $$

   - $ λ $ 是事件的速率参数。

   例子：
   假设在一个顾客服务中心，每小时平均有 3 个顾客到来，那么顾客之间到达的时间间隔可以表示为一个参数为 $ \lambda = 3 $ 的指数分布。即每隔 $ \tfrac{1}{3} $ 小时(约 20 分钟)期望有一个顾客到来。

这五种分布各有应用场景，它们在统计分析和随机过程建模中发挥着重要作用。

## 变量、协变量

在统计学和数据分析中，“变量”和“协变量”是两个常用的术语。虽然它们都涉及数据中的数值或类别，但它们在具体含义和应用上有所不同。以下是对变量和协变量的详细解释和它们之间的区别。

### 变量（Variable）

变量是指数据集中可以变化和测量的任何属性或特征。在统计学和数据分析中，变量可以分为以下几类：

1. **自变量（Independent Variable）**：
   - 也称为预测变量或解释变量。
   - 这些是研究中可以操控或选择的变量。
   - 例子：在一个关于肥料对植物生长影响的实验中，肥料的类型和数量就是自变量。
2. **因变量（Dependent Variable）**：
   - 也称为响应变量或结果变量。
   - 这些是研究中受到自变量影响的变量。
   - 例子：在上述实验中，植物的生长高度就是因变量。
3. **控制变量（Control Variable）**：
   - 这些变量需要保持恒定，以确保研究的有效性。
   - 例子：在植物生长实验中，光照和水分应该是控制变量。

### 协变量（Covariate）

协变量是指在统计模型中包含的额外变量，这些变量可能与研究中的自变量和因变量都有关系。协变量的主要作用是控制其他可能影响因变量的因素，以提高模型的精度和解释力。协变量常用于回归分析和实验设计中。

- 例子：在一个研究体重与血压关系的研究中，年龄和性别可能是协变量，因为它们也会影响血压。

### 变量和协变量的区别

1. **目的和角色**：
   - **变量**：通常是指自变量和因变量，直接涉及到研究的主要问题或假设。
   - **协变量**：是那些可能影响因变量但不是研究主要关注点的变量，主要用于控制其他影响因素。
2. **在模型中的作用**：
   - **自变量**：在模型中作为预测或解释因变量的主要因素。
   - **因变量**：在模型中作为主要结果或响应。
   - **协变量**：在模型中作为控制因素，用于消除混淆或提高模型的精确度。
3. **实例应用**：
   - **变量**：在一个实验中，如果我们研究药物剂量对病人康复时间的影响，药物剂量是自变量，康复时间是因变量。
   - **协变量**：如果我们考虑病人的年龄和初始健康状况对康复时间的影响，这些因素就是协变量。

### 总结

- **变量**是指研究中**可以变化和测量的任何属性或特征**，包括自变量（独立变量）、因变量（依赖变量）和控制变量。
- **协变量**是指在统计模型中包含的**额外变量**，用于**控制其他可能影响因变量的因素**，以提高模型的精度和解释力。

理解这两者的区别和应用，对于设计有效的实验和建立准确的统计模型非常重要。

## 概率密度函数/累积分布函数

**概率密度函数（Probability Density Function，简称 PDF）**和**累积分布函数（Cumulative Distribution Function，简称 CDF）**是描述随机变量分布的两种常用数学工具。它们有不同的作用和使用场景。下面分别介绍这两者。

### 概率密度函数（PDF）

**定义：**
概率分布函数是描述连续随机变量在某个特定值附近取值的“概率密度”的函数。对于离散随机变量，它叫做**概率质量函数（PMF）**。PDF 的值不直接表示概率，而是概率密度。真正的概率是通过在某个区间内对 PDF 进行积分得到的。

**数学表达：**
对于连续随机变量 $ X $，其概率密度函数 $ f_X(x)$ 满足：

$$ P(a \leq X \leq b) = \int_a^b f_X(x) dx $$

这里，PDF 本身的值并不代表概率，而是概率密度，表示随机变量取某个值的相对可能性。

**性质**：

- PDF 必须满足非负性：$ f_X(x) \geq 0 $ 对所有 $ x $ 都成立。
- PDF 的积分值为 1：$ \int\_{-\infty}^{\infty} f_X(x) dx = 1 $，因为总概率为 1。

**作用：**
PDF 描述了随机变量可能取各个值的相对“可能性”，它反映了概率在某个点附近的密集程度。通过对 PDF 进行积分，可以得到该随机变量落在某个区间内的概率。

**使用场景：**

- **概率计算**：通过 PDF，可以计算随机变量在某个区间内取值的概率，或者计算某个点附近的概率密度。
- **生成随机数**：许多算法，如蒙特卡洛模拟，会基于给定的 PDF 生成符合分布的随机数。
- **建模与分析**：PDF 常用于建模连续随机变量，如金融市场的价格波动、气象数据、物理实验中的测量误差等。

**例子：**
假设一个随机变量 $ X $ 服从正态分布 $ X \sim N(\mu, \sigma^2) $，则其 PDF 为：

$$ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left( - \frac{(x - \mu)^2}{2\sigma^2} \right) $$

它描述了正态分布在各个点的概率密度。

### 累积分布函数（CDF）

> 在一些文献中，**概率分布函数**这个术语有时会被用来指代 **累积分布函数（CDF）**，尤其是在一些早期的或简化的统计学和概率学教材中。
> 但在严格的概率论中，**概率分布函数**这个术语应当指代 **累积分布函数（CDF）**，而 **概率密度函数（PDF）** 则描述随机变量的概率密度。

**定义：**
累积分布函数是描述随机变量取值小于或等于某一特定值的概率的函数。它是通过对概率密度函数进行积分得到的。CDF 是一个非递减函数，其值在区间 [0, 1] 内。

**数学表达：**
对于随机变量 $ X $，其累积分布函数 $ F_X(x) $ 定义为：

<!--prettier-ignore-->
$$ F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(t) dt $$

其中，$ f_X(x) $ 是概率密度函数。对于离散随机变量，CDF 是通过求和来定义的。
CDF 给出了 $ X $ 取值小于或等于 $ x $ 的概率。

**作用：**
CDF 给出了随机变量小于或等于某个值的累积概率，它可以帮助我们回答“随机变量取值小于或等于某一数值的概率有多大？”的问题。

**使用场景：**

- **概率计算**：当需要计算随机变量小于某个特定值的概率时，CDF 非常有用。直接通过 CDF 可以得到这样的概率。
- **模型评估**：CDF 也可以用来比较不同分布的表现，例如在机器学习和统计中，比较模型的误差分布。
- **区间概率**：通过 CDF，可以轻松计算随机变量落在某个区间内的概率： $ P(a \leq X \leq b) = F_X(b) - F_X(a) $

**例子：**
如果 $ X \sim N(\mu, \sigma^2) $，则其 CDF 是正态分布的累积分布函数：

$$ F_X(x) = \frac{1}{2} \left[ 1 + \text{erf}\left(\frac{x - \mu}{\sigma \sqrt{2}}\right) \right] $$

这里的 `erf` 是误差函数。

### 总结

1. 使用场景对比

   - 如果你想知道一个随机变量在某个区间内的概率，通常需要通过 PDF 进行积分来计算。
   - 如果你想要知道一个随机变量小于等于某个值的累积概率，CDF 会更加直观方便。

2. PDF 和 CDF 之间的关系

   - **数学表达上的关系：**

     假设 $ X $ 是一个连续随机变量，且其概率分布函数为 $ f_X(x) $（PDF），累积分布函数为 $ F_X(x) $（CDF）。

     1. **从 PDF 到 CDF：** 累积分布函数 $ F_X(x) 可以通过对概率密度函数 $ f_X(x) $ 进行积分得到：

        $$ F*X(x) = P(X \leq x) = \int*{-\infty}^x f_X(t) dt $$

        这表示 CDF 是 PDF 在区间 $ (-\infty, x] $ 上的累积概率。

     2. **从 CDF 到 PDF：** 如果 $ F_X(x) $ 是可导的，概率密度函数 $ f_X(x) $ 则是累积分布函数 $ F_X(x) $ 的导数：

        $$ f_X(x) = \frac{d}{dx} F_X(x) $$

        换句话说，CDF 对 $ x $ 求导得到 PDF。这个关系意味着，如果你知道了 CDF，你可以通过求导得到 PDF。

   - **实际意义上的关系：**

     - **PDF 是局部的，CDF 是累积的：**
       - **PDF** 描述了随机变量在某个点附近的概率密度。它提供了关于随机变量取值的“密集程度”信息。例如，PDF 在某个点的值告诉我们随机变量在该点附近的相对可能性，但由于它是概率密度，所以它的数值本身并不代表实际概率（实际概率是通过对区间的积分得到的）。
       - **CDF** 则描述了随机变量小于或等于某个值的累计概率。它是一个从零到一单调增加的函数，能够直观地告诉我们“随机变量小于某个值的概率是多少”。
     - **PDF 提供局部概率，CDF 提供累积概率：**
       - 通过 PDF，我们可以获得随机变量在某个小区间内（如 $ [x, x+dx] $）的概率近似值。通常，PDF 的值越高，表示随机变量在这个点附近取值的概率越大。
       - 通过 CDF，我们可以得到随机变量小于或等于某个特定值的总概率。因此，CDF 提供的是一个“累积”或“累计”的视角，回答了“随机变量小于或等于某个值的概率有多大”这样的问题。

3. 概率分布
   - **“概率分布”** 这一术语通常是泛指的，可以指代**离散型随机变量的 PMF** 或 **连续型随机变量的 PDF**。
   - 在**实际应用中**，人们通常会根据上下文来区分离散型和连续型随机变量的概率分布。对于离散型关注 PMF；对于连续型关注 PDF。

- **PDF** 提供随机变量在某一点的概率密度，描述了局部的概率分布，且它的积分（在某个区间上）给出了区间内的概率。
- **CDF** 提供了随机变量小于等于某个值的累积概率，描述了累积的概率分布，并且是一个单调递增的函数。它与 PDF 之间的关系是，CDF 是 PDF 的积分，PDF 是 CDF 的导数。

# 线性代数

## 仿射变换

仿射变换（Affine transformation）是指一种**保持直线平行性质的几何变换**，它包括**平移、旋转、缩放和剪切**等操作。在二维空间中，一个仿射变换可以用一个非奇异的二阶矩阵和一个平移向量来描述，通常表示为：

$T(\mathbf{x}) = A \mathbf{x} + \mathbf{b}$

其中，$\mathbf{x}$ 是原始点的坐标向量，$A$ 是一个 2x2 的矩阵，$\mathbf{b}$ 是一个二维平移向量。

1. 平移变换 (Translation)

   平移变换是将所有点沿某一方向移动一个固定距离。二维平移变换可以表示为：

   $$T(\mathbf{x}) = \mathbf{x} + \mathbf{b}$$

   其中，$\mathbf{x}$ 是原始点的坐标向量，$\mathbf{b}$ 是平移向量。

2. 旋转变换 (Rotation)

   旋转变换是将所有点绕一个固定点旋转一定角度。二维旋转变换可以表示为：

   $$R(\mathbf{x}) = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \mathbf{x}$$

   其中，$\theta$ 是旋转角度。

3. 缩放变换 (Scaling)

   缩放变换是按比例放大或缩小所有点。二维缩放变换可以表示为：

   $$S(\mathbf{x}) = \begin{pmatrix} s_x & 0 \\ 0 & s_y \end{pmatrix} \mathbf{x}$$

   其中，$s_x$ $s_y$ 分别是 x 和 y 方向的缩放因子。

4. 剪切变换 (Shear)

   剪切变换是将所有点沿某一方向按比例移动，使图形倾斜。二维剪切变换可以表示为：

   $$H(\mathbf{x}) = \begin{pmatrix} 1 & k_x \\ k_y & 1 \end{pmatrix} \mathbf{x}$$

   其中，$k_x$ 和 $k_y$ 分别是 x 和 y 方向的剪切因子。

5. 仿射变换 (Affine Transformation)

   仿射变换是包括上述所有变换的一种综合变换，它可以保持线的平行性。二维仿射变换可以表示为：

   $$A(\mathbf{x}) = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \mathbf{x} + \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$$

6. 齐次变换 (Homogeneous Transformation)

   齐次变换是一种在项目空间中表示变换的方法，通过引入齐次坐标，可以将仿射变换表示为矩阵乘法，从而统一各种变换。三维齐次变换表示为：

   $$T(\mathbf{x}) = \begin{pmatrix} A & \mathbf{b} \\ 0 & 1 \end{pmatrix} \mathbf{x}$$

   其中，$A$ 是一个 3x3 的矩阵，$\mathbf{b}$ 是平移向量，$\mathbf{x}$ 是齐次坐标向量。

通过这些变换，可以灵活地操纵图形和图像，实现各种效果。

## 二维旋转矩阵

假设原坐标为：$ P = (x, y) $，绕**原点**逆时针旋转角度 $ \theta $ 后得到：$ P' = (x', y') $，公式为：

$$ \begin{cases} x' = x \cos\theta - y \sin\theta \\ y' = x \sin\theta + y \cos\theta \end{cases} $$

旋转也可以写成矩阵乘法的形式：

$$ \begin{bmatrix} x' \\[2pt] y' \end{bmatrix} = \begin{bmatrix} \cos\theta & -\sin\theta \\[2pt] \sin\theta & \cos\theta \end{bmatrix} \begin{bmatrix} x \\[2pt] y \end{bmatrix} $$

这里的矩阵：

$$ R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix} $$

就是二维旋转矩阵（逆时针方向）。

## 矩阵乘法

[矩阵乘法](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%99%A3%E4%B9%98%E6%B3%95)
[矩阵的几种乘法](https://zhaoyangchen.github.io/2023/07/03/matrix-multiply.html)

1. **矩阵乘积**

   矩阵乘积(matrix product，也叫 matmul product)。

   - 符号： $ \cdot $ 或 省略（ `省略 或 $\cdot$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/ecf70982d400cece8e8f6c86ba8a4796.png)
   - 运算律：不满足交换律；满足结合律；

2. **矩阵哈德玛乘积**

   Hadamard product(又称 element\-wise product)。

   - 符号：$ \circ $ 或 $ \odot $（`$\circ$ 或 $\odot$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/dc52a6e57be0d132c5212653f6ed3f9a.png#pic_center)
   - 运算律：满足交换律；满足结合律；

3. **矩阵克罗内克积**

   Kronecker product(克罗内克积)。

   - 符号： $ \otimes $ （`$\otimes$`）
   - 公式：
     ![](https://i-blog.csdnimg.cn/blog_migrate/86f8a96edd438d4ff2ed0570ad62634b.png#pic_center)
   - 运算律：不满足交换律；满足结合律；

4. 共同性质

   上述三种乘积都符合：

   - 结合律
     $ A(BC) = (AB)C $
   - 分配律：
     $ A(B + C) = AB + AC $
     $ (A + B)C = AC + BC $
   - 和标量乘积相容：
     $ c(AB) = c(A)B $
     $ (Ac)B = A(cB) $
     $ (AB)c = A(Bc) $

## 矩阵与向量乘法

### 数学形式上的区别

1. 矩阵乘向量（列向量）：$Ax$

   - $A$ 是 $m×n$ 矩阵，$x$ 是 $n×1$ 列向量
   - 结果是 $m×1$ 列向量

2. 向量乘矩阵（行向量）：$x^T A$

   - $x^T$ 是 $1×n$ 行向量，$A$ 是 $n×m$ 矩阵
   - 结果是 $1×m$ 行向量

### 物理意义的深层本质

1. **矩阵乘向量 $Ax$**：矩阵主动变换向量
   把矩阵看作**算子/变换**，向量是**被作用的对象**。是从**输入空间到输出空间的映射**。

   - 映射：X 空间 → Y 空间
   - 物理过程：施加变换、演化、响应
   - 思维方式："**如果我这样做，会发生什么？**"

2. **向量乘矩阵 $x^T A$**：向量主动探测矩阵
   把行向量看作**测量者/观察者**，矩阵是**被观察的系统**。是在**对偶空间（余空间）中的操作**。

   - 对偶：余空间 → 余空间
   - 数学工具：测量、投影、能量计算
   - 思维方式："**如果我这样观察/测试，会得到什么？**"

3. **记忆口诀**：

   - 右乘列向量 = 主动施为（DO）
   - 左乘行向量 = 被动观察（MEASURE）

4. **直观对比**

   |          | 矩阵乘向量 $Ax$  | 向量乘矩阵 $x^T A$ |
   | :------: | :--------------: | :----------------: |
   |   角色   |  矩阵是"变换器"  |   向量是"观察者"   |
   |   方向   |   作用在输入上   |      测量输出      |
   |   组合   | A 的列的线性组合 |  A 的行的线性组合  |
   | 物理场景 |  施加变换、映射  |  测量、投影、加权  |

5. **物理含义**

   |              | $Ax$                    | $x^T A$               |
   | ------------ | ----------------------- | --------------------- |
   | **空间**     | 切空间（tangent）       | 余切空间（cotangent） |
   | **角色**     | 被作用的对象            | 作用在对象上的测量    |
   | **物理过程** | 因果关系（输入 → 输出） | 能量/功/观测          |
   | **数学性质** | 保持向量性质            | 产生线性泛函          |
   | **典型应用** | 动力学方程              | 变分原理、虚功        |

#### 例 1：三维刚体旋转

假设我们有一个旋转矩阵 R（绕 z 轴旋转 θ）：

$$
R =
\begin{bmatrix}
\cos(\theta)& -\sin(\theta)& 0 \\
\sin(\theta)& \cos(\theta) & 0 \\
0           & 0            & 1 \\
\end{bmatrix}
$$

1. **$Rv$：主动变换（Active Transformation）**

   物理含义：**把物体本身旋转**

   $$
   \begin{align*}
   初始位置： & v = [1, 0, 0]^T（指向x轴） \\
   旋转后：  & Rv = [\cos(\theta), \sin(\theta), 0]^T
   \end{align*}
   $$

   - 向量 $v$ **在固定坐标系中**被旋转
   - 这是**物体运动**：你抓住一根杆子，把它转了角度 $\theta$
   - 能量守恒：$||Rv|| = ||v||$（长度不变）
   - 物理过程：**施加力矩 → 角动量变化 → 物体旋转**

   **工程应用**：机器人正运动学，关节角求末端位置

2. **$v^T R$：被动变换（Passive Transformation）**

   物理含义：**观察者坐标系旋转**（物体不动）

   $$
   \begin{align*}
   物体在实验室坐标系: & v^T = [1, 0, 0] \\
   在旋转后的坐标系看: & v^T R^T
   \end{align*}
   $$

   等价于：**我不动物体，而是把坐标纸旋转**

   **更深层理解**：这是**余切空间**（cotangent space）中的操作

   - $v^T$ 可以理解为**线性泛函**（linear functional）
   - 它"吃掉"一个向量，输出一个数：$v^T · w = 标量$
   - $v^T R$ 是把这个"测量装置"在新坐标系重新定义

3. **物理类比**：

   - $Rv$：用力转动陀螺
   - $v^T R*$：自己围着陀螺转圈看（陀螺没动，你的视角变了）

#### 例 2：应力-应变关系

固体力学中，材料的本构关系（胡克定律）：
$$\sigma = C \varepsilon$$

- $C$：4 阶刚度张量（简化为 6×6 矩阵）
- $\varepsilon$：应变向量（6×1）
- $\sigma$：应力向量（6×1）

1. **$C \varepsilon$：因果链（物理过程）**

   $$外力 → 变形（应变 \varepsilon）→ 材料响应（应力 \sigma = C \varepsilon）$$

   **物理意义**：

   1. 你拉伸一根钢筋（施加应变）
   2. 钢筋内部原子键被拉长
   3. 原子间恢复力产生应力
   4. $C$ 描述"材料如何抵抗变形"

   这是**输入-输出关系**：

   - 输入：应变（几何量）
   - 输出：应力（力学量）
   - 矩阵：材料性质

2. **$\varepsilon^T C$：虚功原理**

   在有限元分析中，虚功：

   $$\delta W = (\delta \varepsilon)^T \sigma = (\delta \varepsilon)^T C \varepsilon$$

   **物理意义**：

   - $(\delta \varepsilon)^T$ 是**虚位移场**（测试函数）
   - $(\delta \varepsilon)^T C$ 是"虚位移感受到的广义力"
   - 这是**弱形式**（weak form）：不直接求解力，而是通过能量平衡

   **本质区别**：

   - **$C \varepsilon$**：真实物理过程（应变 → 应力）
   - **$(\delta \varepsilon)^T C$**：数学测试（用虚位移"探测"系统）

## $x^T A$ 应用解释

向量乘矩阵 $x^T A$ 的使用场景：测量、投影、加权，举例如下：

1. **核心理解：x^T A 的几何本质**

数学展开：

$$x^T A = x^T [a_1, a_2, ..., a_n] = [x^T a_1, x^T a_2, ..., x^T a_n]$$

- 其中 $a_i$ 是 $A$ 的第 $i$ 列。

**关键**：$x^T a_i$ 是**内积**（投影），结果是**标量**！

所以 $x^T A$ 的结果是：

- 用 $x$ 对 $A$ 的**每一列进行投影/测量**，得到一组数值。
- **列代表什么**，结果就是对"那个东西"的加权

### 测量（Measurement）

**方式 1：列是时间，行是空间位置**

1. 矩阵结构

   ```log
   A = [t₁时刻  t₂时刻  t₃时刻]
       [传感器1  传感器1  传感器1]  ← 第1行：传感器1在各时刻的读数
       [传感器2  传感器2  传感器2]  ← 第2行：传感器2在各时刻的读数
       [传感器3  传感器3  传感器3]  ← 第3行：传感器3在各时刻的读数
   ```

2. 具体数值示例

   **场景设置：**

   - 3 个传感器：位置 1（窗边）、位置 2（中央）、位置 3（门口）
   - 3 个时刻：上午 9 点、中午 12 点、下午 3 点
   - 空间权重：w^T = [0.5, 0.3, 0.2]（窗边影响大，门口影响小）

   **数据矩阵：**

   ```log
   A = [9点   12点   15点]
       [22°C  25°C   28°C]  ← 传感器1（窗边）
       [20°C  23°C   26°C]  ← 传感器2（中央）
       [19°C  22°C   24°C]  ← 传感器3（门口）
   ```

   **计算 $w^T A$：每个时刻的"有效室温"**

   ```log
   w^T A = [0.5, 0.3, 0.2] × [22  25  28]
                             [20  23  26]
                             [19  22  24]
         = [20.8°C, 23.8°C, 26.6°C]
   ```

   **物理解释：**

   - **原始数据**：3 个传感器 × 3 个时刻 = 9 个温度读数
   - **加权后**：每个时刻的"有效室温"
   - **测量含义**：w^T 是"温度计算策略"，窗边温度最重要（50%），中央次之（30%），门口最不重要（20%）
   - **应用**：空调系统用这个"有效温度"决定制冷强度

**方式 2：行是时间，列是空间位置**

1. 矩阵结构

   ```log
   A = [传感器1  传感器2  传感器3]
       [t₁时刻  t₁时刻  t₁时刻]  ← 第1行：t₁时刻各传感器的读数
       [t₂时刻  t₂时刻  t₂时刻]  ← 第2行：t₂时刻各传感器的读数
       [t₃时刻  t₃时刻  t₃时刻]  ← 第3行：t₃时刻各传感器的读数
   ```

2. 具体数值示例

   **场景设置：**

   - 同样的传感器和时刻
   - 但现在时间权重：w^T = [0.5, 0.3, 0.2]（最近时刻权重大）

   **数据矩阵：**

   ```log
   A = [传感器1  传感器2  传感器3]
       [22°C    20°C    19°C]  ← 9点
       [25°C    23°C    22°C]  ← 12点
       [28°C    26°C    24°C]  ← 15点
   ```

   **计算 $w^T A$：每个位置的"近期平均温度"**

   ```log
   w^T A = [0.5, 0.3, 0.2] × [22  20  19]
                             [25  23  22]
                             [28  26  24]
         = [24.1°C, 22.1°C, 20.9°C]
   ```

   **物理解释**

   - **原始数据**：同样的 9 个读数，但矩阵组织不同
   - **加权后**：每个位置的"近期平均温度"
   - **测量含义**：w^T 是时间衰减权重，最近的测量最重要（50%）
   - **应用**：预测系统用这个"近期平均"来预测未来趋势

**对比**

| 方式   | 矩阵结构         | $w^T$ 的含义 | $w^T A$ 的结果               | 应用场景           |
| ------ | ---------------- | ------------ | ---------------------------- | ------------------ |
| 方式 1 | 行=空间，列=时间 | 空间权重     | 时间序列（各时刻的综合温度） | 实时监控、空调控制 |
| 方式 2 | 行=时间，列=空间 | 时间权重     | 空间分布（各位置的近期平均） | 趋势预测、热力图   |

### 投影（Projection）

**例子 4：计算机图形学中的视图变换**

你在 3D 空间放置一个相机，相机的朝向是：

```log
相机右方向: r^T = [1, 0, 0]
相机上方向: u^T = [0, 1, 0]
相机前方向: f^T = [0, 0, -1]
```

**场景中的物体顶点矩阵 V**（每列是一个顶点的世界坐标）：

```log
V = [顶点1  顶点2  顶点3  ...  顶点n]
    (3×n)
```

**投影到相机坐标系**：

```log
V_camera = [r^T V]  =  [每个顶点的x坐标（屏幕水平）]
           [u^T V]     [每个顶点的y坐标（屏幕垂直）]
           [f^T V]     [每个顶点的深度]
```

**物理意义**：

- r^T V：测量每个顶点"向右多远"（屏幕 X 坐标）
- u^T V：测量每个顶点"向上多远"（屏幕 Y 坐标）
- f^T V：测量每个顶点的深度（用于遮挡判断）

这就是**从世界坐标投影到相机坐标**的过程！

**例子 3：主成分分析（PCA）降维**

你有 1000 个高维数据点（比如 100 维的人脸图像）。

通过 PCA 找到主方向（特征向量）：

```log
v₁^T = 第一主成分（最大方差方向）
v₂^T = 第二主成分
...
```

**数据矩阵 X**（每列是一个数据点）：

```log
X = [数据1  数据2  ...  数据1000]
    (100×1000)
```

**投影到第一主成分**：

```log
v₁^T X = [v₁^T·数据1, v₁^T·数据2, ..., v₁^T·数据1000]
```

**物理/几何意义**：

- v₁^T·数据 ᵢ 是数据点在 v₁ 方向上的**投影长度**（坐标）
- 原本 100 维的数据，现在用 1 个数表示（降维！）
- 这个数保留了"沿最大方差方向的信息"

**完整降维**：

```log
V^T X = [v₁^T X]  =  [第一主成分坐标]
        [v₂^T X]     [第二主成分坐标]
        [v₃^T X]     [第三主成分坐标]
```

从 100 维降到 3 维，保留最重要的信息。

### 加权（Weighting）

**例子 6：神经网络的线性层**

神经网络的一层：

```log
输入: x^T = [x₁, x₂, ..., xₙ]
权重矩阵: W (每列是一个神经元的权重向量)
输出: y^T = x^T W
```

**具体例子**：

```log
输入 x^T = [亮度, 对比度, 边缘强度] = [0.8, 0.6, 0.3]

权重矩阵 W = [神经元1的权重  神经元2的权重]
             [w₁₁            w₁₂          ]
             [w₂₁            w₂₂          ]
             [w₃₁            w₃₂          ]
           = [[2.0,  -1.5]
              [1.0,   0.5]
              [-0.5,  2.0]]
```

**计算 x^T W**：

```log
神经元1的激活 = 0.8×2.0 + 0.6×1.0 + 0.3×(-0.5) = 1.85
神经元2的激活 = 0.8×(-1.5) + 0.6×0.5 + 0.3×2.0 = -0.3

y^T = [1.85, -0.3]
```

**物理意义**：

- 每个神经元是一个**特征检测器**
- 神经元 1 的权重 [2.0, 1.0, -0.5] 表示它"关注亮度和对比度，抑制边缘"
- $x^T W$ 计算每个神经元对输入的**加权响应**
- 本质是**多个线性泛函的并行计算**

**例子 5：投资组合的风险评估**

你有 5 只股票，过去 250 天的收益率数据：

```log
R = [股票1的收益率序列]
    [股票2的收益率序列]
    [股票3的收益率序列]
    [股票4的收益率序列]
    [股票5的收益率序列]
    (5×250)
```

你的投资组合配置：

```log
w^T = [0.3, 0.2, 0.2, 0.15, 0.15]
      （30%买股票1，20%买股票2，...）
```

**计算组合收益率**：

```log
组合收益率序列 = w^T R

展开有：

第t天的组合收益 = 0.3×股票1(t) + 0.2×股票2(t) + ... + 0.15×股票5(t)
```

**物理意义**：

- $w^T$ 是你的**加权方案**
- $w^T R$ 把 5 个股票的每日表现，按你的持仓比例**加权合成**
- 结果是你的组合整体表现（250 个数）

**进一步分析**：

```log
组合方差 = Var(w^T R)
组合夏普比率 = Mean(w^T R) / Std(w^T R)
```

这就是现代投资组合理论（MPT）的数学基础。

---

**综合例子：有限元分析中的虚功**

**真实系统**：

```log
刚度矩阵 K × 位移 u = 外力 f
Ku = f
```

**虚功原理**（弱形式）：

对任意虚位移 δu，要求：

```log
(δu)^T K u = (δu)^T f
```

**物理含义**：

- δu^T K：虚位移场感受到的"内力响应"
- (δu)^T K u：虚位移在内力上做的虚功
- (δu)^T f：虚位移在外力上做的虚功
- 平衡条件：内虚功 = 外虚功

**为什么用 δu^T K**？

因为我们不是真的施加位移 u（那是 Ku），而是用**测试函数 δu** 去"试探"系统：

- "如果我给系统一个微小虚位移 δu，能量平衡吗？"
- 这是**加权残差法**的本质

---

**总结对比**

| 概念     | 数学形式                      | 物理意义                               | 典型应用             |
| -------- | ----------------------------- | -------------------------------------- | -------------------- |
| **测量** | x^T A = [x^T a₁, x^T a₂, ...] | 用 x 对 A 的每列进行测量，得到一组读数 | 传感器融合、信号检测 |
| **投影** | x^T aᵢ = \|x\| \|aᵢ\| cos(θ)  | aᵢ 在 x 方向上的投影长度               | PCA 降维、坐标变换   |
| **加权** | x^T A = Σ xᵢ × (第 i 行)      | 按 x 的分量对 A 的行加权求和           | 投资组合、神经网络   |

---

**记忆技巧**

**x^T A** 的本质：**x 是观察者/测量器**

- **测量**：用 x 这个"仪器"测量 A 的各列
- **投影**：A 的各列在 x 方向的影子
- **加权**：用 x 的分量作为权重，对 A 的行进行线性组合

**关键直觉**：

```log
Ax：A 作用在 x 上（主动变换）
x^T A：x 作用在 A 上（主动测量）
```

就像：

- Ax：医生给病人治疗
- x^T A：医生用听诊器检查病人

这就是为什么在变分法、有限元、机器学习中，左乘和右乘有本质区别！

# 被动变换

1. 为什么被动变换能达到相同效果

   **直觉例子：读指南针**

   想象你拿着一个指南针：

   **场景 A：主动变换**

   ```log
   你站着不动
   指南针顺时针转 90°
   原来指向北的指针，现在指向东
   ```

   **场景 B：被动变换**

   ```log
   指南针不动
   你逆时针转 90°（相当于坐标系旋转）
   从你的视角看，指针从指向"你的前方"变成指向"你的右方"
   ```

   **结果：指针相对于你的方向完全相同！**

2. 数学推导

   - **主动变换**

     向量 $v$ 逆时针旋转 $\theta$ 后得到向量 $v'$，二者满足：

     $$
     \begin{align*}
     v'   & = R(\theta) v \\
     R(\theta) & =
     \begin{bmatrix}
     \cos(\theta) & -\sin(\theta) \\
     \sin(\theta) &  \cos(\theta) \\
     \end{bmatrix}
     \end{align*}
     $$

   - **被动变换**

     坐标系顺时针旋转 $\theta$，向量 $v$ 在新坐标系中的表示 $v'$ 推导如下：

     设原坐标系基向量：

     $$
     \vec{e_1} = [1, 0]^T, \\
     \vec{e_2} = [0, 1]^T
     $$

     新坐标系基向量（顺时针旋转 $\theta$）：

     $$
     \begin{align*}
     \vec{e'_1} &= R(-\theta) \vec{e_1} \\
     \vec{e'_2} &= R(-\theta) \vec{e_2}
     \end{align*}
     $$

     向量 $v$ 在原坐标系：$v = v_1 \vec{e_1} + v_2 \vec{e_2}$，将上式代入此式有：

     $$
     v = v_1 R(-\theta)^{-1} \vec{e'_1} + v_2 R(-\theta)^{-1} \vec{e'_2}
     $$

     其中 $[v_1 R(-\theta)^{-1}, v_2 R(-\theta)^{-1}]$ 即为新坐标：

     $$
     v R(-\theta)^{-1} = v'
     $$

     由 $R(-\theta) = R(\theta)^{T}$，得到：

     $$
     v' = R(\theta) v
     $$

   - **结论：**
     主动变换和被动变换可以达到相同的效果。$v' = R(\theta) v$

3. 计算机图形学中的视图变换详解

   **问题：如何把世界坐标转换为相机坐标？**

   **场景设置：**

   ```log
   世界坐标系：固定不动
   相机：可以移动和旋转
   物体：在世界坐标系中的位置已知
   ```

   **目标：**
   求物体在相机坐标系中的坐标

   **相机的新坐标系基向量（在世界坐标系中表示）：**

   ```log
   相机向左转90° = 绕Y轴逆时针旋转90°

   相机右向量 r：原来是[1,0,0]，旋转后是[0,0,1]
   相机上向量 u：始终是[0,1,0]（没绕X轴转）
   相机前向量 f：原来是[0,0,-1]，旋转后是[-1,0,0]
   ```

   **视图矩阵：**

   ```log
   V = [r^T]   [0   0   1]
       [u^T] = [0   1   0]
       [f^T]   [-1  0   0]
   ```

   **变换物体：**

   ```log
   P_camera = V × P_world
            = [0   0   1]   [10]   [0]
              [0   1   0] × [0]  = [0]
              [-1  0   0]   [0]    [-10]
   ```

   **物理意义**

   原来物体在你**右边** 10 米

   相机向左转 90° 后，物体现在在你**后面** 10 米（Z = -10）

   这完全符合直觉！

## **6. 为什么 v^T A 用于投影？**

回到投影的本质：

### **投影 = 测量在某个方向上的分量**

```
相机右向量：r^T = [0, 0, 1]
物体位置：P = [10, 0, 0]

物体在相机右方向的分量：
r^T · P = 0×10 + 0×0 + 1×0 = 0
```

意思是：物体在相机的右方向上没有分量（在正前方）

```
相机前向量：f^T = [-1, 0, 0]
f^T · P = (-1)×10 + 0×0 + 0×0 = -10
```

意思是：物体在相机后方 10 米

### **为什么是 r^T 而不是 r？**

因为投影是**内积**（配对）：

```
投影长度 = ⟨r, P⟩ = r^T P
```

这是余切空间对切空间的作用！

## **7. 完整的视图变换流程**

```log
步骤1：世界坐标
物体位置：P_world = [x, y, z]

步骤2：视图变换（被动变换，坐标系旋转）
P_camera = [r^T]
           [u^T] × P_world
           [f^T]

步骤3：投影到屏幕
每一行的计算：
  屏幕X坐标 = r^T · P_world（测量"向右多少"）
  屏幕Y坐标 = u^T · P_world（测量"向上多少"）
  深度      = f^T · P_world（测量"多远"）
```

---

## **8. 对比表：主动 vs 被动**

| 特性           | 主动变换（Av）       | 被动变换（v^T A）    |
| -------------- | -------------------- | -------------------- |
| **物理过程**   | 旋转物体             | 旋转坐标系           |
| **矩阵形式**   | R(θ)                 | R^T(θ) = R(-θ)       |
| **几何意义**   | 向量在空间中转动     | 观察视角改变         |
| **图形学应用** | 模型变换（旋转物体） | 视图变换（移动相机） |
| **结果空间**   | 仍是切空间（向量）   | 余切空间（坐标）     |

## **为什么图形学用被动变换**

1. 直观性

   在游戏/3D 软件中：

   - 物体在世界中的位置固定
   - 我们移动相机来观察
   - 用被动变换直接对应"移动相机"的操作

2. 效率

   场景有 100 万 个顶点，只有 1 个相机

   - 主动方式：移动相机 = 变换 100 万个顶点
   - 被动方式：移动相机 = 改变 1 个视图矩阵

   被动方式更高效！

3. 组合性

   可以把平移、旋转、缩放组合成一个矩阵：

   ```log
   V = T × R × S  （视图矩阵）
   ```

   然后一次性变换所有顶点：

   ```log
   P_camera = V × P_world
   ```

   而不需要分别对每个物体做变换。

## **11. 深入理解：齐次坐标中的变换**

在 3D 图形学中，我们实际使用 **4×4 齐次矩阵**：

### **完整的视图变换矩阵**

```log
V = [r_x  r_y  r_z  -r·C]
    [u_x  u_y  u_z  -u·C]
    [f_x  f_y  f_z  -f·C]
    [0    0    0     1  ]
```

其中：

- r, u, f 是相机坐标系的三个轴（单位向量）
- C 是相机在世界坐标系中的位置
- -r·C, -u·C, -f·C 是平移项

**为什么有负号？**：因为这是**逆变换**！相机从原点移动到位置 C，等价于：世界向相反方向移动 -C。

**完整例子：相机在 (5, 2, 3)，看向原点**

**步骤 1：确定相机坐标系**

```log
相机位置：C = [5, 2, 3]
目标点：Target = [0, 0, 0]

前向量（从相机指向目标）：
f_unnorm = Target - C = [-5, -2, -3]
f = normalize(f_unnorm) = [-0.802, -0.321, -0.481]

上向量（假设世界上方向）：
world_up = [0, 1, 0]

右向量（叉乘）：
r = normalize(f × world_up) = [0.857, 0, -0.514]

上向量（重新计算，保证正交）：
u = r × f = [0.165, 0.947, -0.275]
```

**步骤 2：构造视图矩阵**

```log
V = [0.857   0      -0.514   -(r·C)]
    [0.165   0.947  -0.275   -(u·C)]
    [-0.802  -0.321 -0.481   -(f·C)]
    [0       0       0        1     ]

计算平移项：
-r·C = -(0.857×5 + 0×2 + (-0.514)×3) = -2.743
-u·C = -(0.165×5 + 0.947×2 + (-0.275)×3) = -1.893
-f·C = -(-0.802×5 + (-0.321)×2 + (-0.481)×3) = 6.897

最终：
V = [0.857   0      -0.514   -2.743]
    [0.165   0.947  -0.275   -1.893]
    [-0.802  -0.321 -0.481    6.897]
    [0       0       0        1     ]
```

**步骤 3：变换原点**

```log
P_world = [0, 0, 0, 1]^T（齐次坐标）

P_camera = V × P_world = [-2.743, -1.893, 6.897, 1]^T
```

**物理意义：**

- 原点在相机坐标系中位于 (-2.743, -1.893, 6.897)
- Z = 6.897 > 0：原点在相机前方（符合 f 指向目标）
- 这正是"从 C 看向原点"的逆变换结果！

## **12. 对偶视角：为什么行向量是"测量器"？**

回到核心问题：**为什么 v^T A 中的 v^T 是"测量"？**

### **几何解释：投影作为测量**

**行向量 v^T 定义了一个"测量方向"：**

```log
v^T = [v₁, v₂, v₃]
```

对任意向量 w，计算：

```log
v^T · w = v₁w₁ + v₂w₂ + v₃w₃
```

这个数值是 **w 在 v 方向上的投影长度**。

### **视图变换中的应用**

```log
r^T = [0.857, 0, -0.514]  （相机右方向）
```

当我们计算 r^T · P 时，我们在问：

> "点 P 在相机右方向上有多少分量？"

答案就是屏幕的 X 坐标！

### **为什么不是 r · P（列向量的点乘）？**

在数学上，列向量的点乘需要转置：

```log
r · P = r^T P  （必须转置才能做内积）
```

所以本质上还是 r^T！

**但在概念上：**

- **r**（列向量）：表示"一个方向"（切空间）
- **r^T**（行向量）：表示"一个测量器"（余切空间）

虽然数值相同，**物理角色不同**！

## **13. 完整的图形管线：变换序列**

### **从模型到屏幕的完整旅程**

```log
1. 局部坐标（Local/Model Space）
   ↓ 模型矩阵 M（主动变换：旋转、缩放物体）

2. 世界坐标（World Space）
   ↓ 视图矩阵 V（被动变换：移动相机）

3. 相机坐标（View/Eye Space）
   ↓ 投影矩阵 P（透视投影）

4. 裁剪坐标（Clip Space）
   ↓ 透视除法

5. 标准化设备坐标（NDC）
   ↓ 视口变换

6. 屏幕坐标（Screen Space）
```

**矩阵组合**

```
P_clip = P × V × M × P_local
```

**关键点：**

- **M**（模型矩阵）：主动变换（Av）
- **V**（视图矩阵）：被动变换（v^T A 的概念）
- 虽然都是矩阵乘法，但物理意义完全不同！

**视图变换的例子**

```log
V = [r^T]   （每一行是一个测量器）
    [u^T]
    [f^T]

P_camera = V × P_world
         = [r^T · P_world]   （X坐标：向右的投影）
           [u^T · P_world]   （Y坐标：向上的投影）
           [f^T · P_world]   （Z坐标：深度）
```

每一行 **独立地** 对 P_world 进行测量！

# 工程概念

## numpy 数组中轴

### 数组形状与轴维度的关系

- 数组的形状（shape）描述了每个轴的维度。对于形状为 `(m, n)` 的二维数组：
  - `m` 表示 `axis 0` 上的元素数量（**即行数**），沿着这个轴可以访问不同的行。
  - `n` 表示 `axis 1` 上的元素数量（**即列数**），沿着这个轴可以访问不同的列。
- 在更高维的数组中，每个维度对应于特定的轴，形状的每个数字表示相应轴上的元素数量。

**举例说明**：

- 一维数组:
  - `a = np.array([1, 2, 3, 4])`
  - 形状为 `(4,)`，表示这个数组只有一个轴（`axis 0`），该轴上有 4 个元素。
- 二维数组:
  - `b = np.array([[1, 2, 3], [4, 5, 6]])`
  - 形状为 `(2, 3)`，表示这个数组有两个轴：
    - `axis 0` 有 2 个元素（即有 2 行）。
    - `axis 1` 有 3 个元素（即每行有 3 列）。
- 三维数组:
  - `c = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])`
  - 形状为 `(2, 2, 3)`，表示这个数组有三个轴：
    - `axis 0` 有 2 个元素（即最外层的两个矩阵）。
    - `axis 1` 有 2 个元素（即每个矩阵中的两行）。
    - `axis 2` 有 3 个元素（即每行中的三个值）。

### 轴的意义与作用

1. **数据处理**

   - 在进行操作时，指定轴可以控制操作沿着哪个维度进行。例如，求和、平均、最大值、最小值等操作可以沿特定轴进行，得到不同的结果。
   - 示例: 对二维数组 A 进行操作：
     - `np.sum(A, axis=0)`: 沿着轴 0 求和（压缩行），结果是对每一列求和。
     - `np.sum(A, axis=1)`: 沿着轴 1 求和（压缩列），结果是对每一行求和。

2. **数据表示**

   - 轴提供了理解数据形状和结构的途径。在多维数据中，轴决定了数据的排列方式。例如，在图像处理中，常见的图像张量通常是 `(height, width, channels)`，其中 `height` 对应于轴 0，`width` 对应于轴 1，`channels` 对应于轴 2。

3. **广播机制**

   - 轴的概念在广播机制中也很重要。广播允许不同形状的数组在一起进行运算时，沿某个轴自动扩展维度以适配另一个数组。

## matlab 与 numpy 中的轴

### MATLAB 中数组的轴

> **每新增一个轴，原来的轴保持不变，新增的轴变为新的维度，看起来轴是在尾部新增的。**

MATLAB 中的数组是**列优先的**（column-major order），轴的编号从 1 开始。多维数组中的轴表示如下：

- **第 1 轴 (axis 1)**: 行方向（row），即沿着第一个维度变化。
- **第 2 轴 (axis 2)**: 列方向（column），即沿着第二个维度变化。
- **第 3 轴 (axis 3)**: 第三维度方向，依此类推。

例如，一个 $3 \times 4 \times 5$ 的数组：

- 第 1 轴（axis 1）长度为 3（行数）。
- 第 2 轴（axis 2）长度为 4（列数）。
- 第 3 轴（axis 3）长度为 5。

### NumPy 中数组的轴

> **新增的轴总是添加在最前面，变成 0 轴，原来的轴依次加 1。**

NumPy 中的数组是**行优先的**（row-major order），轴的编号从 0 开始。多维数组中的轴表示如下：

- **第 0 轴 (axis 0)**: 行方向（row），即沿着第一个维度变化。
- **第 1 轴 (axis 1)**: 列方向（column），即沿着第二个维度变化。
- **第 2 轴 (axis 2)**: 第三维度方向，依此类推。

例如，一个形状为 `(3, 4, 5)` 的 NumPy 数组：

- 第 0 轴（axis 0）长度为 3（行数）。
- 第 1 轴（axis 1）长度为 4（列数）。
- 第 2 轴（axis 2）长度为 5。

### 总结

- MATLAB 轴的编号从 **1** 开始，NumPy 轴的编号从 **0** 开始。
- MATLAB 的第`n`轴对应 NumPy 的第 `n-1` 轴。

举个例子：

- 在 MATLAB 中，对一个二维数组 `A`，`A(2, :)` 取第二行数据，对应 NumPy 中的 `A[1, :]`。
- 在 MATLAB 中，`sum(A, 2)` 是沿着列（第 2 轴）求和，对应 NumPy 中的 `np.sum(A, axis=1)`。

## numpy 中 axis=-1

在 NumPy 中，`axis=-1`表示数组的最后一个轴。这是一种便捷的方式来引用数组的最后一个维度，而不需要知道数组具体有多少维。

- 对于一维数组，`axis=-1`等同于`axis=0`
- 对于二维数组，`axis=-1`等同于`axis=1`
- 对于三维数组，`axis=-1`等同于`axis=2`
- 以此类推...

假设我们有以下数组：

```python
import numpy as np

# 创建一个3x4x5的数组
arr = np.array([[[1, 2, 3, 4, 5],
                 [6, 7, 8, 9, 10],
                 [11, 12, 13, 14, 15],
                 [16, 17, 18, 19, 20]],

                [[21, 22, 23, 24, 25],
                 [26, 27, 28, 29, 30],
                 [31, 32, 33, 34, 35],
                 [36, 37, 38, 39, 40]],

                [[41, 42, 43, 44, 45],
                 [46, 47, 48, 49, 50],
                 [51, 52, 53, 54, 55],
                 [56, 57, 58, 59, 60]]])
```

这个数组的形状是 `(3, 4, 5)`，即有 3 个矩阵，每个矩阵有 4 行 5 列。

如果我们对这个数组沿着 `axis=-1` 进行求和：

```python
sum_along_last_axis = np.sum(arr, axis=-1)
print(sum_along_last_axis)
```

输出将是一个形状为 `(3, 4)` 的数组：

```python
[[ 15  40  65  90]
 [115 140 165 190]
 [215 240 265 290]]
```

**详细解释：**

- **原始数组形状为 `(3, 4, 5)`**，表示有 3 个 4x5 的矩阵。
- **`axis=-1`** 表示最后一个轴（即每个 5 元素的列），沿着这个轴进行求和。
- 求和的结果是在每个 5 个元素的集合上进行的，因此原始的 `(3, 4, 5)` 数组变成了 `(3, 4)` 数组，最后一个轴消失，剩下的元素是沿着最后一个轴求和的结果。

## numpy 中多个轴做规约

假设我们有一个三维数组，我们想沿着最后两个轴计算均值。

```python
import numpy as np

# 创建一个形状为 (2, 3, 4) 的三维数组
arr = np.array([[[1, 2, 3, 4],
                 [5, 6, 7, 8],
                 [9, 10, 11, 12]],

                [[13, 14, 15, 16],
                 [17, 18, 19, 20],
                 [21, 22, 23, 24]]])

# 沿着轴1和轴2计算均值
mean_across_axes = np.mean(arr, axis=(1, 2))
print("Mean across axis 1 and 2:\n", mean_across_axes)
```

输出：

```log
Mean across axis 1 and 2:
 [ 6.5 18.5]
```

解释：

- **输入数组 `arr`** 的形状为 `(2, 3, 4)`，表示有 2 个 3x4 的矩阵。
- **`axis=(1, 2)`** 表示我们希望沿着第二个和第三个轴（即每个矩阵的行和列）计算均值。
- 对于第一个矩阵（形状 `(3, 4)`），其元素的均值为 6.5。第二个矩阵，其元素的均值为 18.5。
- 结果是一个一维数组，形状为 `(2,)`，每个元素对应于原数组中沿着指定轴计算的均值。

**更多示例**：

1. **计算二维数组中最后两个轴的均值**：

   ```python
   arr = np.random.rand(4, 5, 6)
   mean_across_last_two_axes = np.mean(arr, axis=(-2, -1))
   print("Mean across last two axes:\n", mean_across_last_two_axes)
   ```

   - 此例子中，`axis=(-2, -1)` 表示沿着倒数第二和最后一个轴（即二维数组的行和列）计算均值。

2. **保持原维度**：

   ```python
   mean_with_keepdims = np.mean(arr, axis=(1, 2), keepdims=True)
   print("Mean with keepdims:\n", mean_with_keepdims)
   ```

   - 设置 `keepdims=True` 时，输出数组保持原维度，只是沿着指定轴计算的均值位置的维度被压缩为 1。
