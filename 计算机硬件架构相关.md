# 计算机硬件架构相关

[toc]



1. 计算机体系结构基础

   - **冯诺依曼结构的基本组件**：输入设备、输出设备、控制器、运算器、存储器
   - **CPU主要由控制器与运算器组成**
   - **CPU运作原理分为四步：**指令提取、指令解码、指令执行、结果写回
   - **CPU指令类型主要有三种：**数据传输、算数与逻辑运算、分支跳转



   - **CPU ( Central Processing Unit):** 中央处理单元
   - **ALU ( Arithmetic Logic Unit):** 算术逻辑单元
   - **IR (Instruction Register):** 指令寄存器
   - **ID (Instruction Decoder):** 指令解码器
   - **OC (Operation Controller):** 操作控制器

2. 存储层级与Cache基本原理

   1. Cache的工作原理是基于程序访问的局部性。 包括`时间局部性`和`空间局部性`：

      对大量典型程序运行情况的分析结果表明，

        在一个较短的时间间隔内，由程序产生的地址往往集中在存储器逻辑地址空间的很小范围内。指令地址的分布本来就是连续的，再加上循环程序段和子程序段要重复执行多次。因此，对这些地址的访问就自然地具有时间上集中分布的倾向。--> **时间局部性，主要是指令**

      　数据分布的这种集中倾向不如指令明显，但对数组的存储和访问以及工作单元的选择都可以使存储器地址相对集中。这种对局部范围的存储器地址频繁访问，而对此范围以外的地址则访问甚少的现象，就称为程序访问的局部性。 --> **空间局部性，主要是数据**

      > 链接：[Cache的原理、设计及实现](https://www.cnblogs.com/hackvilin/p/3251986.html)
      >
      > 根据程序的局部性原理，可以在主存和CPU通用寄存器之间设置一个高速的容量相对较小的存储器，把正在执行的指令地址附近的一部分指令或数据从主存调入这个存储器，供CPU在一段时间内使用。这对提高程序的运行速度有很大的作用。这个介于主存和CPU之间的高速小容量存储器称作高速缓冲存储器(Cache)。
      >
      > 系统正是依据此原理，不断地将与当前指令集相关联的一个不太大的后继指令集从内存读到Cache，然后再与CPU高速传送，从而达到速度匹配。
      >
      > CPU对存储器进行数据请求时，通常先访问Cache。由于局部性原理不能保证所请求的数据百分之百地在Cache中，这里便存在一个命中率。即CPU在任一时刻从Cache中可靠获取数据的几率。
      >
      > 命中率越高，正确获取数据的可靠性就越大。一般来说，Cache的存储容量比主存的容量小得多，但不能太小，太小会使命中率太低；也没有必要过大，过大不仅会增加成本，而且当容量超过一定值后，命中率随容量的增加将不会有明显地增长。
      >
      > 只要Cache的空间与主存空间在一定范围内保持适当比例的映射关系，Cache的命中率还是相当高的。
      >
      > 一般规定`Cache与内存的空间比`为`4：1000`，即128kB Cache可映射32MB内存；256kB Cache可映射64MB内存。在这种情况下，命中率都在`90％以上`。至于没有命中的数据，CPU只好直接从内存获取。获取的同时，也把它拷进Cache，以备下次访问。

   2. 存储层级

      主流存储技术如下：

      - SRAM，作为Cache
      - DRAM，作为Main Memory
      - Flash，作为Storage
      - Magnetic disk，作为Storage

      |     Memory technology      |   Typical access time   | $ per GiB in 2012 |
      | :------------------------: | :---------------------: | :---------------: |
      | SRAM semiconductor memory  |       0.5-2.5 ns        |   \$500-\$1000    |
      | DRAM semiconductor memory  |        50-70 ns         |     \$10-\$20     |
      | Flash semiconductor memory |     5,000-50,000 ns     |   \$0.75-\$1.00   |
      |       Magnetic disk        | 5,000,000-20,000,000 ns |   \$0.05-\$0.10   |

   3. Cache 基本原理

      Cache Hit：缓存中找到想要的数据

      Cache Miss：缓存中无想要的数据

      Cache 有三种组织形式：

      - 直接映射（Direct Mapping）格式：主存字块标志，cache字块索引，块内地址

        主存中的每一块只能被放置到Cache中的唯一的一个位置。其特点是空间利用率低，冲突概率最高，实现最简单。

        - Pros:查找快，一次寻址，有就是有，没有就是没有，不啰嗦（因为只需要验证一个Cache Line中是否存在该地址）
        - Cons:命中率低，CPU经常需要相邻地址的数据，而根据规则，同属于第N个Cache Line的数据会互相排斥，不会同时出现在缓存里

        ![img](https://pic2.zhimg.com/80/v2-b3d111caabc93c638bb08bde5026d711_720w.webp)

      - 全相连（Fully Associative）。格式：主存字块标志，块内地址

        主存中的任一块可以被放置到Cache中的任意一个位置。其特点是空间利用率高，冲突概率最低，实现最复杂。

        - 优点： 十分灵活、命中率高，减少了块冲突率
        - 缺点： 所需逻辑电路多且复杂，成本高

        ![img](https://pic3.zhimg.com/80/v2-1e61e8d13030ed4f0b42c2d1a854ffce_720w.webp)

      - 组相连（n-Way Set Associative）格式：主存字块标志，组号，块内地址

        主存中的每一块可以被放置到Cache中的唯一的一个组中的任何一个位置。组相联映射是直接映射和全相联映射的一种折中。

        有2路相连、4路相连、8路相连、...等。

        - 可以充分发挥前两种方式的优势。既可以存在相邻内存中的数据以提高命中，同时也一定程度上减少了查找范围，提升查找效率。

        ![img](https://pic4.zhimg.com/80/v2-9db10cd5b86e5a10f08980ab1d1cfc07_720w.webp)

      > 链接：[Cache的组织形式](https://zhuanlan.zhihu.com/p/27653161?from_voters_page=true)
      >
      > Cache通常理解为`一个表`，表中的每个条目记录了一个`内存块的数据`及这个`内存块的起始地址`。这样一个条目称为一个`Cache line`。一个Cache line存储的数据大小（即内存块的大小）在一个系统中通常是固定的，即每个cache line都是等长的。在x86系统中，这个数据是`64Bytes`。这也正常是一次`DDR内存访问`能得到的数据大小。
      >
      > 一个Cache line不但要记录数据，还要记录数据对应的地址，通常只记录`地址的一部分`，比如说在32位系统中，Cache line为64Bytes的情况下，只要记录32 - 6 = 26个高位就好了。这里的5取自2^6=64。
      >
      >
      >
      > [几句话说清楚2:CPU缓存的组织形式](https://decodezp.github.io/2018/11/25/quickwords2-cacheassociativity/)
      >
      > [Cache的基本原理](https://zhuanlan.zhihu.com/p/102293437)
      >
      > [Cache组织方式](https://zhuanlan.zhihu.com/p/107096130)
      >
      > [图解 | CPU-Cache](https://zhuanlan.zhihu.com/p/492935813)
      >
      > [计算机组成原理存储体系 ------------ 主存与Cache的地址映射](https://juejin.cn/post/7177399768111382583)
      >
      > [Cache与主存的三种地址映射详细解读](https://blog.csdn.net/aiden_kevin/article/details/113198046)
      >
      > [计算机组成原理 Cache超仔细详解 期末一遍过](https://blog.csdn.net/qq_43663263/article/details/102797565)



      补充完整图片：
    
      概念介绍：
    
      1、`块`：主存分块，Cache也分块（叫 Cache Line），两者大小相同，都是有若干连续字组成。
    
      2、分块的大小与组织形式，决定了地址划分方法。
    
      cache size是64 Bytes并且cache line size是8 bytes。offset、index和tag分别使用3 bits、3 bits和42 bits（假设地址宽度是48 bits）
    
      注：512 Bytes cache size，64 Bytes cache line size。根据之前的地址划分方法，offset、index和tag分别使用6 bits、3 bits和39 bits。如下图所示。
    
      参考：
    
      ![img](https://static.mianbaoban-assets.eet-china.com/xinyu-images/MBXY-CR-38142df952037027d17932b669a5ec41.png)
    
      ![img](https://pic2.zhimg.com/80/v2-f6fdf760d314f146941e2192957f1a81_720w.webp)
    
      ![img](https://img-blog.csdnimg.cn/20210202211234409.png)
    
      ![img](https://img-blog.csdnimg.cn/20210202211310167.png)
    
      ![img](https://img-blog.csdnimg.cn/20210202211329845.png)
    
      ![img]()

   4. 并行计算及编程思想

      - 时间并行的主要技术之一是**流水线**

        假设 single cycle cpu 的吞吐量为1，那么理想情况下，5级流水线的最大吞吐量为5。有数据依赖，可以采用停止流水线直到结果写回或采用数据前送的方法。

      - 空间并行的主要技术之一是**SIMD**，多核/多处理器

        对于相互独立的数据，可以通过增加硬件资源来提高计算性能。

      - 针对特定的应用场景，可以进行**专用的硬件设计**，我们称之为DSA（Domain Specific Architecture)： **领域专用架构**

        对特定软件业务场景而设计的专用硬件架构，Davinci就是一种DSA的硬件实现，用于深度学习加速领域。

        DSA编程思想与传统CPU编程思想对比：

        - CPU的MemoryHierarchy主要为Cache，程序员`不需要`显式管理数据转移。
        - Davinci的MemoryHierarchy主要为Buffer，程序员`需要`显式管理数据转移。

   5. 深度学习基础

      深度学习是机器学习的子集，机器学习是人工智能的子集。

      神经网络基础算子：卷积、全连接层、池化pooling、激活函数ReLU。





# 高性能计算

高性能计算中的归约算法实现与优化：<https://www.ydma.com/portal.php?mod=view&aid=3642&page=1&mobile=no>





## CPU/GPU/NPU中各种单元

在 CPU、GPU、NPU 或类似的处理器架构中，IFU、IDU、OOO、ISU 和 EXU 代表了指令执行过程中不同阶段的硬件单元。尽管这些缩写和概念在不同的架构中可能有所变化，但在现代处理器设计（包括 CPU、GPU 和 NPU）中，它们往往代表类似的功能。以下是对这些名词的猜测和解释：

### 1. **IFU (Instruction Fetch Unit)** —— **指令获取单元**

负责从内存或指令缓存中获取（**fetch**）下一条要执行的指令。在流水线处理器架构中，这是处理流程的第一步。IFU 的主要功能是确保处理器在需要执行新指令时能够快速从内存中获取指令。

- **CPU**：在现代 CPU 中，指令获取单元从内存或指令缓存中获取要执行的指令。由于 CPU 通常是串行执行的流水线结构，IFU 在执行的每个时钟周期都会尝试获取下一条指令，确保流水线中不断有指令进入。
- **GPU**：GPU 的指令获取单元类似，虽然 GPU 主要是并行执行大量相似的指令（如在 CUDA 线程中执行同一个内核），但 IFU 仍然负责为每个线程块获取指令。GPU 的 IFU 通常会有更多并行性，因为它需要为多个执行单元同时获取指令。

### 2. **IDU (Instruction Decode Unit)** —— **指令解码单元**

负责对获取到的指令进行解码（**decode**）。在这个阶段，处理器会解析指令的操作码和操作数，并确定接下来该如何执行这条指令。对于矢量指令或深度学习加速器中的特殊指令集，解码单元还会识别指令的类型（如矩阵运算、卷积操作等）。

- **CPU**：指令解码是 CPU 处理中的关键一步，CPU 需要解析指令操作码，并确定指令的操作数和执行路径。现代复杂指令集架构（如 x86）可能会涉及到多步骤的解码过程，尤其是高级优化如微操作（micro-ops）解码。
- **GPU**：GPU 的指令解码单元相对简单，因为 GPU 的指令集通常专注于并行计算，指令类型较少且更一致化。IDU 负责将指令翻译为实际的硬件操作，如矩阵运算、向量计算等。

### 3. **OOO (Out-of-Order Execution)** —— **乱序执行单元**

代表**乱序执行**（Out-of-Order Execution），这是现代高性能处理器中的一个常见特性。乱序执行允许处理器不必按照指令原始的顺序执行，而是根据数据依赖性和资源可用性，动态调整指令的执行顺序，以提高执行效率。NPU 中可能也采用类似机制来处理大量并行计算任务。

- **CPU**：这是高性能 CPU 的核心特性之一。乱序执行允许 CPU 在遇到数据依赖时，继续执行其他不依赖于该数据的指令，以提高整体吞吐量。通过硬件调度器，CPU 可以动态调整指令的执行顺序，从而避免流水线停顿。Intel 和 AMD 的高端 CPU 都支持乱序执行。
- **GPU**：大多数 GPU 不支持传统意义上的乱序执行，因为它们的设计目标是执行大量并行的相同指令集。在 GPU 中，流水线通常是精心设计的，有着非常高的并行度。因此，GPU 更多依赖硬件多线程（如 NVIDIA 的 CUDA warp）来隐藏延迟，而不是通过乱序执行来优化指令流。

### 4. **ISU (Issue Unit)** —— **指令发射单元**

负责从已经解码并准备执行的指令队列中选择可用的指令，并将其发射到合适的执行单元（**issue**）。在这里，ISU 会根据处理器的资源和指令的依赖关系，安排指令进入执行阶段。这个过程确保指令能够合理安排到不同的执行单元中，避免资源冲突或数据依赖问题。

- **CPU**：ISU 负责将解码后的指令分发到合适的执行单元。通常在乱序 CPU 中，ISU 会根据当前可用的执行单元（如整数、浮点、向量执行单元等）和指令的资源需求来调度指令。不同指令可以并行发射到多个执行单元中。
- **GPU**：在 GPU 中，指令发射通常会对成千上万个线程进行调度和分发。GPU 的 ISU 将线程块（thread block）的指令调度到多个并行执行的核心上。由于 GPU 注重并行性，这个过程通常需要同步大量指令。

### 5. **EXU (Execution Unit)** —— **执行单元**

执行单元是实际执行指令的硬件模块。在这里，指令的操作会真正被执行，比如加法、乘法、逻辑运算、矩阵运算或神经网络相关的运算（如卷积、激活函数等）。现代处理器架构中可能包含多个执行单元来并行处理任务，如整数执行单元（ALU）、浮点执行单元（FPU）、矢量执行单元（SIMD/AVX）等。

- **CPU**：执行单元是 CPU 中真正进行计算的部分，通常包括 ALU（算术逻辑单元）、FPU（浮点运算单元）、矢量处理单元等。CPU 的 EXU 通常支持复杂的算术和逻辑操作，以及不同的数据类型（整数、浮点数、向量等）。
- **GPU**：GPU 的执行单元专门针对大规模并行计算进行优化，通常包括多个专门处理浮点运算和向量运算的单元。GPU 的 EXU 主要用于处理图形渲染、矩阵乘法、卷积等计算密集型任务。每个执行单元通常负责多个线程的操作，具备高度并行性。

### 整体流程解析：

- **IFU** 获取指令后交给 **IDU** 进行解码，解析出指令的具体操作。
- **OOO** 阶段可能会对指令重新排序，以优化执行效率，尽量减少流水线停顿（pipeline stalls）和资源冲突。
- **ISU** 决定何时将指令分派到执行单元，并发射指令到可用的 **EXU** 中进行真正的计算。

这种流水线的设计允许指令并行执行，减少等待时间，提高了处理器的整体性能。在 NPU 中，类似的架构可能会被用来处理神经网络中的矩阵运算、卷积计算和其他涉及大量数据并行处理的任务。

### 总结：

这些名词在 **CPU** 和 **GPU** 中都有对应的实现，功能相似但用途有所不同：

- **CPU**：专注于串行指令的高效执行，更多依赖乱序执行、复杂的指令解码以及多样化的执行单元。
- **GPU**：专注于大规模的并行计算，简化了乱序执行和复杂的指令解码，强调通过大量执行单元并行处理相似的计算任务。

在 **NPU**（神经网络处理单元）中，这些单元可能会进一步优化以适应深度学习的需求，如矩阵乘法加速器或卷积引擎等，但基本的架构流程仍然类似于 CPU 和 GPU。

