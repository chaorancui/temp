# 计算机硬件架构相关

[toc]



1. 计算机体系结构基础

   - **冯诺依曼结构的基本组件**：输入设备、输出设备、控制器、运算器、存储器
   - **CPU主要由控制器与运算器组成**
   - **CPU运作原理分为四步：**指令提取、指令解码、指令执行、结果写回
   - **CPU指令类型主要有三种：**数据传输、算数与逻辑运算、分支跳转



   - **CPU ( Central Processing Unit):** 中央处理单元
   - **ALU ( Arithmetic Logic Unit):** 算术逻辑单元
   - **IR (Instruction Register):** 指令寄存器
   - **ID (Instruction Decoder):** 指令解码器
   - **OC (Operation Controller):** 操作控制器

2. 存储层级与Cache基本原理

   1. Cache的工作原理是基于程序访问的局部性。 包括`时间局部性`和`空间局部性`：

      对大量典型程序运行情况的分析结果表明，

        在一个较短的时间间隔内，由程序产生的地址往往集中在存储器逻辑地址空间的很小范围内。指令地址的分布本来就是连续的，再加上循环程序段和子程序段要重复执行多次。因此，对这些地址的访问就自然地具有时间上集中分布的倾向。--> **时间局部性，主要是指令**

      　数据分布的这种集中倾向不如指令明显，但对数组的存储和访问以及工作单元的选择都可以使存储器地址相对集中。这种对局部范围的存储器地址频繁访问，而对此范围以外的地址则访问甚少的现象，就称为程序访问的局部性。 --> **空间局部性，主要是数据**

      > 链接：[Cache的原理、设计及实现](https://www.cnblogs.com/hackvilin/p/3251986.html)
      >
      > 根据程序的局部性原理，可以在主存和CPU通用寄存器之间设置一个高速的容量相对较小的存储器，把正在执行的指令地址附近的一部分指令或数据从主存调入这个存储器，供CPU在一段时间内使用。这对提高程序的运行速度有很大的作用。这个介于主存和CPU之间的高速小容量存储器称作高速缓冲存储器(Cache)。
      >
      > 系统正是依据此原理，不断地将与当前指令集相关联的一个不太大的后继指令集从内存读到Cache，然后再与CPU高速传送，从而达到速度匹配。
      >
      > CPU对存储器进行数据请求时，通常先访问Cache。由于局部性原理不能保证所请求的数据百分之百地在Cache中，这里便存在一个命中率。即CPU在任一时刻从Cache中可靠获取数据的几率。
      >
      > 命中率越高，正确获取数据的可靠性就越大。一般来说，Cache的存储容量比主存的容量小得多，但不能太小，太小会使命中率太低；也没有必要过大，过大不仅会增加成本，而且当容量超过一定值后，命中率随容量的增加将不会有明显地增长。
      >
      > 只要Cache的空间与主存空间在一定范围内保持适当比例的映射关系，Cache的命中率还是相当高的。
      >
      > 一般规定`Cache与内存的空间比`为`4：1000`，即128kB Cache可映射32MB内存；256kB Cache可映射64MB内存。在这种情况下，命中率都在`90％以上`。至于没有命中的数据，CPU只好直接从内存获取。获取的同时，也把它拷进Cache，以备下次访问。

   2. 存储层级

      主流存储技术如下：

      - SRAM，作为Cache
      - DRAM，作为Main Memory
      - Flash，作为Storage
      - Magnetic disk，作为Storage

      |     Memory technology      |   Typical access time   | $ per GiB in 2012 |
      | :------------------------: | :---------------------: | :---------------: |
      | SRAM semiconductor memory  |       0.5-2.5 ns        |   \$500-\$1000    |
      | DRAM semiconductor memory  |        50-70 ns         |     \$10-\$20     |
      | Flash semiconductor memory |     5,000-50,000 ns     |   \$0.75-\$1.00   |
      |       Magnetic disk        | 5,000,000-20,000,000 ns |   \$0.05-\$0.10   |

   3. Cache 基本原理

      Cache Hit：缓存中找到想要的数据

      Cache Miss：缓存中无想要的数据

      Cache 有三种组织形式：

      - 直接映射（Direct Mapping）格式：主存字块标志，cache字块索引，块内地址

        主存中的每一块只能被放置到Cache中的唯一的一个位置。其特点是空间利用率低，冲突概率最高，实现最简单。

        - Pros:查找快，一次寻址，有就是有，没有就是没有，不啰嗦（因为只需要验证一个Cache Line中是否存在该地址）
        - Cons:命中率低，CPU经常需要相邻地址的数据，而根据规则，同属于第N个Cache Line的数据会互相排斥，不会同时出现在缓存里

        ![img](https://pic2.zhimg.com/80/v2-b3d111caabc93c638bb08bde5026d711_720w.webp)

      - 全相连（Fully Associative）。格式：主存字块标志，块内地址

        主存中的任一块可以被放置到Cache中的任意一个位置。其特点是空间利用率高，冲突概率最低，实现最复杂。

        - 优点： 十分灵活、命中率高，减少了块冲突率
        - 缺点： 所需逻辑电路多且复杂，成本高

        ![img](https://pic3.zhimg.com/80/v2-1e61e8d13030ed4f0b42c2d1a854ffce_720w.webp)

      - 组相连（n-Way Set Associative）格式：主存字块标志，组号，块内地址

        主存中的每一块可以被放置到Cache中的唯一的一个组中的任何一个位置。组相联映射是直接映射和全相联映射的一种折中。

        有2路相连、4路相连、8路相连、...等。

        - 可以充分发挥前两种方式的优势。既可以存在相邻内存中的数据以提高命中，同时也一定程度上减少了查找范围，提升查找效率。

        ![img](https://pic4.zhimg.com/80/v2-9db10cd5b86e5a10f08980ab1d1cfc07_720w.webp)

      > 链接：[Cache的组织形式](https://zhuanlan.zhihu.com/p/27653161?from_voters_page=true)
      >
      > Cache通常理解为`一个表`，表中的每个条目记录了一个`内存块的数据`及这个`内存块的起始地址`。这样一个条目称为一个`Cache line`。一个Cache line存储的数据大小（即内存块的大小）在一个系统中通常是固定的，即每个cache line都是等长的。在x86系统中，这个数据是`64Bytes`。这也正常是一次`DDR内存访问`能得到的数据大小。
      >
      > 一个Cache line不但要记录数据，还要记录数据对应的地址，通常只记录`地址的一部分`，比如说在32位系统中，Cache line为64Bytes的情况下，只要记录32 - 6 = 26个高位就好了。这里的5取自2^6=64。
      >
      >
      >
      > [几句话说清楚2:CPU缓存的组织形式](https://decodezp.github.io/2018/11/25/quickwords2-cacheassociativity/)
      >
      > [Cache的基本原理](https://zhuanlan.zhihu.com/p/102293437)
      >
      > [Cache组织方式](https://zhuanlan.zhihu.com/p/107096130)
      >
      > [图解 | CPU-Cache](https://zhuanlan.zhihu.com/p/492935813)
      >
      > [计算机组成原理存储体系 ------------ 主存与Cache的地址映射](https://juejin.cn/post/7177399768111382583)
      >
      > [Cache与主存的三种地址映射详细解读](https://blog.csdn.net/aiden_kevin/article/details/113198046)
      >
      > [计算机组成原理 Cache超仔细详解 期末一遍过](https://blog.csdn.net/qq_43663263/article/details/102797565)



      补充完整图片：

      概念介绍：

      1、`块`：主存分块，Cache也分块（叫 Cache Line），两者大小相同，都是有若干连续字组成。

      2、分块的大小与组织形式，决定了地址划分方法。

      cache size是64 Bytes并且cache line size是8 bytes。offset、index和tag分别使用3 bits、3 bits和42 bits（假设地址宽度是48 bits）

      注：512 Bytes cache size，64 Bytes cache line size。根据之前的地址划分方法，offset、index和tag分别使用6 bits、3 bits和39 bits。如下图所示。

      参考：

      ![img](https://static.mianbaoban-assets.eet-china.com/xinyu-images/MBXY-CR-38142df952037027d17932b669a5ec41.png)

      ![img](https://pic2.zhimg.com/80/v2-f6fdf760d314f146941e2192957f1a81_720w.webp)

      ![img](https://img-blog.csdnimg.cn/20210202211234409.png)

      ![img](https://img-blog.csdnimg.cn/20210202211310167.png)

      ![img](https://img-blog.csdnimg.cn/20210202211329845.png)

      ![img]()

   4. 并行计算及编程思想

      - 时间并行的主要技术之一是**流水线**

        假设 single cycle cpu 的吞吐量为1，那么理想情况下，5级流水线的最大吞吐量为5。有数据依赖，可以采用停止流水线直到结果写回或采用数据前送的方法。

      - 空间并行的主要技术之一是**SIMD**，多核/多处理器

        对于相互独立的数据，可以通过增加硬件资源来提高计算性能。

      - 针对特定的应用场景，可以进行**专用的硬件设计**，我们称之为DSA（Domain Specific Architecture)： **领域专用架构**

        对特定软件业务场景而设计的专用硬件架构，Davinci就是一种DSA的硬件实现，用于深度学习加速领域。

        DSA编程思想与传统CPU编程思想对比：

        - CPU的MemoryHierarchy主要为Cache，程序员`不需要`显式管理数据转移。
        - Davinci的MemoryHierarchy主要为Buffer，程序员`需要`显式管理数据转移。

   5. 深度学习基础

      深度学习是机器学习的子集，机器学习是人工智能的子集。

      神经网络基础算子：卷积、全连接层、池化pooling、激活函数ReLU。





# 高性能计算

高性能计算中的归约算法实现与优化：<https://www.ydma.com/portal.php?mod=view&aid=3642&page=1&mobile=no>











